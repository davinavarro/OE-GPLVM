{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# from baseline.OE_GPLVM.aeb_gplvm import AEB_GPLVM, NNEncoder, kl_gaussian_loss_term\n",
    "from baseline.PyOD import PYOD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gpytorch.means import ConstantMean, ZeroMean\n",
    "from gpytorch.priors import NormalPrior, MultivariateNormalPrior\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import VariationalELBO, KLGaussianAddedLossTerm\n",
    "from torch.distributions import kl_divergence\n",
    "from tqdm import trange\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.myutils import Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"experiments_normal.json\", \"r\") as file:\n",
    "    experiments = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "datagenerator = DataGenerator()  # data generator\n",
    "utils = Utils()  # utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import kl_divergence\n",
    "from gpytorch.mlls.added_loss_term import AddedLossTerm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LatentVariable(gpytorch.Module):\n",
    "    def __init__(self, n, dim):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.latent_dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class kl_gaussian_loss_term(AddedLossTerm):\n",
    "    def __init__(self, q_x, p_x, n, data_dim):\n",
    "        self.q_x = q_x\n",
    "        self.p_x = p_x\n",
    "        self.n = n\n",
    "        self.data_dim = data_dim\n",
    "\n",
    "    def loss(self):\n",
    "        kl_per_latent_dim = kl_divergence(self.q_x, self.p_x).sum(axis=0)\n",
    "        kl_per_point = kl_per_latent_dim.sum() / self.n  # scalar\n",
    "        return kl_per_point / self.data_dim\n",
    "\n",
    "\n",
    "class NNEncoder(LatentVariable):\n",
    "    def __init__(self, n, latent_dim, prior_x, data_dim, layers):\n",
    "        super().__init__(n, latent_dim)\n",
    "        self.prior_x = prior_x\n",
    "        self.data_dim = data_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self._init_mu_nnet(layers)\n",
    "        self._init_sg_nnet(len(layers))\n",
    "        self.register_added_loss_term(\"x_kl\")\n",
    "\n",
    "    def _get_mu_layers(self, layers):\n",
    "        return (self.data_dim,) + layers + (self.latent_dim,)\n",
    "\n",
    "    def _init_mu_nnet(self, layers):\n",
    "        layers = self._get_mu_layers(layers)\n",
    "        n_layers = len(layers)\n",
    "\n",
    "        self.mu_layers = nn.ModuleList(\n",
    "            [nn.Linear(layers[i], layers[i + 1]) for i in range(n_layers - 1)]\n",
    "        )\n",
    "\n",
    "    def _get_sg_layers(self, n_layers):\n",
    "        n_sg_out = self.latent_dim**2\n",
    "        n_sg_nodes = (self.data_dim + n_sg_out) // 2\n",
    "        sg_layers = (self.data_dim,) + (n_sg_nodes,) * n_layers + (n_sg_out,)\n",
    "        return sg_layers\n",
    "\n",
    "    def _init_sg_nnet(self, n_layers):\n",
    "        layers = self._get_sg_layers(n_layers)\n",
    "        n_layers = len(layers)\n",
    "\n",
    "        self.sg_layers = nn.ModuleList(\n",
    "            [nn.Linear(layers[i], layers[i + 1]) for i in range(n_layers - 1)]\n",
    "        )\n",
    "\n",
    "    def mu(self, Y):\n",
    "        mu = torch.tanh(self.mu_layers[0](Y))\n",
    "        for i in range(1, len(self.mu_layers)):\n",
    "            mu = torch.tanh(self.mu_layers[i](mu))\n",
    "            if i == (len(self.mu_layers) - 1):\n",
    "                mu = mu * 5\n",
    "        return mu\n",
    "\n",
    "    def sigma(self, Y):\n",
    "        sg = torch.tanh(self.sg_layers[0](Y))\n",
    "        for i in range(1, len(self.sg_layers)):\n",
    "            sg = torch.tanh(self.sg_layers[i](sg))\n",
    "            if i == (len(self.sg_layers) - 1):\n",
    "                sg = sg * 5\n",
    "\n",
    "        sg = sg.reshape(len(sg), self.latent_dim, self.latent_dim)\n",
    "        sg = torch.einsum(\"aij,akj->aik\", sg, sg)\n",
    "\n",
    "        jitter = torch.eye(self.latent_dim).unsqueeze(0) * 1e-5\n",
    "        self.jitter = torch.cat([jitter for i in range(len(Y))], axis=0)\n",
    "\n",
    "        return sg + self.jitter\n",
    "\n",
    "    def forward(self, Y, batch_idx=None):\n",
    "        mu = self.mu(Y)\n",
    "        sg = self.sigma(Y)\n",
    "\n",
    "        q_x = torch.distributions.MultivariateNormal(mu, sg)\n",
    "\n",
    "        prior_x = self.prior_x\n",
    "        prior_x.loc = prior_x.loc[: len(Y), ...]\n",
    "        prior_x.covariance_matrix = prior_x.covariance_matrix[: len(Y), ...]\n",
    "        x_kl = kl_gaussian_loss_term(q_x, prior_x, self.n, self.data_dim)\n",
    "        self.update_added_loss_term(\"x_kl\", x_kl)\n",
    "        return q_x.rsample()\n",
    "\n",
    "\n",
    "class BayesianGPLVM(ApproximateGP):\n",
    "    def __init__(self, X, variational_strategy):\n",
    "        super(BayesianGPLVM, self).__init__(variational_strategy)\n",
    "        self.X = X\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_latent_variable(self, *args, **kwargs):\n",
    "        sample = self.X(*args, **kwargs)\n",
    "        return sample\n",
    "\n",
    "    def predict_latent(self, Y_test):\n",
    "        mu_star = self.X.mu(Y_test)\n",
    "        sigma_star = self.X.sigma(Y_test)\n",
    "        return mu_star, sigma_star\n",
    "\n",
    "    def get_X_mean(self, Y):\n",
    "        return self.X.mu(Y).detach()\n",
    "\n",
    "    def get_X_scales(self, Y):\n",
    "        return np.array([torch.sqrt(x.diag()) for x in self.X.sigma(Y).detach()])\n",
    "\n",
    "    def reconstruct_y(self, Y):\n",
    "        y_pred = self(self.X.mu(Y))\n",
    "        y_pred_mean = y_pred.loc.detach()\n",
    "        y_pred_covar = y_pred.covariance_matrix.detach()\n",
    "        return y_pred_mean, y_pred_covar\n",
    "\n",
    "    def get_trainable_param_names(self):\n",
    "        table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "        total_params = 0\n",
    "        for name, parameter in self.named_parameters():\n",
    "            if not parameter.requires_grad:\n",
    "                continue\n",
    "            param = parameter.numel()\n",
    "            table.add_row([name, param])\n",
    "            total_params += param\n",
    "        print(table)\n",
    "        print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "    def store(self, losses, likelihood):\n",
    "        self.losses = losses\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "\n",
    "class AEB_GPLVM(BayesianGPLVM):\n",
    "    def __init__(self, n, data_dim, latent_dim, n_inducing, X, nn_layers=None):\n",
    "        self.n = n\n",
    "        self.batch_shape = torch.Size([data_dim])\n",
    "\n",
    "        # Locations Z corresponding to u_{d}, they can be randomly initialized or\n",
    "        # regularly placed with shape (n_inducing x latent_dim).\n",
    "        self.inducing_inputs = torch.randn(n_inducing, latent_dim)\n",
    "\n",
    "        # Sparse Variational Formulation\n",
    "        q_u = CholeskyVariationalDistribution(n_inducing, batch_shape=self.batch_shape)\n",
    "        q_f = VariationalStrategy(\n",
    "            self, self.inducing_inputs, q_u, learn_inducing_locations=True\n",
    "        )\n",
    "        super(AEB_GPLVM, self).__init__(X, q_f)\n",
    "\n",
    "        self.mean_module = ZeroMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=latent_dim))\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean_x = self.mean_module(X)\n",
    "        covar_x = self.covar_module(X)\n",
    "        dist = MultivariateNormal(mean_x, covar_x)\n",
    "        return dist\n",
    "    \n",
    "    def _get_batch_idx(self, batch_size):\n",
    "        valid_indices = np.arange(self.n)\n",
    "        batch_indices = np.random.choice(valid_indices, size=batch_size, replace=False)\n",
    "        return np.sort(batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nome do arquivo JSON\n",
    "nome_arquivo = \"output.json\"\n",
    "\n",
    "\n",
    "# Função para adicionar um novo dicionário ao arquivo JSON\n",
    "def adicionar_dict_ao_json(novo_dict):\n",
    "    # Carregar dados existentes do arquivo JSON\n",
    "    try:\n",
    "        with open(nome_arquivo, \"r\") as json_file:\n",
    "            lista_de_dicts = json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        lista_de_dicts = []\n",
    "\n",
    "    # Adicionar o novo dicionário à lista\n",
    "    lista_de_dicts.append(novo_dict)\n",
    "\n",
    "    # Salvar a lista atualizada no arquivo JSON\n",
    "    with open(nome_arquivo, \"w\") as json_file:\n",
    "        json.dump(lista_de_dicts, json_file, indent=2)\n",
    "\n",
    "\n",
    "def create_dist_qx(model, batch_target):\n",
    "    mu = model.predict_latent(batch_target)[0]\n",
    "    sigma = model.predict_latent(batch_target)[1]\n",
    "    local_q_x = MultivariateNormal(mu, sigma)\n",
    "    return mu, sigma, local_q_x\n",
    "\n",
    "\n",
    "def create_dist_prior(\n",
    "    batch_target,\n",
    "    mu,\n",
    "):\n",
    "    local_p_x_mean = torch.zeros(batch_target.shape[0], mu.shape[1])\n",
    "    local_p_x_covar = torch.eye(mu.shape[1])\n",
    "    local_p_x = MultivariateNormalPrior(local_p_x_mean, local_p_x_covar)\n",
    "    return local_p_x\n",
    "\n",
    "\n",
    "def kl_divergence_variational(target):\n",
    "    ll_shape = torch.zeros_like(target.T)\n",
    "    klu = (\n",
    "        ll_shape.T.add_(model.variational_strategy.kl_divergence().div(batch_size))\n",
    "        .sum(-1)\n",
    "        .T.div((n_train))\n",
    "    )\n",
    "    return klu\n",
    "\n",
    "\n",
    "def predict_score(X_test):\n",
    "    n_test = len(X_test)\n",
    "    mu, sigma, local_q_x, local_p_x = create_dist_qx(model, X_test), create_dist_prior(\n",
    "        X_test, mu\n",
    "    )\n",
    "    X_pred = model(model.sample_latent_variable(X_test))\n",
    "    log_likelihood = likelihood.expected_log_prob(X_test.T, X_pred).sum(0).div(n_test)\n",
    "    kl_x = kl_divergence(local_q_x, local_p_x).div(n_test)\n",
    "    kl_u = kl_divergence_variational(X_test)\n",
    "    score = -(log_likelihood - kl_u - kl_x).detach().numpy()\n",
    "    score = MinMaxScaler().fit_transform(np.reshape(score, (-1, 1)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset': '02_annthyroid',\n",
       "  'kernel': 'rbf',\n",
       "  'batch_size': '128',\n",
       "  'learning_rate': '0.01',\n",
       "  'loss': 'normal',\n",
       "  'layers': '5,5,5',\n",
       "  'n_epochs': '5000',\n",
       "  'latent_dim': 3}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments[8:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current noise type: None\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|     | 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_terms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#_, _, batch_index, ratio = get_indices(lb_train)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#idx_n, idx_a = get_loe_index(Y_train, batch_index)\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m ll_n, klu_n, kl_n, lm_n \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_terms\u001b[49m(Y_train, idx_n)\n\u001b[1;32m     77\u001b[0m loss_normal \u001b[38;5;241m=\u001b[39m (ll_n \u001b[38;5;241m-\u001b[39m klu_n \u001b[38;5;241m-\u001b[39m kl_n)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     78\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(loss_normal)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_terms' is not defined"
     ]
    }
   ],
   "source": [
    "experiment_results = []\n",
    "random.seed(42)\n",
    "for experiment in experiments[8:9]:\n",
    "    dataset = experiment[\"dataset\"]\n",
    "    nn_layers = 10, 10  # tuple(map(int, experiment[\"layers\"].split(\",\")))\n",
    "    kernel_type = experiment[\"kernel\"]\n",
    "    gplvm_type = experiment[\"loss\"]\n",
    "    labeled_anomalies = 0.99  # float(experiment[\"labeled_anomalies\"])\n",
    "    noise_type = None  # experiment[\"noise_type\"]\n",
    "    n_epochs = 5000  # int(experiment[\"n_epochs\"])\n",
    "\n",
    "    datagenerator.dataset = dataset\n",
    "    data = datagenerator.generator(\n",
    "        la=labeled_anomalies,\n",
    "        realistic_synthetic_mode=None,\n",
    "        stdscale=True,\n",
    "        minmax=False,\n",
    "        noise_type=None,\n",
    "    )\n",
    "\n",
    "    Y_train = torch.tensor(data[\"X_train\"], dtype=torch.float32)\n",
    "    Y_test = torch.tensor(data[\"X_test\"], dtype=torch.float32)\n",
    "    lb_train = torch.tensor(data[\"y_train\"], dtype=torch.float32)\n",
    "    lb_test = torch.tensor(data[\"y_test\"], dtype=torch.float32)\n",
    "\n",
    "    idx_n = np.where(lb_train == 0)[0]\n",
    "    Y_train = Y_train[idx_n]\n",
    "    lb_train = lb_train[idx_n]\n",
    "\n",
    "    N = len(Y_train)\n",
    "    data_dim = Y_train.shape[-1]\n",
    "    latent_dim = int(experiment[\"latent_dim\"])\n",
    "    n_inducing = 50\n",
    "    lr = float(experiment[\"learning_rate\"])\n",
    "    batch_size = int(experiment[\"batch_size\"])\n",
    "    n_train = len(Y_train)\n",
    "\n",
    "    model_dict = {}\n",
    "    noise_trace_dict = {}\n",
    "    loss_list = []\n",
    "    noise_trace = []\n",
    "    lln_list = []\n",
    "    kln_list = []\n",
    "    lla_list = []\n",
    "    kla_list = []\n",
    "\n",
    "    X_prior_mean = torch.zeros(n_train, latent_dim)\n",
    "    X_prior_covar = torch.eye(X_prior_mean.shape[1])\n",
    "    prior_x = MultivariateNormalPrior(X_prior_mean, X_prior_covar)\n",
    "    encoder = NNEncoder(n_train, latent_dim, prior_x, data_dim, nn_layers)\n",
    "    model = AEB_GPLVM(\n",
    "        n_train,\n",
    "        data_dim,\n",
    "        latent_dim,\n",
    "        n_inducing,\n",
    "        encoder,\n",
    "        nn_layers,\n",
    "    )\n",
    "\n",
    "    likelihood = GaussianLikelihood()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\": model.parameters()},\n",
    "            {\"params\": likelihood.parameters()},\n",
    "        ],\n",
    "        lr,\n",
    "    )\n",
    "    model.train()\n",
    "    iterator = trange(n_epochs, leave=True)\n",
    "\n",
    "    # try:\n",
    "    for i in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        #_, _, batch_index, ratio = get_indices(lb_train)\n",
    "        #idx_n, idx_a = get_loe_index(Y_train, batch_index)\n",
    "        ll_n, klu_n, kl_n, lm_n = calculate_terms(Y_train, idx_n)\n",
    "        loss_normal = (ll_n - klu_n - kl_n).sum()\n",
    "        loss = -(loss_normal).sum()\n",
    "\n",
    "        # records = [ll_n, klu_n, kl_n, lm_n, ll_a, klu_a, kl_a, lm_a]\n",
    "        # save_records(records_dict, records)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iterator.set_description(\n",
    "            \"Loss: \" + str(float(np.round(loss.item(), 2))) + \", iter no: \"\n",
    "        )\n",
    "        if i % 10 == 0:\n",
    "            loss_list.append(loss.item())\n",
    "        if float(np.round(loss.item(), 2)) < -40:\n",
    "            break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "    Y_pred_mean, Y_pred_covar = model.reconstruct_y(Y_test)\n",
    "    X_pred_mean, X_pred_covar = model.predict_latent(Y_test)\n",
    "    # Adicionar Erro de Reconstrucao\n",
    "    #\n",
    "    # Adicionar Standard Scale\n",
    "\n",
    "    inv_lengthscale = 1 / model.covar_module.base_kernel.lengthscale\n",
    "    values, indices = torch.topk(\n",
    "        model.covar_module.base_kernel.lengthscale, k=2, largest=False\n",
    "    )\n",
    "    component_1 = indices.numpy().flatten()[0]\n",
    "    component_2 = indices.numpy().flatten()[1]\n",
    "\n",
    "    X = model.X.q_mu.detach().numpy()\n",
    "    std = torch.nn.functional.softplus(model.X.q_log_sigma).detach().numpy()\n",
    "\n",
    "    metrics = utils.metric(y_true=lb_test, y_score=predict_score(Y_test))\n",
    "    exp = {\n",
    "        \"gplvm_type\": \"normal\",\n",
    "        \"dataset\": dataset,\n",
    "        \"noise_type\": noise_type,\n",
    "        \"labeled_anomalies\": labeled_anomalies,\n",
    "        \"data_dim\": data_dim,\n",
    "        \"n_samples\": N,\n",
    "        \"n_dim_latent\": latent_dim,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"n_inducing\": n_inducing,\n",
    "        \"n_layers\": nn_layers,\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"auc_roc\": metrics[\"aucroc\"],\n",
    "        \"auc_pr\": metrics[\"aucpr\"],\n",
    "        \"elbo\": loss_list,\n",
    "        \"inv_lenghtscale\": [],\n",
    "    }\n",
    "\n",
    "    # experiment_results.append(exp)\n",
    "    adicionar_dict_ao_json(exp)\n",
    "    # except:\n",
    "    #    print(\"erro no experimento: \" , experiment )\n",
    "\n",
    "    # with open(\"results_100.json\", \"a\") as final:\n",
    "    #    json.dump(exp, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_mean = model.get_X_mean(Y_train)\n",
    "X_train_scales = model.get_X_scales(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([tensor([0.0464, 0.0428, 0.0256]), tensor([0.0425, 0.0505, 0.0314]),\n",
       "       tensor([0.0422, 0.0501, 0.0304]), ...,\n",
       "       tensor([0.0440, 0.0486, 0.0303]), tensor([0.0473, 0.0411, 0.0245]),\n",
       "       tensor([0.0453, 0.0447, 0.0269])], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
