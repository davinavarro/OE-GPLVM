{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install prettytable\n",
    "import warnings\n",
    "import json\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import the necessary package\n",
    "from baseline.OE_GPLVM.aeb_gplvm import AEB_GPLVM, NNEncoder, kl_gaussian_loss_term\n",
    "from baseline.OE_GPLVM.train import *\n",
    "from baseline.OE_GPLVM.utils import *\n",
    "from baseline.PyOD import PYOD\n",
    "from gpytorch.mlls import KLGaussianAddedLossTerm\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import VariationalELBO, KLGaussianAddedLossTerm\n",
    "from torch.distributions import kl_divergence\n",
    "from gpytorch.priors import MultivariateNormalPrior\n",
    "from tqdm import trange\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.myutils import Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "datagenerator = DataGenerator()  # data generator\n",
    "utils = Utils()  # utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset and model list / dict\n",
    "dataset_list = [\n",
    "    \"01_ALOI\",\n",
    "    \"02_annthyroid\",\n",
    "    \"03_backdoor\",\n",
    "    \"04_breastw\",\n",
    "    \"05_campaign\",\n",
    "    \"06_cardio\",\n",
    "    \"07_Cardiotocography\",\n",
    "    \"08_celeba\",\n",
    "]\n",
    "\n",
    "kernel_list = [\"RBF\", \"MATERN\"]\n",
    "layers_list = [(10, 10), (5, 5)]\n",
    "gplvm_list = [\"normal\", \"loe_hard\", \"loe_soft\"]\n",
    "labeled_anomalies_list = [0.2, 0.5, 0.8, 1.0]\n",
    "noise_type_list = [\"no_noise\", \"local\", \"global\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_indices(y_train):\n",
    "    idx_a = np.where(y_train == 1)[0]\n",
    "    idx_n = np.where(y_train == 0)[0]\n",
    "    ratio = len(idx_a) / (len(idx_a) + len(idx_n))\n",
    "    qtd_anomaly = int(ratio * batch_size)\n",
    "    qtd_normal = batch_size - qtd_anomaly\n",
    "    idx_n = torch.tensor(np.random.choice(idx_n, qtd_normal, replace=True))\n",
    "\n",
    "    if qtd_anomaly == 0:\n",
    "        idx_a = torch.tensor(np.random.choice(idx_n, qtd_anomaly, replace=True))\n",
    "    else:\n",
    "        idx_a = torch.tensor(np.random.choice(idx_a, qtd_anomaly, replace=True))\n",
    "\n",
    "    batch_index = torch.cat([idx_n, idx_a])\n",
    "\n",
    "    return idx_n, idx_a, batch_index, ratio\n",
    "\n",
    "\n",
    "def get_loe_index(X, indices):\n",
    "    ll_0, klu_0, kl_0, _ = calculate_terms(X, indices)\n",
    "    score = ll_0 - kl_0\n",
    "\n",
    "    qtd_normal = int(score.shape[0] * (1 - ratio))\n",
    "    qtd_anormal = batch_size - int(score.shape[0] * (1 - ratio))\n",
    "\n",
    "    _, loe_idx_n = torch.topk(score, qtd_normal, largest=True, sorted=False)\n",
    "    _, loe_idx_a = torch.topk(score, qtd_anormal, largest=False, sorted=False)\n",
    "    return indices[loe_idx_n], indices[loe_idx_a]\n",
    "\n",
    "\n",
    "def create_dist_qx(model, batch_target):\n",
    "    mu = model.predict_latent(batch_target)[0]\n",
    "    sigma = model.predict_latent(batch_target)[1]\n",
    "    local_q_x = MultivariateNormal(mu, sigma)\n",
    "    return mu, sigma, local_q_x\n",
    "\n",
    "\n",
    "def create_dist_prior(\n",
    "    batch_target,\n",
    "    mu,\n",
    "):\n",
    "    local_p_x_mean = torch.zeros(batch_target.shape[0], mu.shape[1])\n",
    "    local_p_x_covar = torch.eye(mu.shape[1])\n",
    "    local_p_x = MultivariateNormalPrior(local_p_x_mean, local_p_x_covar)\n",
    "    return local_p_x\n",
    "\n",
    "\n",
    "def kl_divergence_variational(target):\n",
    "    ll_shape = torch.zeros_like(target.T)\n",
    "    klu = (\n",
    "        ll_shape.T.add_(model.variational_strategy.kl_divergence().div(batch_size))\n",
    "        .sum(-1)\n",
    "        .T.div((n_train))\n",
    "    )\n",
    "    return klu\n",
    "\n",
    "\n",
    "def calculate_terms(X, indices):\n",
    "    batch_target = X[indices]\n",
    "    mu, sigma, local_q_x = create_dist_qx(model, batch_target)\n",
    "    local_p_x = create_dist_prior(batch_target, mu)\n",
    "    batch_output = model(model.sample_latent_variable(batch_target))\n",
    "    log_likelihood = (\n",
    "        likelihood.expected_log_prob(batch_target.T, batch_output)\n",
    "        .sum(0)\n",
    "        .div(batch_size)\n",
    "    )\n",
    "    kl_x = kl_divergence(local_q_x, local_p_x).div(n_train)\n",
    "    kl_u = kl_divergence_variational(batch_target)\n",
    "    log_marginal = (\n",
    "        likelihood.log_marginal(batch_target.T, batch_output).sum(0).div(batch_size)\n",
    "    )\n",
    "    return log_likelihood, kl_u, kl_x, log_marginal\n",
    "\n",
    "\n",
    "def predict_score(X_test):\n",
    "    n_test = len(X_test)\n",
    "    mu, sigma, local_q_x = create_dist_qx(model, X_test)\n",
    "    local_p_x = create_dist_prior(X_test, mu)\n",
    "    X_pred = model(model.sample_latent_variable(X_test))\n",
    "    exp_log_prob = likelihood.expected_log_prob(X_test.T, X_pred)\n",
    "    log_likelihood = exp_log_prob.sum(0).div(n_test)\n",
    "    kl_x = kl_divergence(local_q_x, local_p_x).div(n_test)\n",
    "    kl_u = kl_divergence_variational(X_test)\n",
    "    score = -(log_likelihood - kl_u - kl_x).detach().numpy()\n",
    "    score = MinMaxScaler().fit_transform(np.reshape(score, (-1, 1)))\n",
    "    return score\n",
    "\n",
    "\n",
    "records_dict = {\n",
    "    \"log_likelihood_normal\": [],\n",
    "    \"kl_divergence_variational_normal\": [],\n",
    "    \"kl_divergence_latent_normal\": [],\n",
    "    \"log_marginal_normal\": [],\n",
    "    \"log_likelihood_abnormal\": [],\n",
    "    \"kl_divergence_variational_abnormal\": [],\n",
    "    \"kl_divergence_latent_abnormal\": [],\n",
    "    \"log_marginal_abnormal\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def save_records(records_dict, records):\n",
    "    records_dict[\"log_likelihood_normal\"].append(records[0].sum().detach().numpy())\n",
    "    records_dict[\"kl_divergence_variational_normal\"].append(\n",
    "        records[1].sum().detach().numpy()\n",
    "    )\n",
    "    records_dict[\"kl_divergence_latent_normal\"].append(\n",
    "        records[2].sum().detach().numpy()\n",
    "    )\n",
    "    records_dict[\"log_marginal_normal\"].append(records[3].sum().detach().numpy())\n",
    "    records_dict[\"log_likelihood_abnormal\"].append(records[4].sum().detach().numpy())\n",
    "    records_dict[\"kl_divergence_variational_abnormal\"].append(\n",
    "        records[5].sum().detach().numpy()\n",
    "    )\n",
    "    records_dict[\"kl_divergence_latent_abnormal\"].append(\n",
    "        records[6].sum().detach().numpy()\n",
    "    )\n",
    "    records_dict[\"log_marginal_abnormal\"].append(records[7].sum().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_results = []\n",
    "random.seed(42)\n",
    "for _ in range(1):\n",
    "    # dataset = np.random.choice(dataset_list, replace=False)\n",
    "    dataset = \"06_cardio\"\n",
    "    nn_idx = np.random.choice([0, 1], replace=False)\n",
    "    nn_layers = layers_list[nn_idx]\n",
    "    kernel_type = np.random.choice(kernel_list)\n",
    "    gplvm_type = np.random.choice(gplvm_list)\n",
    "    labeled_anomalies = 1.0  # random.choice(labeled_anomalies_list)\n",
    "    noise_type = np.random.choice(noise_type_list)\n",
    "\n",
    "    print(dataset, nn_layers)\n",
    "\n",
    "    datagenerator.dataset = dataset\n",
    "    data = datagenerator.generator(\n",
    "        la=labeled_anomalies, realistic_synthetic_mode=None, noise_type=None\n",
    "    )\n",
    "    \n",
    "    Y_train = torch.tensor(data[\"X_train\"], dtype=torch.float32)\n",
    "    Y_test = torch.tensor(data[\"X_test\"], dtype=torch.float32)\n",
    "    lb_train = torch.tensor(data[\"y_train\"], dtype=torch.float32)\n",
    "    lb_test = torch.tensor(data[\"y_test\"], dtype=torch.float32)\n",
    "    \n",
    "    #\n",
    "    N = len(Y_train)\n",
    "    data_dim = Y_train.shape[-1]\n",
    "    latent_dim = Y_train.shape[-1] / 2\n",
    "    n_inducing = 50\n",
    "    n_epochs = 100\n",
    "    lr = 0.0025\n",
    "    batch_size = 100\n",
    "    n_train = len(Y_train)\n",
    "    #\n",
    "    model_dict = {}\n",
    "    noise_trace_dict = {}\n",
    "    loss_list = []\n",
    "    noise_trace = []\n",
    "    lln_list = []\n",
    "    kln_list = []\n",
    "    lla_list = []\n",
    "    kla_list = []\n",
    "\n",
    "    X_prior_mean = torch.zeros(n_train, latent_dim)\n",
    "    X_prior_covar = torch.eye(X_prior_mean.shape[1])\n",
    "    prior_x = MultivariateNormalPrior(X_prior_mean, X_prior_covar)\n",
    "    encoder = NNEncoder(n_train, latent_dim, prior_x, data_dim, nn_layers)\n",
    "    model = AEB_GPLVM(\n",
    "        n_train,\n",
    "        data_dim,\n",
    "        latent_dim,\n",
    "        n_inducing,\n",
    "        encoder,\n",
    "        nn_layers,\n",
    "    )\n",
    "    \n",
    "    likelihood = GaussianLikelihood()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\": model.parameters()},\n",
    "            {\"params\": likelihood.parameters()},\n",
    "        ],\n",
    "        lr,\n",
    "    )\n",
    "    model.train()\n",
    "\n",
    "    iterator = trange(3000, leave=True)\n",
    "    for i in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        _, _, batch_index, ratio = get_indices(lb_train)\n",
    "        idx_n, idx_a = get_loe_index(Y_train, batch_index)\n",
    "\n",
    "        ll_n, klu_n, kl_n, lm_n = calculate_terms(Y_train, idx_n)\n",
    "        ll_a, klu_a, kl_a, lm_a = calculate_terms(Y_train, idx_a)\n",
    "        loss_normal, loss_anomaly = (ll_n - klu_n - kl_n).sum(), (\n",
    "            ll_a - klu_a - kl_a\n",
    "        ).sum()\n",
    "        loss = -(loss_normal + loss_anomaly).sum()\n",
    "\n",
    "        # records = [ll_n, klu_n, kl_n, lm_n, ll_a, klu_a, kl_a, lm_a]\n",
    "        # save_records(records_dict, records)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iterator.set_description(\n",
    "            \"Loss: \" + str(float(np.round(loss.item(), 2))) + \", iter no: \"\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "    Y_pred_mean, Y_pred_covar = model.reconstruct_y(Y_test)\n",
    "    X_pred_mean, X_pred_covar = model.predict_latent(Y_test)\n",
    "    metrics = utils.metric(y_true=lb_test, y_score=predict_score(Y_test))\n",
    "\n",
    "    exp = {\n",
    "        \"gplvm_type\": \"normal\",\n",
    "        \"dataset\": dataset,\n",
    "        \"noise_type\": noise_type,\n",
    "        \"labeled_anomalies\": labeled_anomalies,\n",
    "        \"data_dim\": data_dim,\n",
    "        \"n_samples\": N,\n",
    "        \"n_dim_latent\": latent_dim,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"n_inducing\": n_inducing,\n",
    "        \"n_layers\": nn_layers,\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"auc_roc\": metrics[\"aucroc\"],\n",
    "        \"auc_pr\": metrics[\"aucpr\"],\n",
    "        \"elbo_normal\": [],\n",
    "        \"inv_lenghtscale\": [],\n",
    "    }\n",
    "\n",
    "    experiment_results.append(exp)\n",
    "\n",
    "    with open(\"results_100.json\", \"a\") as final:\n",
    "        json.dump(exp, final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Idéias pro Sumário ( Dissertação )\n",
    "\n",
    "## Introdução\n",
    "- Explicar problema de detecção de anomalias\n",
    "- Comentar sobre trabalho -> shallow e deep anomaly\n",
    "- Explicar foco em nao-supervisionado \n",
    "- Explicar a utilização dos gps\n",
    "- Explicar onde é utilizado as detecções\n",
    "- Explicar como é a junção dos universos ( GPs e OD )\n",
    "\n",
    "## Fundamentação Teórica ( Pra artigos, pode reduzir isso )\n",
    "- Falar sobre GPs, GPLVM (Mostrar Amortização) , AD, LOE-AD\n",
    "- Equações, Algoritmos ( Geral, Equacionamento GP etc) \n",
    "\n",
    "## Revisão Literatura ( Pra artigos, pode reduzir isso )\n",
    "- Comparar os trabalhos ( Deteção de Anomalias, GPs com AD, Mundo de AD no geral )\n",
    "- Comparação ( Coisas Diferentes e Semelhantes ao nosso trabalho )\n",
    "- Deixar explícita a lacuna. Existe uma falta de GPs em literatura de AD.\n",
    "\n",
    "## Metodologia ( Proposta ) \n",
    "- Definir o problema que estamos lidando ( Problema nao-supervisionado, com uma quantidade X de outliers no treino )\n",
    "- Aplicar GPLVM bayesiana ( amortizada )\n",
    "- Modificar o ELBO ( Decomposição Normal e Anomalias ( Consideradas no Treino ) ) \n",
    "- Como estamos treinando ? Treina primeiro a componente normal ? Destravamento e depois treina com LOE\n",
    "- Arquitetura, Kernels, Parametros.\n",
    "- Definir framework de detecção.\n",
    "\n",
    "## Resultados Experimentais ( Na proposta, itens 1 e 2 ) ( Pra artigos, recorte disso aqui )\n",
    "- Dados Sintéticos ( Plotáveis 2D )\n",
    "- Experimentos Exaustivos ( Vários Datasets )\n",
    "- Experimentos Exóticos ( Cenários Desafiadores,  Ruídos no Geral ...)\n",
    "\n",
    "## Conclusões e Trabalhos Futuros ( Na proposta temos aqui Cronograma ) \n",
    "\n",
    "## Ideias pra Artigo Talvez ?!\n",
    "- Utilizar valor de anomalias rotulados como sendo limiar ( treshold ) da aucroc.\n",
    "- Como utilizar incerteza do GPLVM para ajudar no processo de detecção de anomalias.\n",
    "\n",
    "## Data Marcada pro Dia 23/11 as 14h (Sala de Seminários la na UFC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
