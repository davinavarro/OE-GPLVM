{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run ADBench \n",
    "- Here we provide a demo for testing AD algorithms on the datasets proposed in ADBench.\n",
    "- Feel free to evaluate any customized algorithm in ADBench.\n",
    "- For reproducing the complete experiment results in ADBench, please run the code in the run.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:53.643140Z",
     "start_time": "2022-07-08T07:41:41.552946Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 16:21:56.718803: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-03 16:21:56.757045: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# import basic package\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import the necessary package\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.myutils import Utils\n",
    "\n",
    "datagenerator = DataGenerator()  # data generator\n",
    "utils = Utils()  # utils function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We include all the datasets of ADBench in the \"datasets\" folder, as the \"number_data_class.npz\" filename. Please see the table in the markdown for details.\n",
    "    - You can specify the dataset name by removing the filename \".npz\" suffix in the data generator, e.g., \"10_cover.npz\" as \"10_cover\". \n",
    "    \n",
    "    \n",
    "- All the algorithms included in the ADBench are illustrated in the table of markdown.\n",
    "    - You need to specify the model name when initialization, as some algorithms (e.g., supervised algorithms) are integrated in one class, please see the table in the markdown for details.\n",
    "    - You can also test your own AD algorithms on our generated dataset, as long as the algorithm can output anomaly score for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:53.675055Z",
     "start_time": "2022-07-08T07:41:53.648127Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01_ALOI.npz',\n",
       " '02_annthyroid.npz',\n",
       " '03_backdoor.npz',\n",
       " '04_breastw.npz',\n",
       " '05_campaign.npz',\n",
       " '06_cardio.npz',\n",
       " '07_Cardiotocography.npz',\n",
       " '08_celeba.npz',\n",
       " '09_census.npz',\n",
       " '99_circles.npz',\n",
       " '99_clusters.npz',\n",
       " '99_linear.npz',\n",
       " '99_moons.npz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"datasets/Classical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:55.627834Z",
     "start_time": "2022-07-08T07:41:53.682035Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from baseline.PyOD import PYOD\n",
    "from aeb_gplvm import *\n",
    "\n",
    "# dataset and model list / dict\n",
    "dataset_list = [\n",
    "    \"01_ALOI\",\n",
    "    \"02_annthyroid\",\n",
    "    \"03_backdoor\",\n",
    "    \"04_breastw\",\n",
    "    \"05_campaign\",\n",
    "    \"06_cardio\",\n",
    "    \"07_Cardiotocography\",\n",
    "    \"08_celeba\",\n",
    "    \"09_census\",\n",
    "]\n",
    "model_dict = {\n",
    "    \"IForest\": PYOD,\n",
    "    \"PCA\": PYOD,\n",
    "    \"GPLVM\": \"\",\n",
    "    \"AEB_GPLVM\": \"\"\n",
    "}\n",
    "\n",
    "# save the results\n",
    "df_AUCROC = pd.DataFrame(data=None, index=dataset_list, columns=model_dict.keys())\n",
    "df_AUCPR = pd.DataFrame(data=None, index=dataset_list, columns=model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:50:14.507244Z",
     "start_time": "2022-07-08T07:41:55.631823Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsampling for dataset 01_ALOI...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 302, 'Anomalies Ratio(%)': 3.02}\n",
      "best param: None\n",
      "best param: None\n",
      "  1/219 [..............................] - ETA: 18s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 16:05:34.402561: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 524us/step\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 27)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                1728      \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_1 (TFOpLam  (None, 32)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.math.pow_1 (TFOpLambda)  (None, 32)                0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_1 (TFOpL  (None,)                  0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.math.reduce_mean_1 (TFOp  ()                       0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.__operators__.add_1 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " add_loss_1 (AddLoss)        ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,776\n",
      "Trainable params: 3,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 2.8914 - val_loss: 2.0055\n",
      "Epoch 2/100\n",
      "197/197 [==============================] - 0s 792us/step - loss: 1.5045 - val_loss: 1.3301\n",
      "Epoch 3/100\n",
      "197/197 [==============================] - 0s 794us/step - loss: 1.0112 - val_loss: 0.9001\n",
      "Epoch 4/100\n",
      "197/197 [==============================] - 0s 770us/step - loss: 0.7238 - val_loss: 0.6696\n",
      "Epoch 5/100\n",
      "197/197 [==============================] - 0s 810us/step - loss: 0.5669 - val_loss: 0.5538\n",
      "Epoch 6/100\n",
      "197/197 [==============================] - 0s 782us/step - loss: 0.4795 - val_loss: 0.4720\n",
      "Epoch 7/100\n",
      "197/197 [==============================] - 0s 774us/step - loss: 0.4239 - val_loss: 0.4062\n",
      "Epoch 8/100\n",
      "197/197 [==============================] - 0s 776us/step - loss: 0.3725 - val_loss: 0.3614\n",
      "Epoch 9/100\n",
      "197/197 [==============================] - 0s 793us/step - loss: 0.3317 - val_loss: 0.3201\n",
      "Epoch 10/100\n",
      "197/197 [==============================] - 0s 776us/step - loss: 0.3040 - val_loss: 0.2954\n",
      "Epoch 11/100\n",
      "197/197 [==============================] - 0s 834us/step - loss: 0.2807 - val_loss: 0.2619\n",
      "Epoch 12/100\n",
      "197/197 [==============================] - 0s 800us/step - loss: 0.2551 - val_loss: 0.2592\n",
      "Epoch 13/100\n",
      "197/197 [==============================] - 0s 775us/step - loss: 0.2399 - val_loss: 0.2387\n",
      "Epoch 14/100\n",
      "197/197 [==============================] - 0s 814us/step - loss: 0.2285 - val_loss: 0.2345\n",
      "Epoch 15/100\n",
      "197/197 [==============================] - 0s 796us/step - loss: 0.2202 - val_loss: 0.2280\n",
      "Epoch 16/100\n",
      "197/197 [==============================] - 0s 821us/step - loss: 0.2132 - val_loss: 0.2161\n",
      "Epoch 17/100\n",
      "197/197 [==============================] - 0s 864us/step - loss: 0.2047 - val_loss: 0.2098\n",
      "Epoch 18/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2026 - val_loss: 0.2109\n",
      "Epoch 19/100\n",
      "197/197 [==============================] - 0s 922us/step - loss: 0.1983 - val_loss: 0.2065\n",
      "Epoch 20/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.1947 - val_loss: 0.1933\n",
      "Epoch 21/100\n",
      "197/197 [==============================] - 0s 846us/step - loss: 0.1920 - val_loss: 0.1978\n",
      "Epoch 22/100\n",
      "197/197 [==============================] - 0s 951us/step - loss: 0.1904 - val_loss: 0.1935\n",
      "Epoch 23/100\n",
      "197/197 [==============================] - 0s 867us/step - loss: 0.1898 - val_loss: 0.1902\n",
      "Epoch 24/100\n",
      "197/197 [==============================] - 0s 776us/step - loss: 0.1877 - val_loss: 0.1897\n",
      "Epoch 25/100\n",
      "197/197 [==============================] - 0s 776us/step - loss: 0.1875 - val_loss: 0.1855\n",
      "Epoch 26/100\n",
      "197/197 [==============================] - 0s 790us/step - loss: 0.1840 - val_loss: 0.1837\n",
      "Epoch 27/100\n",
      "197/197 [==============================] - 0s 811us/step - loss: 0.1808 - val_loss: 0.1891\n",
      "Epoch 28/100\n",
      "197/197 [==============================] - 0s 774us/step - loss: 0.1808 - val_loss: 0.1820\n",
      "Epoch 29/100\n",
      "197/197 [==============================] - 0s 766us/step - loss: 0.1794 - val_loss: 0.1803\n",
      "Epoch 30/100\n",
      "197/197 [==============================] - 0s 791us/step - loss: 0.1796 - val_loss: 0.1788\n",
      "Epoch 31/100\n",
      "197/197 [==============================] - 0s 761us/step - loss: 0.1793 - val_loss: 0.1863\n",
      "Epoch 32/100\n",
      "197/197 [==============================] - 0s 766us/step - loss: 0.1770 - val_loss: 0.1743\n",
      "Epoch 33/100\n",
      "197/197 [==============================] - 0s 779us/step - loss: 0.1746 - val_loss: 0.1796\n",
      "Epoch 34/100\n",
      "197/197 [==============================] - 0s 770us/step - loss: 0.1763 - val_loss: 0.1735\n",
      "Epoch 35/100\n",
      "197/197 [==============================] - 0s 769us/step - loss: 0.1785 - val_loss: 0.1951\n",
      "Epoch 36/100\n",
      "197/197 [==============================] - 0s 803us/step - loss: 0.1771 - val_loss: 0.1779\n",
      "Epoch 37/100\n",
      "197/197 [==============================] - 0s 794us/step - loss: 0.1746 - val_loss: 0.1769\n",
      "Epoch 38/100\n",
      "197/197 [==============================] - 0s 847us/step - loss: 0.1732 - val_loss: 0.1728\n",
      "Epoch 39/100\n",
      "197/197 [==============================] - 0s 758us/step - loss: 0.1726 - val_loss: 0.1740\n",
      "Epoch 40/100\n",
      "197/197 [==============================] - 0s 797us/step - loss: 0.1737 - val_loss: 0.1758\n",
      "Epoch 41/100\n",
      "197/197 [==============================] - 0s 778us/step - loss: 0.1722 - val_loss: 0.1709\n",
      "Epoch 42/100\n",
      "197/197 [==============================] - 0s 776us/step - loss: 0.1750 - val_loss: 0.1762\n",
      "Epoch 43/100\n",
      "197/197 [==============================] - 0s 798us/step - loss: 0.1745 - val_loss: 0.1750\n",
      "Epoch 44/100\n",
      "197/197 [==============================] - 0s 787us/step - loss: 0.1714 - val_loss: 0.1769\n",
      "Epoch 45/100\n",
      "197/197 [==============================] - 0s 753us/step - loss: 0.1706 - val_loss: 0.1684\n",
      "Epoch 46/100\n",
      "197/197 [==============================] - 0s 739us/step - loss: 0.1707 - val_loss: 0.1694\n",
      "Epoch 47/100\n",
      "197/197 [==============================] - 0s 725us/step - loss: 0.1713 - val_loss: 0.1715\n",
      "Epoch 48/100\n",
      "197/197 [==============================] - 0s 929us/step - loss: 0.1718 - val_loss: 0.1717\n",
      "Epoch 49/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.1737 - val_loss: 0.1751\n",
      "Epoch 50/100\n",
      "197/197 [==============================] - 0s 942us/step - loss: 0.1741 - val_loss: 0.1820\n",
      "Epoch 51/100\n",
      "197/197 [==============================] - 0s 988us/step - loss: 0.1708 - val_loss: 0.1688\n",
      "Epoch 52/100\n",
      "197/197 [==============================] - 0s 878us/step - loss: 0.1688 - val_loss: 0.1777\n",
      "Epoch 53/100\n",
      "197/197 [==============================] - 0s 839us/step - loss: 0.1701 - val_loss: 0.1703\n",
      "Epoch 54/100\n",
      "197/197 [==============================] - 0s 948us/step - loss: 0.1709 - val_loss: 0.1763\n",
      "Epoch 55/100\n",
      "197/197 [==============================] - 0s 857us/step - loss: 0.1702 - val_loss: 0.1788\n",
      "Epoch 56/100\n",
      "197/197 [==============================] - 0s 822us/step - loss: 0.1676 - val_loss: 0.1686\n",
      "Epoch 57/100\n",
      "197/197 [==============================] - 0s 944us/step - loss: 0.1705 - val_loss: 0.1680\n",
      "Epoch 58/100\n",
      "197/197 [==============================] - 0s 863us/step - loss: 0.1703 - val_loss: 0.1818\n",
      "Epoch 59/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.1712 - val_loss: 0.1730\n",
      "Epoch 60/100\n",
      "197/197 [==============================] - 0s 791us/step - loss: 0.1715 - val_loss: 0.1761\n",
      "Epoch 61/100\n",
      "197/197 [==============================] - 0s 752us/step - loss: 0.1678 - val_loss: 0.1710\n",
      "Epoch 62/100\n",
      "197/197 [==============================] - 0s 744us/step - loss: 0.1676 - val_loss: 0.1738\n",
      "Epoch 63/100\n",
      "197/197 [==============================] - 0s 767us/step - loss: 0.1771 - val_loss: 0.1972\n",
      "Epoch 64/100\n",
      "197/197 [==============================] - 0s 819us/step - loss: 0.1746 - val_loss: 0.1770\n",
      "Epoch 65/100\n",
      "197/197 [==============================] - 0s 772us/step - loss: 0.1669 - val_loss: 0.1679\n",
      "Epoch 66/100\n",
      "197/197 [==============================] - 0s 755us/step - loss: 0.1666 - val_loss: 0.1686\n",
      "Epoch 67/100\n",
      "197/197 [==============================] - 0s 761us/step - loss: 0.1677 - val_loss: 0.1719\n",
      "Epoch 68/100\n",
      "197/197 [==============================] - 0s 815us/step - loss: 0.1687 - val_loss: 0.1743\n",
      "Epoch 69/100\n",
      "197/197 [==============================] - 0s 805us/step - loss: 0.1674 - val_loss: 0.1666\n",
      "Epoch 70/100\n",
      "197/197 [==============================] - 0s 808us/step - loss: 0.1685 - val_loss: 0.1695\n",
      "Epoch 71/100\n",
      "197/197 [==============================] - 0s 763us/step - loss: 0.1679 - val_loss: 0.1669\n",
      "Epoch 72/100\n",
      "197/197 [==============================] - 0s 751us/step - loss: 0.1696 - val_loss: 0.1713\n",
      "Epoch 73/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.1681 - val_loss: 0.1659\n",
      "Epoch 74/100\n",
      "197/197 [==============================] - 0s 827us/step - loss: 0.1663 - val_loss: 0.1678\n",
      "Epoch 75/100\n",
      "197/197 [==============================] - 0s 839us/step - loss: 0.1689 - val_loss: 0.1730\n",
      "Epoch 76/100\n",
      "197/197 [==============================] - 0s 823us/step - loss: 0.1735 - val_loss: 0.1742\n",
      "Epoch 77/100\n",
      "197/197 [==============================] - 0s 758us/step - loss: 0.1692 - val_loss: 0.1743\n",
      "Epoch 78/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.1668 - val_loss: 0.1687\n",
      "Epoch 79/100\n",
      "197/197 [==============================] - 0s 925us/step - loss: 0.1653 - val_loss: 0.1662\n",
      "Epoch 80/100\n",
      "197/197 [==============================] - 0s 955us/step - loss: 0.1658 - val_loss: 0.1718\n",
      "Epoch 81/100\n",
      "197/197 [==============================] - 0s 890us/step - loss: 0.1660 - val_loss: 0.1747\n",
      "Epoch 82/100\n",
      "197/197 [==============================] - 0s 840us/step - loss: 0.1786 - val_loss: 0.1734\n",
      "Epoch 83/100\n",
      "197/197 [==============================] - 0s 764us/step - loss: 0.1717 - val_loss: 0.1767\n",
      "Epoch 84/100\n",
      "197/197 [==============================] - 0s 764us/step - loss: 0.1698 - val_loss: 0.1684\n",
      "Epoch 85/100\n",
      "197/197 [==============================] - 0s 978us/step - loss: 0.1662 - val_loss: 0.1682\n",
      "Epoch 86/100\n",
      "197/197 [==============================] - 0s 856us/step - loss: 0.1654 - val_loss: 0.1714\n",
      "Epoch 87/100\n",
      "197/197 [==============================] - 0s 919us/step - loss: 0.1684 - val_loss: 0.1707\n",
      "Epoch 88/100\n",
      "197/197 [==============================] - 0s 916us/step - loss: 0.1666 - val_loss: 0.1692\n",
      "Epoch 89/100\n",
      "197/197 [==============================] - 0s 770us/step - loss: 0.1650 - val_loss: 0.1684\n",
      "Epoch 90/100\n",
      "197/197 [==============================] - 0s 792us/step - loss: 0.1685 - val_loss: 0.1682\n",
      "Epoch 91/100\n",
      "197/197 [==============================] - 0s 771us/step - loss: 0.1678 - val_loss: 0.1725\n",
      "Epoch 92/100\n",
      "197/197 [==============================] - 0s 754us/step - loss: 0.1696 - val_loss: 0.1666\n",
      "Epoch 93/100\n",
      "197/197 [==============================] - 0s 775us/step - loss: 0.1670 - val_loss: 0.1701\n",
      "Epoch 94/100\n",
      "197/197 [==============================] - 0s 751us/step - loss: 0.1697 - val_loss: 0.1776\n",
      "Epoch 95/100\n",
      "197/197 [==============================] - 0s 752us/step - loss: 0.1663 - val_loss: 0.1753\n",
      "Epoch 96/100\n",
      "197/197 [==============================] - 0s 746us/step - loss: 0.1649 - val_loss: 0.1717\n",
      "Epoch 97/100\n",
      "197/197 [==============================] - 0s 753us/step - loss: 0.1656 - val_loss: 0.1762\n",
      "Epoch 98/100\n",
      "197/197 [==============================] - 0s 749us/step - loss: 0.1664 - val_loss: 0.1656\n",
      "Epoch 99/100\n",
      "197/197 [==============================] - 0s 769us/step - loss: 0.1652 - val_loss: 0.1714\n",
      "Epoch 100/100\n",
      "197/197 [==============================] - 0s 757us/step - loss: 0.1647 - val_loss: 0.1679\n",
      "219/219 [==============================] - 0s 474us/step\n",
      "94/94 [==============================] - 0s 498us/step\n",
      "current noise type: None\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "best param: None\n",
      "best param: None\n",
      "158/158 [==============================] - 0s 452us/step\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                384       \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_3 (TFOpLam  (None, 32)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.math.pow_3 (TFOpLambda)  (None, 32)                0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_3 (TFOpL  (None,)                  0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.math.reduce_mean_3 (TFOp  ()                       0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.__operators__.add_3 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " add_loss_3 (AddLoss)        ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,432\n",
      "Trainable params: 2,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 1.0333 - val_loss: 0.5814\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 0s 766us/step - loss: 0.4848 - val_loss: 0.3562\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 0s 758us/step - loss: 0.3200 - val_loss: 0.2535\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 0s 771us/step - loss: 0.2333 - val_loss: 0.1962\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 0s 778us/step - loss: 0.1913 - val_loss: 0.1708\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 0s 765us/step - loss: 0.1683 - val_loss: 0.1543\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 0s 762us/step - loss: 0.1549 - val_loss: 0.1505\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 0s 759us/step - loss: 0.1469 - val_loss: 0.1380\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 0s 749us/step - loss: 0.1411 - val_loss: 0.1332\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 0s 750us/step - loss: 0.1369 - val_loss: 0.1286\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 0s 755us/step - loss: 0.1345 - val_loss: 0.1279\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 0s 863us/step - loss: 0.1331 - val_loss: 0.1260\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1315 - val_loss: 0.1240\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 0s 894us/step - loss: 0.1303 - val_loss: 0.1315\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 0s 946us/step - loss: 0.1303 - val_loss: 0.1246\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 0s 927us/step - loss: 0.1302 - val_loss: 0.1268\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 0s 893us/step - loss: 0.1289 - val_loss: 0.1214\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 0s 891us/step - loss: 0.1290 - val_loss: 0.1265\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 0s 792us/step - loss: 0.1274 - val_loss: 0.1210\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 0s 777us/step - loss: 0.1268 - val_loss: 0.1226\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 0s 764us/step - loss: 0.1272 - val_loss: 0.1197\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 0s 769us/step - loss: 0.1269 - val_loss: 0.1202\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 0s 767us/step - loss: 0.1267 - val_loss: 0.1210\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 0s 772us/step - loss: 0.1267 - val_loss: 0.1202\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1259 - val_loss: 0.1201\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 0s 937us/step - loss: 0.1286 - val_loss: 0.1276\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 0s 794us/step - loss: 0.1264 - val_loss: 0.1210\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 0s 777us/step - loss: 0.1260 - val_loss: 0.1197\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 0s 765us/step - loss: 0.1253 - val_loss: 0.1193\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 0s 798us/step - loss: 0.1250 - val_loss: 0.1249\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 0s 779us/step - loss: 0.1260 - val_loss: 0.1177\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 0s 789us/step - loss: 0.1242 - val_loss: 0.1200\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 0s 767us/step - loss: 0.1239 - val_loss: 0.1172\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 0s 785us/step - loss: 0.1237 - val_loss: 0.1170\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 0s 781us/step - loss: 0.1251 - val_loss: 0.1197\n",
      "Epoch 36/100\n",
      "142/142 [==============================] - 0s 796us/step - loss: 0.1236 - val_loss: 0.1179\n",
      "Epoch 37/100\n",
      "142/142 [==============================] - 0s 770us/step - loss: 0.1255 - val_loss: 0.1220\n",
      "Epoch 38/100\n",
      "142/142 [==============================] - 0s 789us/step - loss: 0.1245 - val_loss: 0.1185\n",
      "Epoch 39/100\n",
      "142/142 [==============================] - 0s 771us/step - loss: 0.1236 - val_loss: 0.1161\n",
      "Epoch 40/100\n",
      "142/142 [==============================] - 0s 762us/step - loss: 0.1229 - val_loss: 0.1176\n",
      "Epoch 41/100\n",
      "142/142 [==============================] - 0s 776us/step - loss: 0.1236 - val_loss: 0.1181\n",
      "Epoch 42/100\n",
      "142/142 [==============================] - 0s 755us/step - loss: 0.1236 - val_loss: 0.1181\n",
      "Epoch 43/100\n",
      "142/142 [==============================] - 0s 777us/step - loss: 0.1224 - val_loss: 0.1170\n",
      "Epoch 44/100\n",
      "142/142 [==============================] - 0s 759us/step - loss: 0.1230 - val_loss: 0.1161\n",
      "Epoch 45/100\n",
      "142/142 [==============================] - 0s 765us/step - loss: 0.1230 - val_loss: 0.1234\n",
      "Epoch 46/100\n",
      "142/142 [==============================] - 0s 769us/step - loss: 0.1237 - val_loss: 0.1162\n",
      "Epoch 47/100\n",
      "142/142 [==============================] - 0s 797us/step - loss: 0.1227 - val_loss: 0.1161\n",
      "Epoch 48/100\n",
      "142/142 [==============================] - 0s 777us/step - loss: 0.1214 - val_loss: 0.1197\n",
      "Epoch 49/100\n",
      "142/142 [==============================] - 0s 767us/step - loss: 0.1226 - val_loss: 0.1173\n",
      "Epoch 50/100\n",
      "142/142 [==============================] - 0s 763us/step - loss: 0.1223 - val_loss: 0.1168\n",
      "Epoch 51/100\n",
      "142/142 [==============================] - 0s 761us/step - loss: 0.1223 - val_loss: 0.1192\n",
      "Epoch 52/100\n",
      "142/142 [==============================] - 0s 786us/step - loss: 0.1226 - val_loss: 0.1179\n",
      "Epoch 53/100\n",
      "142/142 [==============================] - 0s 749us/step - loss: 0.1217 - val_loss: 0.1147\n",
      "Epoch 54/100\n",
      "142/142 [==============================] - 0s 770us/step - loss: 0.1225 - val_loss: 0.1168\n",
      "Epoch 55/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1222 - val_loss: 0.1173\n",
      "Epoch 56/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1227 - val_loss: 0.1174\n",
      "Epoch 57/100\n",
      "142/142 [==============================] - 0s 843us/step - loss: 0.1224 - val_loss: 0.1185\n",
      "Epoch 58/100\n",
      "142/142 [==============================] - 0s 990us/step - loss: 0.1213 - val_loss: 0.1155\n",
      "Epoch 59/100\n",
      "142/142 [==============================] - 0s 852us/step - loss: 0.1215 - val_loss: 0.1142\n",
      "Epoch 60/100\n",
      "142/142 [==============================] - 0s 868us/step - loss: 0.1208 - val_loss: 0.1185\n",
      "Epoch 61/100\n",
      "142/142 [==============================] - 0s 812us/step - loss: 0.1215 - val_loss: 0.1180\n",
      "Epoch 62/100\n",
      "142/142 [==============================] - 0s 776us/step - loss: 0.1211 - val_loss: 0.1138\n",
      "Epoch 63/100\n",
      "142/142 [==============================] - 0s 774us/step - loss: 0.1204 - val_loss: 0.1164\n",
      "Epoch 64/100\n",
      "142/142 [==============================] - 0s 781us/step - loss: 0.1220 - val_loss: 0.1161\n",
      "Epoch 65/100\n",
      "142/142 [==============================] - 0s 769us/step - loss: 0.1227 - val_loss: 0.1187\n",
      "Epoch 66/100\n",
      "142/142 [==============================] - 0s 757us/step - loss: 0.1217 - val_loss: 0.1148\n",
      "Epoch 67/100\n",
      "142/142 [==============================] - 0s 789us/step - loss: 0.1206 - val_loss: 0.1148\n",
      "Epoch 68/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1203 - val_loss: 0.1152\n",
      "Epoch 69/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1227 - val_loss: 0.1186\n",
      "Epoch 70/100\n",
      "142/142 [==============================] - 0s 900us/step - loss: 0.1208 - val_loss: 0.1141\n",
      "Epoch 71/100\n",
      "142/142 [==============================] - 0s 828us/step - loss: 0.1198 - val_loss: 0.1159\n",
      "Epoch 72/100\n",
      "142/142 [==============================] - 0s 775us/step - loss: 0.1212 - val_loss: 0.1139\n",
      "Epoch 73/100\n",
      "142/142 [==============================] - 0s 773us/step - loss: 0.1203 - val_loss: 0.1138\n",
      "Epoch 74/100\n",
      "142/142 [==============================] - 0s 767us/step - loss: 0.1199 - val_loss: 0.1138\n",
      "Epoch 75/100\n",
      "142/142 [==============================] - 0s 734us/step - loss: 0.1198 - val_loss: 0.1147\n",
      "Epoch 76/100\n",
      "142/142 [==============================] - 0s 767us/step - loss: 0.1207 - val_loss: 0.1152\n",
      "Epoch 77/100\n",
      "142/142 [==============================] - 0s 782us/step - loss: 0.1197 - val_loss: 0.1170\n",
      "Epoch 78/100\n",
      "142/142 [==============================] - 0s 778us/step - loss: 0.1199 - val_loss: 0.1172\n",
      "Epoch 79/100\n",
      "142/142 [==============================] - 0s 785us/step - loss: 0.1191 - val_loss: 0.1162\n",
      "Epoch 80/100\n",
      "142/142 [==============================] - 0s 780us/step - loss: 0.1204 - val_loss: 0.1140\n",
      "Epoch 81/100\n",
      "142/142 [==============================] - 0s 788us/step - loss: 0.1197 - val_loss: 0.1166\n",
      "Epoch 82/100\n",
      "142/142 [==============================] - 0s 785us/step - loss: 0.1197 - val_loss: 0.1181\n",
      "Epoch 83/100\n",
      "142/142 [==============================] - 0s 816us/step - loss: 0.1212 - val_loss: 0.1166\n",
      "Epoch 84/100\n",
      "142/142 [==============================] - 0s 793us/step - loss: 0.1195 - val_loss: 0.1128\n",
      "Epoch 85/100\n",
      "142/142 [==============================] - 0s 808us/step - loss: 0.1193 - val_loss: 0.1169\n",
      "Epoch 86/100\n",
      "142/142 [==============================] - 0s 771us/step - loss: 0.1206 - val_loss: 0.1134\n",
      "Epoch 87/100\n",
      "142/142 [==============================] - 0s 762us/step - loss: 0.1196 - val_loss: 0.1177\n",
      "Epoch 88/100\n",
      "142/142 [==============================] - 0s 766us/step - loss: 0.1209 - val_loss: 0.1139\n",
      "Epoch 89/100\n",
      "142/142 [==============================] - 0s 775us/step - loss: 0.1196 - val_loss: 0.1135\n",
      "Epoch 90/100\n",
      "142/142 [==============================] - 0s 754us/step - loss: 0.1193 - val_loss: 0.1179\n",
      "Epoch 91/100\n",
      "142/142 [==============================] - 0s 766us/step - loss: 0.1194 - val_loss: 0.1131\n",
      "Epoch 92/100\n",
      "142/142 [==============================] - 0s 759us/step - loss: 0.1189 - val_loss: 0.1160\n",
      "Epoch 93/100\n",
      "142/142 [==============================] - 0s 765us/step - loss: 0.1186 - val_loss: 0.1137\n",
      "Epoch 94/100\n",
      "142/142 [==============================] - 0s 760us/step - loss: 0.1194 - val_loss: 0.1139\n",
      "Epoch 95/100\n",
      "142/142 [==============================] - 0s 775us/step - loss: 0.1182 - val_loss: 0.1178\n",
      "Epoch 96/100\n",
      "142/142 [==============================] - 0s 771us/step - loss: 0.1192 - val_loss: 0.1137\n",
      "Epoch 97/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1182 - val_loss: 0.1205\n",
      "Epoch 98/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.1143\n",
      "Epoch 99/100\n",
      "142/142 [==============================] - 0s 904us/step - loss: 0.1200 - val_loss: 0.1142\n",
      "Epoch 100/100\n",
      "142/142 [==============================] - 0s 1ms/step - loss: 0.1191 - val_loss: 0.1123\n",
      "158/158 [==============================] - 0s 538us/step\n",
      "68/68 [==============================] - 0s 522us/step\n",
      "subsampling for dataset 03_backdoor...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 236, 'Anomalies Ratio(%)': 2.36}\n",
      "best param: None\n",
      "best param: None\n",
      "219/219 [==============================] - 0s 457us/step\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 196)]             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                12544     \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_5 (TFOpLam  (None, 32)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.math.pow_5 (TFOpLambda)  (None, 32)                0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_5 (TFOpL  (None,)                  0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.math.reduce_mean_5 (TFOp  ()                       0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.__operators__.add_5 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " add_loss_5 (AddLoss)        ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,592\n",
      "Trainable params: 14,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 7.1151 - val_loss: 4.1415\n",
      "Epoch 2/100\n",
      "197/197 [==============================] - 0s 815us/step - loss: 4.0822 - val_loss: 6.7658\n",
      "Epoch 3/100\n",
      "197/197 [==============================] - 0s 817us/step - loss: 3.3583 - val_loss: 12.3271\n",
      "Epoch 4/100\n",
      "197/197 [==============================] - 0s 836us/step - loss: 2.7077 - val_loss: 21.3384\n",
      "Epoch 5/100\n",
      "197/197 [==============================] - 0s 803us/step - loss: 2.1605 - val_loss: 34.7672\n",
      "Epoch 6/100\n",
      "197/197 [==============================] - 0s 780us/step - loss: 1.7527 - val_loss: 52.4049\n",
      "Epoch 7/100\n",
      "197/197 [==============================] - 0s 784us/step - loss: 1.4734 - val_loss: 73.7922\n",
      "Epoch 8/100\n",
      "197/197 [==============================] - 0s 797us/step - loss: 1.2517 - val_loss: 97.9846\n",
      "Epoch 9/100\n",
      "197/197 [==============================] - 0s 788us/step - loss: 1.0760 - val_loss: 125.7384\n",
      "Epoch 10/100\n",
      "197/197 [==============================] - 0s 798us/step - loss: 0.8946 - val_loss: 158.2882\n",
      "Epoch 11/100\n",
      "197/197 [==============================] - 0s 792us/step - loss: 0.7844 - val_loss: 193.4727\n",
      "Epoch 12/100\n",
      "197/197 [==============================] - 0s 799us/step - loss: 0.6796 - val_loss: 229.8149\n",
      "Epoch 13/100\n",
      "197/197 [==============================] - 0s 786us/step - loss: 0.6046 - val_loss: 268.1414\n",
      "Epoch 14/100\n",
      "197/197 [==============================] - 0s 806us/step - loss: 0.5496 - val_loss: 307.1984\n",
      "Epoch 15/100\n",
      "197/197 [==============================] - 0s 793us/step - loss: 0.4947 - val_loss: 346.3704\n",
      "Epoch 16/100\n",
      "197/197 [==============================] - 0s 812us/step - loss: 0.4579 - val_loss: 387.3603\n",
      "Epoch 17/100\n",
      "197/197 [==============================] - 0s 802us/step - loss: 0.4366 - val_loss: 430.8516\n",
      "Epoch 18/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 472.1349\n",
      "Epoch 19/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 513.6278\n",
      "Epoch 20/100\n",
      "197/197 [==============================] - 0s 925us/step - loss: 0.3880 - val_loss: 562.1111\n",
      "Epoch 21/100\n",
      "197/197 [==============================] - 0s 929us/step - loss: 0.3734 - val_loss: 606.6335\n",
      "Epoch 22/100\n",
      "197/197 [==============================] - 0s 870us/step - loss: 0.3813 - val_loss: 649.2231\n",
      "Epoch 23/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.3572 - val_loss: 694.1658\n",
      "Epoch 24/100\n",
      "197/197 [==============================] - 0s 862us/step - loss: 0.3542 - val_loss: 734.8725\n",
      "Epoch 25/100\n",
      "197/197 [==============================] - 0s 807us/step - loss: 0.3534 - val_loss: 782.2068\n",
      "Epoch 26/100\n",
      "197/197 [==============================] - 0s 791us/step - loss: 0.3324 - val_loss: 826.2330\n",
      "Epoch 27/100\n",
      "197/197 [==============================] - 0s 975us/step - loss: 0.3199 - val_loss: 873.8090\n",
      "Epoch 28/100\n",
      "197/197 [==============================] - 0s 858us/step - loss: 0.3281 - val_loss: 917.6227\n",
      "Epoch 29/100\n",
      "197/197 [==============================] - 0s 824us/step - loss: 0.3268 - val_loss: 960.9058\n",
      "Epoch 30/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3218 - val_loss: 1004.2797\n",
      "Epoch 31/100\n",
      "197/197 [==============================] - 0s 917us/step - loss: 0.3120 - val_loss: 1053.4794\n",
      "Epoch 32/100\n",
      "197/197 [==============================] - 0s 805us/step - loss: 0.3053 - val_loss: 1101.0280\n",
      "Epoch 33/100\n",
      "197/197 [==============================] - 0s 799us/step - loss: 0.3235 - val_loss: 1150.0912\n",
      "Epoch 34/100\n",
      "197/197 [==============================] - 0s 782us/step - loss: 0.3068 - val_loss: 1193.2644\n",
      "Epoch 35/100\n",
      "197/197 [==============================] - 0s 787us/step - loss: 0.3037 - val_loss: 1244.9495\n",
      "Epoch 36/100\n",
      "197/197 [==============================] - 0s 810us/step - loss: 0.2903 - val_loss: 1287.2352\n",
      "Epoch 37/100\n",
      "197/197 [==============================] - 0s 815us/step - loss: 0.2927 - val_loss: 1331.4761\n",
      "Epoch 38/100\n",
      "197/197 [==============================] - 0s 803us/step - loss: 0.2947 - val_loss: 1396.5604\n",
      "Epoch 39/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.2907 - val_loss: 1445.2950\n",
      "Epoch 40/100\n",
      "197/197 [==============================] - 0s 805us/step - loss: 0.2911 - val_loss: 1487.9550\n",
      "Epoch 41/100\n",
      "197/197 [==============================] - 0s 791us/step - loss: 0.2900 - val_loss: 1532.0808\n",
      "Epoch 42/100\n",
      "197/197 [==============================] - 0s 789us/step - loss: 0.2872 - val_loss: 1582.4364\n",
      "Epoch 43/100\n",
      "197/197 [==============================] - 0s 806us/step - loss: 0.2880 - val_loss: 1622.0270\n",
      "Epoch 44/100\n",
      "197/197 [==============================] - 0s 836us/step - loss: 0.2846 - val_loss: 1661.2058\n",
      "Epoch 45/100\n",
      "197/197 [==============================] - 0s 852us/step - loss: 0.2885 - val_loss: 1703.0848\n",
      "Epoch 46/100\n",
      "197/197 [==============================] - 0s 858us/step - loss: 0.2886 - val_loss: 1761.7460\n",
      "Epoch 47/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2810 - val_loss: 1815.5593\n",
      "Epoch 48/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2914 - val_loss: 1869.7117\n",
      "Epoch 49/100\n",
      "197/197 [==============================] - 0s 953us/step - loss: 0.2855 - val_loss: 1922.7543\n",
      "Epoch 50/100\n",
      "197/197 [==============================] - 0s 953us/step - loss: 0.2817 - val_loss: 1976.1772\n",
      "Epoch 51/100\n",
      "197/197 [==============================] - 0s 905us/step - loss: 0.2741 - val_loss: 2030.1965\n",
      "Epoch 52/100\n",
      "197/197 [==============================] - 0s 844us/step - loss: 0.2852 - val_loss: 2070.4436\n",
      "Epoch 53/100\n",
      "197/197 [==============================] - 0s 834us/step - loss: 0.2721 - val_loss: 2111.7798\n",
      "Epoch 54/100\n",
      "197/197 [==============================] - 0s 803us/step - loss: 0.2632 - val_loss: 2165.8203\n",
      "Epoch 55/100\n",
      "197/197 [==============================] - 0s 795us/step - loss: 0.2512 - val_loss: 2238.6470\n",
      "Epoch 56/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2543 - val_loss: 2293.2126\n",
      "Epoch 57/100\n",
      "197/197 [==============================] - 0s 843us/step - loss: 0.2509 - val_loss: 2339.1570\n",
      "Epoch 58/100\n",
      "197/197 [==============================] - 0s 820us/step - loss: 0.2463 - val_loss: 2382.4934\n",
      "Epoch 59/100\n",
      "197/197 [==============================] - 0s 798us/step - loss: 0.2453 - val_loss: 2437.2966\n",
      "Epoch 60/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2417 - val_loss: 2551.6819\n",
      "Epoch 61/100\n",
      "197/197 [==============================] - 0s 997us/step - loss: 0.2599 - val_loss: 2609.0322\n",
      "Epoch 62/100\n",
      "197/197 [==============================] - 0s 816us/step - loss: 0.2939 - val_loss: 2706.8748\n",
      "Epoch 63/100\n",
      "197/197 [==============================] - 0s 820us/step - loss: 0.2474 - val_loss: 2941.4666\n",
      "Epoch 64/100\n",
      "197/197 [==============================] - 0s 805us/step - loss: 0.2493 - val_loss: 3016.0835\n",
      "Epoch 65/100\n",
      "197/197 [==============================] - 0s 826us/step - loss: 0.2470 - val_loss: 3544.0852\n",
      "Epoch 66/100\n",
      "197/197 [==============================] - 0s 823us/step - loss: 0.2470 - val_loss: 3396.0774\n",
      "Epoch 67/100\n",
      "197/197 [==============================] - 0s 813us/step - loss: 0.2474 - val_loss: 3799.3296\n",
      "Epoch 68/100\n",
      "197/197 [==============================] - 0s 838us/step - loss: 0.2446 - val_loss: 4128.6650\n",
      "Epoch 69/100\n",
      "197/197 [==============================] - 0s 798us/step - loss: 0.2439 - val_loss: 4627.7935\n",
      "Epoch 70/100\n",
      "197/197 [==============================] - 0s 799us/step - loss: 0.2466 - val_loss: 4610.3716\n",
      "Epoch 71/100\n",
      "197/197 [==============================] - 0s 800us/step - loss: 0.2440 - val_loss: 5199.9409\n",
      "Epoch 72/100\n",
      "197/197 [==============================] - 0s 806us/step - loss: 0.2443 - val_loss: 4645.1772\n",
      "Epoch 73/100\n",
      "197/197 [==============================] - 0s 817us/step - loss: 0.2426 - val_loss: 5271.9404\n",
      "Epoch 74/100\n",
      "197/197 [==============================] - 0s 809us/step - loss: 0.2412 - val_loss: 5961.0996\n",
      "Epoch 75/100\n",
      "197/197 [==============================] - 0s 812us/step - loss: 0.2401 - val_loss: 5342.0620\n",
      "Epoch 76/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2419 - val_loss: 6881.3701\n",
      "Epoch 77/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2755 - val_loss: 6483.1143\n",
      "Epoch 78/100\n",
      "197/197 [==============================] - 0s 952us/step - loss: 0.2594 - val_loss: 4635.0215\n",
      "Epoch 79/100\n",
      "197/197 [==============================] - 0s 941us/step - loss: 0.2475 - val_loss: 6012.1934\n",
      "Epoch 80/100\n",
      "197/197 [==============================] - 0s 876us/step - loss: 0.2444 - val_loss: 6458.0483\n",
      "Epoch 81/100\n",
      "197/197 [==============================] - 0s 827us/step - loss: 0.2435 - val_loss: 6895.4824\n",
      "Epoch 82/100\n",
      "197/197 [==============================] - 0s 824us/step - loss: 0.2406 - val_loss: 7613.5391\n",
      "Epoch 83/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.2390 - val_loss: 8001.7471\n",
      "Epoch 84/100\n",
      "197/197 [==============================] - 0s 832us/step - loss: 0.2427 - val_loss: 9531.5088\n",
      "Epoch 85/100\n",
      "197/197 [==============================] - 0s 998us/step - loss: 0.2445 - val_loss: 7604.9448\n",
      "Epoch 86/100\n",
      "197/197 [==============================] - 0s 873us/step - loss: 0.2418 - val_loss: 8309.0010\n",
      "Epoch 87/100\n",
      "197/197 [==============================] - 0s 801us/step - loss: 0.2422 - val_loss: 9014.6162\n",
      "Epoch 88/100\n",
      "197/197 [==============================] - 0s 859us/step - loss: 0.2572 - val_loss: 8565.1934\n",
      "Epoch 89/100\n",
      "197/197 [==============================] - 0s 828us/step - loss: 0.3123 - val_loss: 7872.5610\n",
      "Epoch 90/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2542 - val_loss: 8516.8574\n",
      "Epoch 91/100\n",
      "197/197 [==============================] - 0s 908us/step - loss: 0.2413 - val_loss: 11087.5205\n",
      "Epoch 92/100\n",
      "197/197 [==============================] - 0s 833us/step - loss: 0.2449 - val_loss: 11800.0850\n",
      "Epoch 93/100\n",
      "197/197 [==============================] - 0s 820us/step - loss: 0.2447 - val_loss: 13236.9678\n",
      "Epoch 94/100\n",
      "197/197 [==============================] - 0s 828us/step - loss: 0.2414 - val_loss: 14147.4062\n",
      "Epoch 95/100\n",
      "197/197 [==============================] - 0s 814us/step - loss: 0.2401 - val_loss: 15897.5225\n",
      "Epoch 96/100\n",
      "197/197 [==============================] - 0s 817us/step - loss: 0.2444 - val_loss: 14305.3037\n",
      "Epoch 97/100\n",
      "197/197 [==============================] - 0s 826us/step - loss: 0.2393 - val_loss: 16846.5078\n",
      "Epoch 98/100\n",
      "197/197 [==============================] - 0s 805us/step - loss: 0.2696 - val_loss: 13379.4854\n",
      "Epoch 99/100\n",
      "197/197 [==============================] - 0s 809us/step - loss: 0.2408 - val_loss: 15135.3848\n",
      "Epoch 100/100\n",
      "197/197 [==============================] - 0s 815us/step - loss: 0.2407 - val_loss: 18457.9395\n",
      "219/219 [==============================] - 0s 500us/step\n",
      "94/94 [==============================] - 0s 517us/step\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 360, 'Anomalies Ratio(%)': 36.0}\n",
      "best param: None\n",
      "best param: None\n",
      "22/22 [==============================] - 0s 541us/step\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 9)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_7 (TFOpLam  (None, 32)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.math.pow_7 (TFOpLambda)  (None, 32)                0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_7 (TFOpL  (None,)                  0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.math.reduce_mean_7 (TFOp  ()                       0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.__operators__.add_7 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " add_loss_7 (AddLoss)        ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,624\n",
      "Trainable params: 2,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 2.2745 - val_loss: 1.5891\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.3267 - val_loss: 1.1264\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0527 - val_loss: 0.9276\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9139 - val_loss: 0.8177\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8160 - val_loss: 0.7300\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7272 - val_loss: 0.6518\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6508 - val_loss: 0.5979\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5994 - val_loss: 0.5603\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5579 - val_loss: 0.5307\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 0.5030\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4927 - val_loss: 0.4614\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4372 - val_loss: 0.4144\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4049 - val_loss: 0.3973\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3858 - val_loss: 0.3784\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3678 - val_loss: 0.3631\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3505\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3408 - val_loss: 0.3381\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3276 - val_loss: 0.3265\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3146 - val_loss: 0.3161\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3026 - val_loss: 0.3052\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2907 - val_loss: 0.2939\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2790 - val_loss: 0.2833\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2688 - val_loss: 0.2727\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2588 - val_loss: 0.2636\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2498 - val_loss: 0.2551\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2399 - val_loss: 0.2446\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2315 - val_loss: 0.2369\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2227 - val_loss: 0.2300\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2154 - val_loss: 0.2235\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2070 - val_loss: 0.2169\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2007 - val_loss: 0.2108\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1954 - val_loss: 0.2067\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1898 - val_loss: 0.2014\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1857 - val_loss: 0.1975\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1806 - val_loss: 0.1937\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1768 - val_loss: 0.1922\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1733 - val_loss: 0.1878\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1696 - val_loss: 0.1841\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1667 - val_loss: 0.1840\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1640 - val_loss: 0.1794\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1612 - val_loss: 0.1762\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1591 - val_loss: 0.1751\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1564 - val_loss: 0.1744\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1550 - val_loss: 0.1722\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1523 - val_loss: 0.1707\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1509 - val_loss: 0.1689\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1493 - val_loss: 0.1675\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1473 - val_loss: 0.1664\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1465 - val_loss: 0.1638\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1444 - val_loss: 0.1642\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1432 - val_loss: 0.1631\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1416 - val_loss: 0.1602\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1403 - val_loss: 0.1600\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1387 - val_loss: 0.1577\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1379 - val_loss: 0.1567\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1369 - val_loss: 0.1571\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1359 - val_loss: 0.1563\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1350 - val_loss: 0.1561\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1563\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1330 - val_loss: 0.1525\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1318 - val_loss: 0.1530\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1309 - val_loss: 0.1518\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1303 - val_loss: 0.1514\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.1530\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.1505\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1285 - val_loss: 0.1513\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.1500\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1272 - val_loss: 0.1510\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1264 - val_loss: 0.1505\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1261 - val_loss: 0.1493\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1254 - val_loss: 0.1516\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1250 - val_loss: 0.1510\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1246 - val_loss: 0.1489\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1244 - val_loss: 0.1488\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1245 - val_loss: 0.1505\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1234 - val_loss: 0.1496\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1230 - val_loss: 0.1477\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 0.1469\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1222 - val_loss: 0.1468\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.1220 - val_loss: 0.1470\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.1479\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1219 - val_loss: 0.1455\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.1453\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1205 - val_loss: 0.1459\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1204 - val_loss: 0.1455\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1200 - val_loss: 0.1453\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1195 - val_loss: 0.1454\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.1437\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1193 - val_loss: 0.1441\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1198 - val_loss: 0.1444\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.1451\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1192 - val_loss: 0.1456\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.1445\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1185 - val_loss: 0.1435\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1178 - val_loss: 0.1442\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1185 - val_loss: 0.1430\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1180 - val_loss: 0.1439\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1174 - val_loss: 0.1442\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1174 - val_loss: 0.1441\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1178 - val_loss: 0.1435\n",
      "22/22 [==============================] - 0s 557us/step\n",
      "10/10 [==============================] - 0s 673us/step\n",
      "subsampling for dataset 05_campaign...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1103, 'Anomalies Ratio(%)': 11.03}\n",
      "best param: None\n",
      "best param: None\n",
      "219/219 [==============================] - 0s 455us/step\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 62)]              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                3968      \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_9 (TFOpLam  (None, 32)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.math.pow_9 (TFOpLambda)  (None, 32)                0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_9 (TFOpL  (None,)                  0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.math.reduce_mean_9 (TFOp  ()                       0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.__operators__.add_9 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " add_loss_9 (AddLoss)        ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,016\n",
      "Trainable params: 6,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 5.2999 - val_loss: 2.5935\n",
      "Epoch 2/100\n",
      "197/197 [==============================] - 0s 805us/step - loss: 2.2397 - val_loss: 1.4518\n",
      "Epoch 3/100\n",
      "197/197 [==============================] - 0s 829us/step - loss: 1.3132 - val_loss: 0.9835\n",
      "Epoch 4/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.9723 - val_loss: 0.7794\n",
      "Epoch 5/100\n",
      "197/197 [==============================] - 0s 875us/step - loss: 0.8079 - val_loss: 0.6823\n",
      "Epoch 6/100\n",
      "197/197 [==============================] - 0s 934us/step - loss: 0.7328 - val_loss: 0.6300\n",
      "Epoch 7/100\n",
      "197/197 [==============================] - 0s 865us/step - loss: 0.6912 - val_loss: 0.6042\n",
      "Epoch 8/100\n",
      "197/197 [==============================] - 0s 849us/step - loss: 0.6544 - val_loss: 0.5714\n",
      "Epoch 9/100\n",
      "197/197 [==============================] - 0s 802us/step - loss: 0.6304 - val_loss: 0.5647\n",
      "Epoch 10/100\n",
      "197/197 [==============================] - 0s 788us/step - loss: 0.6186 - val_loss: 0.5555\n",
      "Epoch 11/100\n",
      "197/197 [==============================] - 0s 771us/step - loss: 0.6096 - val_loss: 0.5538\n",
      "Epoch 12/100\n",
      "197/197 [==============================] - 0s 768us/step - loss: 0.5961 - val_loss: 0.5516\n",
      "Epoch 13/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5915 - val_loss: 0.5483\n",
      "Epoch 14/100\n",
      "197/197 [==============================] - 0s 802us/step - loss: 0.5883 - val_loss: 0.5428\n",
      "Epoch 15/100\n",
      "197/197 [==============================] - 0s 775us/step - loss: 0.5764 - val_loss: 0.5407\n",
      "Epoch 16/100\n",
      "197/197 [==============================] - 0s 810us/step - loss: 0.5774 - val_loss: 0.5448\n",
      "Epoch 17/100\n",
      "197/197 [==============================] - 0s 785us/step - loss: 0.5646 - val_loss: 0.5370\n",
      "Epoch 18/100\n",
      "197/197 [==============================] - 0s 778us/step - loss: 0.5575 - val_loss: 0.5396\n",
      "Epoch 19/100\n",
      "197/197 [==============================] - 0s 781us/step - loss: 0.5637 - val_loss: 0.5496\n",
      "Epoch 20/100\n",
      "197/197 [==============================] - 0s 803us/step - loss: 0.5551 - val_loss: 0.5293\n",
      "Epoch 21/100\n",
      "197/197 [==============================] - 0s 932us/step - loss: 0.5456 - val_loss: 0.5442\n",
      "Epoch 22/100\n",
      "197/197 [==============================] - 0s 888us/step - loss: 0.5454 - val_loss: 0.5334\n",
      "Epoch 23/100\n",
      "197/197 [==============================] - 0s 777us/step - loss: 0.5491 - val_loss: 0.5301\n",
      "Epoch 24/100\n",
      "197/197 [==============================] - 0s 773us/step - loss: 0.5346 - val_loss: 0.5521\n",
      "Epoch 25/100\n",
      "197/197 [==============================] - 0s 932us/step - loss: 0.5300 - val_loss: 0.5196\n",
      "Epoch 26/100\n",
      "197/197 [==============================] - 0s 819us/step - loss: 0.5230 - val_loss: 0.5267\n",
      "Epoch 27/100\n",
      "197/197 [==============================] - 0s 797us/step - loss: 0.5298 - val_loss: 0.5168\n",
      "Epoch 28/100\n",
      "197/197 [==============================] - 0s 874us/step - loss: 0.5197 - val_loss: 0.5120\n",
      "Epoch 29/100\n",
      "197/197 [==============================] - 0s 884us/step - loss: 0.5176 - val_loss: 0.5118\n",
      "Epoch 30/100\n",
      "197/197 [==============================] - 0s 858us/step - loss: 0.5181 - val_loss: 0.5159\n",
      "Epoch 31/100\n",
      "197/197 [==============================] - 0s 852us/step - loss: 0.5283 - val_loss: 0.5243\n",
      "Epoch 32/100\n",
      "197/197 [==============================] - 0s 805us/step - loss: 0.5156 - val_loss: 0.5102\n",
      "Epoch 33/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5101 - val_loss: 0.5133\n",
      "Epoch 34/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.5104\n",
      "Epoch 35/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5137 - val_loss: 0.5091\n",
      "Epoch 36/100\n",
      "197/197 [==============================] - 0s 961us/step - loss: 0.5187 - val_loss: 0.5166\n",
      "Epoch 37/100\n",
      "197/197 [==============================] - 0s 871us/step - loss: 0.5149 - val_loss: 0.5188\n",
      "Epoch 38/100\n",
      "197/197 [==============================] - 0s 830us/step - loss: 0.5119 - val_loss: 0.5082\n",
      "Epoch 39/100\n",
      "197/197 [==============================] - 0s 898us/step - loss: 0.5116 - val_loss: 0.5058\n",
      "Epoch 40/100\n",
      "197/197 [==============================] - 0s 889us/step - loss: 0.5111 - val_loss: 0.5098\n",
      "Epoch 41/100\n",
      "197/197 [==============================] - 0s 905us/step - loss: 0.5178 - val_loss: 0.5100\n",
      "Epoch 42/100\n",
      "197/197 [==============================] - 0s 981us/step - loss: 0.5107 - val_loss: 0.5159\n",
      "Epoch 43/100\n",
      "197/197 [==============================] - 0s 878us/step - loss: 0.5110 - val_loss: 0.5135\n",
      "Epoch 44/100\n",
      "197/197 [==============================] - 0s 815us/step - loss: 0.5085 - val_loss: 0.5073\n",
      "Epoch 45/100\n",
      "197/197 [==============================] - 0s 793us/step - loss: 0.5124 - val_loss: 0.5201\n",
      "Epoch 46/100\n",
      "197/197 [==============================] - 0s 826us/step - loss: 0.5128 - val_loss: 0.5097\n",
      "Epoch 47/100\n",
      "197/197 [==============================] - 0s 846us/step - loss: 0.5126 - val_loss: 0.5061\n",
      "Epoch 48/100\n",
      "197/197 [==============================] - 0s 800us/step - loss: 0.5187 - val_loss: 0.5107\n",
      "Epoch 49/100\n",
      "197/197 [==============================] - 0s 848us/step - loss: 0.5080 - val_loss: 0.5130\n",
      "Epoch 50/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.5103 - val_loss: 0.5123\n",
      "Epoch 51/100\n",
      "197/197 [==============================] - 0s 950us/step - loss: 0.5098 - val_loss: 0.5050\n",
      "Epoch 52/100\n",
      "197/197 [==============================] - 0s 825us/step - loss: 0.5107 - val_loss: 0.5167\n",
      "Epoch 53/100\n",
      "197/197 [==============================] - 0s 801us/step - loss: 0.5102 - val_loss: 0.5107\n",
      "Epoch 54/100\n",
      "197/197 [==============================] - 0s 805us/step - loss: 0.5064 - val_loss: 0.5075\n",
      "Epoch 55/100\n",
      "197/197 [==============================] - 0s 812us/step - loss: 0.5066 - val_loss: 0.5089\n",
      "Epoch 56/100\n",
      "197/197 [==============================] - 0s 820us/step - loss: 0.5088 - val_loss: 0.5072\n",
      "Epoch 57/100\n",
      "197/197 [==============================] - 0s 802us/step - loss: 0.5102 - val_loss: 0.5093\n",
      "Epoch 58/100\n",
      "197/197 [==============================] - 0s 817us/step - loss: 0.5109 - val_loss: 0.5044\n",
      "Epoch 59/100\n",
      "197/197 [==============================] - 0s 804us/step - loss: 0.5118 - val_loss: 0.5056\n",
      "Epoch 60/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.5071 - val_loss: 0.5055\n",
      "Epoch 61/100\n",
      "197/197 [==============================] - 0s 816us/step - loss: 0.5061 - val_loss: 0.5067\n",
      "Epoch 62/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5090 - val_loss: 0.5054\n",
      "Epoch 63/100\n",
      "197/197 [==============================] - 0s 944us/step - loss: 0.5081 - val_loss: 0.5127\n",
      "Epoch 64/100\n",
      "197/197 [==============================] - 0s 935us/step - loss: 0.5104 - val_loss: 0.5086\n",
      "Epoch 65/100\n",
      "197/197 [==============================] - 0s 963us/step - loss: 0.5069 - val_loss: 0.5035\n",
      "Epoch 66/100\n",
      "197/197 [==============================] - 0s 897us/step - loss: 0.5046 - val_loss: 0.5057\n",
      "Epoch 67/100\n",
      "197/197 [==============================] - 0s 810us/step - loss: 0.5072 - val_loss: 0.5049\n",
      "Epoch 68/100\n",
      "197/197 [==============================] - 0s 814us/step - loss: 0.5075 - val_loss: 0.5067\n",
      "Epoch 69/100\n",
      "197/197 [==============================] - 0s 802us/step - loss: 0.5157 - val_loss: 0.5071\n",
      "Epoch 70/100\n",
      "197/197 [==============================] - 0s 802us/step - loss: 0.5039 - val_loss: 0.5033\n",
      "Epoch 71/100\n",
      "197/197 [==============================] - 0s 978us/step - loss: 0.5044 - val_loss: 0.5081\n",
      "Epoch 72/100\n",
      "197/197 [==============================] - 0s 837us/step - loss: 0.5066 - val_loss: 0.5036\n",
      "Epoch 73/100\n",
      "197/197 [==============================] - 0s 829us/step - loss: 0.5048 - val_loss: 0.5040\n",
      "Epoch 74/100\n",
      "197/197 [==============================] - 0s 895us/step - loss: 0.5051 - val_loss: 0.5039\n",
      "Epoch 75/100\n",
      "197/197 [==============================] - 0s 946us/step - loss: 0.5081 - val_loss: 0.5082\n",
      "Epoch 76/100\n",
      "197/197 [==============================] - 0s 828us/step - loss: 0.5049 - val_loss: 0.5008\n",
      "Epoch 77/100\n",
      "197/197 [==============================] - 0s 834us/step - loss: 0.5060 - val_loss: 0.5051\n",
      "Epoch 78/100\n",
      "197/197 [==============================] - 0s 822us/step - loss: 0.5037 - val_loss: 0.5036\n",
      "Epoch 79/100\n",
      "197/197 [==============================] - 0s 807us/step - loss: 0.5052 - val_loss: 0.5038\n",
      "Epoch 80/100\n",
      "197/197 [==============================] - 0s 810us/step - loss: 0.5136 - val_loss: 0.5151\n",
      "Epoch 81/100\n",
      "197/197 [==============================] - 0s 961us/step - loss: 0.5060 - val_loss: 0.5019\n",
      "Epoch 82/100\n",
      "197/197 [==============================] - 0s 887us/step - loss: 0.5024 - val_loss: 0.5039\n",
      "Epoch 83/100\n",
      "197/197 [==============================] - 0s 803us/step - loss: 0.5073 - val_loss: 0.5095\n",
      "Epoch 84/100\n",
      "197/197 [==============================] - 0s 843us/step - loss: 0.5039 - val_loss: 0.5025\n",
      "Epoch 85/100\n",
      "197/197 [==============================] - 0s 835us/step - loss: 0.5058 - val_loss: 0.5058\n",
      "Epoch 86/100\n",
      "197/197 [==============================] - 0s 821us/step - loss: 0.5078 - val_loss: 0.5063\n",
      "Epoch 87/100\n",
      "197/197 [==============================] - 0s 861us/step - loss: 0.5039 - val_loss: 0.5040\n",
      "Epoch 88/100\n",
      "197/197 [==============================] - 0s 860us/step - loss: 0.5038 - val_loss: 0.5005\n",
      "Epoch 89/100\n",
      "197/197 [==============================] - 0s 916us/step - loss: 0.5033 - val_loss: 0.5061\n",
      "Epoch 90/100\n",
      "197/197 [==============================] - 0s 848us/step - loss: 0.5050 - val_loss: 0.4999\n",
      "Epoch 91/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5083 - val_loss: 0.5099\n",
      "Epoch 92/100\n",
      "197/197 [==============================] - 0s 899us/step - loss: 0.5036 - val_loss: 0.5014\n",
      "Epoch 93/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5044 - val_loss: 0.5031\n",
      "Epoch 94/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5038 - val_loss: 0.5016\n",
      "Epoch 95/100\n",
      "197/197 [==============================] - 0s 869us/step - loss: 0.5031 - val_loss: 0.5019\n",
      "Epoch 96/100\n",
      "197/197 [==============================] - 0s 815us/step - loss: 0.5050 - val_loss: 0.5034\n",
      "Epoch 97/100\n",
      "197/197 [==============================] - 0s 828us/step - loss: 0.5132 - val_loss: 0.5051\n",
      "Epoch 98/100\n",
      "197/197 [==============================] - 0s 787us/step - loss: 0.5038 - val_loss: 0.5005\n",
      "Epoch 99/100\n",
      "197/197 [==============================] - 0s 797us/step - loss: 0.5005 - val_loss: 0.5020\n",
      "Epoch 100/100\n",
      "197/197 [==============================] - 0s 855us/step - loss: 0.5013 - val_loss: 0.5031\n",
      "219/219 [==============================] - 0s 492us/step\n",
      "94/94 [==============================] - 0s 476us/step\n",
      "current noise type: None\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "best param: None\n",
      "best param: None\n",
      "41/41 [==============================] - 0s 515us/step\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 21)]              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_11 (TFOpLa  (None, 32)               0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " tf.math.pow_11 (TFOpLambda)  (None, 32)               0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_11 (TFOp  (None,)                  0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.math.reduce_mean_11 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " tf.__operators__.add_11 (TF  ()                       0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " add_loss_11 (AddLoss)       ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,392\n",
      "Trainable params: 3,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 4.2048 - val_loss: 2.8086\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 2.5008 - val_loss: 2.0948\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.9348 - val_loss: 1.6905\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.5880 - val_loss: 1.3920\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.3315 - val_loss: 1.1962\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.1521 - val_loss: 1.0539\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.0168 - val_loss: 0.9327\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.9055 - val_loss: 0.8375\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.8118 - val_loss: 0.7614\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.7392 - val_loss: 0.6977\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6812 - val_loss: 0.6451\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6312 - val_loss: 0.6013\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5888 - val_loss: 0.5640\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5513 - val_loss: 0.5312\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5165 - val_loss: 0.4901\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4837 - val_loss: 0.4603\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4563 - val_loss: 0.4343\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4312 - val_loss: 0.4108\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.3921\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3910 - val_loss: 0.3778\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3748 - val_loss: 0.3644\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.3486\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3392\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3349 - val_loss: 0.3284\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3209\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3155\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3102 - val_loss: 0.3055\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3039 - val_loss: 0.3005\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2979 - val_loss: 0.2983\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2929 - val_loss: 0.2923\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2881 - val_loss: 0.2901\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2833 - val_loss: 0.2850\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2791 - val_loss: 0.2810\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2756 - val_loss: 0.2778\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2733 - val_loss: 0.2785\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2701 - val_loss: 0.2719\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2675 - val_loss: 0.2689\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2649 - val_loss: 0.2680\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2626 - val_loss: 0.2640\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2608 - val_loss: 0.2639\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2580 - val_loss: 0.2612\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2558 - val_loss: 0.2605\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2547 - val_loss: 0.2622\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2536 - val_loss: 0.2557\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2522 - val_loss: 0.2551\n",
      "Epoch 46/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2511 - val_loss: 0.2522\n",
      "Epoch 47/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2491 - val_loss: 0.2521\n",
      "Epoch 48/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2483 - val_loss: 0.2506\n",
      "Epoch 49/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2472 - val_loss: 0.2493\n",
      "Epoch 50/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2462 - val_loss: 0.2488\n",
      "Epoch 51/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2458 - val_loss: 0.2467\n",
      "Epoch 52/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2450 - val_loss: 0.2485\n",
      "Epoch 53/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2442 - val_loss: 0.2475\n",
      "Epoch 54/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2431 - val_loss: 0.2453\n",
      "Epoch 55/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2416 - val_loss: 0.2456\n",
      "Epoch 56/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2417 - val_loss: 0.2433\n",
      "Epoch 57/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2414 - val_loss: 0.2433\n",
      "Epoch 58/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2408 - val_loss: 0.2433\n",
      "Epoch 59/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2404 - val_loss: 0.2425\n",
      "Epoch 60/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2390 - val_loss: 0.2421\n",
      "Epoch 61/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2377 - val_loss: 0.2403\n",
      "Epoch 62/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2369 - val_loss: 0.2386\n",
      "Epoch 63/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2366 - val_loss: 0.2405\n",
      "Epoch 64/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2363 - val_loss: 0.2411\n",
      "Epoch 65/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2361 - val_loss: 0.2392\n",
      "Epoch 66/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2355 - val_loss: 0.2377\n",
      "Epoch 67/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2355 - val_loss: 0.2384\n",
      "Epoch 68/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2362 - val_loss: 0.2390\n",
      "Epoch 69/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2344 - val_loss: 0.2369\n",
      "Epoch 70/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2336 - val_loss: 0.2370\n",
      "Epoch 71/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2332 - val_loss: 0.2373\n",
      "Epoch 72/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2338 - val_loss: 0.2358\n",
      "Epoch 73/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2325 - val_loss: 0.2356\n",
      "Epoch 74/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2337 - val_loss: 0.2380\n",
      "Epoch 75/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2331 - val_loss: 0.2350\n",
      "Epoch 76/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2329 - val_loss: 0.2356\n",
      "Epoch 77/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2331 - val_loss: 0.2335\n",
      "Epoch 78/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2322 - val_loss: 0.2338\n",
      "Epoch 79/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2307 - val_loss: 0.2329\n",
      "Epoch 80/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2298 - val_loss: 0.2315\n",
      "Epoch 81/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2299 - val_loss: 0.2345\n",
      "Epoch 82/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2304 - val_loss: 0.2309\n",
      "Epoch 83/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2311 - val_loss: 0.2327\n",
      "Epoch 84/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2295 - val_loss: 0.2321\n",
      "Epoch 85/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2300 - val_loss: 0.2316\n",
      "Epoch 86/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2286 - val_loss: 0.2300\n",
      "Epoch 87/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2289 - val_loss: 0.2300\n",
      "Epoch 88/100\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2290 - val_loss: 0.2298\n",
      "Epoch 89/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2276 - val_loss: 0.2318\n",
      "Epoch 90/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2280 - val_loss: 0.2287\n",
      "Epoch 91/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2269 - val_loss: 0.2279\n",
      "Epoch 92/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2260 - val_loss: 0.2287\n",
      "Epoch 93/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2270 - val_loss: 0.2287\n",
      "Epoch 94/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2273 - val_loss: 0.2301\n",
      "Epoch 95/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2263 - val_loss: 0.2292\n",
      "Epoch 96/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2262 - val_loss: 0.2263\n",
      "Epoch 97/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2248 - val_loss: 0.2302\n",
      "Epoch 98/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2261 - val_loss: 0.2279\n",
      "Epoch 99/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2258 - val_loss: 0.2276\n",
      "Epoch 100/100\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2254 - val_loss: 0.2296\n",
      "41/41 [==============================] - 0s 519us/step\n",
      "18/18 [==============================] - 0s 563us/step\n",
      "current noise type: None\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "best param: None\n",
      "best param: None\n",
      "47/47 [==============================] - 0s 544us/step\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 21)]              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_13 (TFOpLa  (None, 32)               0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " tf.math.pow_13 (TFOpLambda)  (None, 32)               0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_13 (TFOp  (None,)                  0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.math.reduce_mean_13 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " tf.__operators__.add_13 (TF  ()                       0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " add_loss_13 (AddLoss)       ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,392\n",
      "Trainable params: 3,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 4.0183 - val_loss: 2.5857\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 2.3837 - val_loss: 1.9612\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 1.8171 - val_loss: 1.5776\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 1.4608 - val_loss: 1.3060\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 1.2317 - val_loss: 1.1125\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 1.0542 - val_loss: 0.9573\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.9170 - val_loss: 0.8443\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.8144 - val_loss: 0.7547\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.7315 - val_loss: 0.6837\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.6605 - val_loss: 0.6203\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.6028 - val_loss: 0.5706\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.5540 - val_loss: 0.5224\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.5122 - val_loss: 0.4843\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.4776 - val_loss: 0.4534\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.4491 - val_loss: 0.4274\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4236 - val_loss: 0.4067\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.3881\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.3702\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3511\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3374\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.3337 - val_loss: 0.3264\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.3202 - val_loss: 0.3170\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.3089 - val_loss: 0.3073\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2999 - val_loss: 0.2961\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2918 - val_loss: 0.2923\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2855 - val_loss: 0.2857\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2792 - val_loss: 0.2831\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2738 - val_loss: 0.2777\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2693 - val_loss: 0.2716\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2644 - val_loss: 0.2670\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2605 - val_loss: 0.2657\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2570 - val_loss: 0.2643\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2536 - val_loss: 0.2587\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2506 - val_loss: 0.2574\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2470 - val_loss: 0.2529\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2441 - val_loss: 0.2506\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2427 - val_loss: 0.2465\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2397 - val_loss: 0.2458\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2372 - val_loss: 0.2434\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2362 - val_loss: 0.2402\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2345 - val_loss: 0.2388\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2325 - val_loss: 0.2378\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2317 - val_loss: 0.2390\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2293 - val_loss: 0.2363\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2285 - val_loss: 0.2339\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2272 - val_loss: 0.2341\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2258 - val_loss: 0.2315\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2251 - val_loss: 0.2313\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2256 - val_loss: 0.2287\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2234 - val_loss: 0.2275\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2229 - val_loss: 0.2278\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2221 - val_loss: 0.2280\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2210 - val_loss: 0.2254\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2211 - val_loss: 0.2301\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2198 - val_loss: 0.2264\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2189 - val_loss: 0.2257\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2192 - val_loss: 0.2244\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2178 - val_loss: 0.2237\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2181 - val_loss: 0.2207\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2171 - val_loss: 0.2269\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2165 - val_loss: 0.2219\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2163 - val_loss: 0.2204\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2164 - val_loss: 0.2197\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2156 - val_loss: 0.2191\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2147 - val_loss: 0.2173\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2137 - val_loss: 0.2167\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2132 - val_loss: 0.2171\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2124 - val_loss: 0.2177\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2128 - val_loss: 0.2169\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2129 - val_loss: 0.2177\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2120 - val_loss: 0.2153\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2104 - val_loss: 0.2141\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2126 - val_loss: 0.2177\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2117 - val_loss: 0.2144\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2099 - val_loss: 0.2119\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2088 - val_loss: 0.2126\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2099 - val_loss: 0.2145\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2101 - val_loss: 0.2118\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2083 - val_loss: 0.2122\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2078 - val_loss: 0.2096\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2075 - val_loss: 0.2097\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2077 - val_loss: 0.2084\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2066 - val_loss: 0.2118\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2089 - val_loss: 0.2177\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2122 - val_loss: 0.2121\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2062 - val_loss: 0.2114\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2071 - val_loss: 0.2107\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2060 - val_loss: 0.2082\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2057 - val_loss: 0.2069\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2062 - val_loss: 0.2084\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2058 - val_loss: 0.2086\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2065 - val_loss: 0.2194\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2095 - val_loss: 0.2191\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2087 - val_loss: 0.2068\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2063 - val_loss: 0.2056\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2063 - val_loss: 0.2122\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2081 - val_loss: 0.2057\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2051 - val_loss: 0.2052\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2039 - val_loss: 0.2053\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.2032 - val_loss: 0.2045\n",
      "47/47 [==============================] - 0s 548us/step\n",
      "20/20 [==============================] - 0s 556us/step\n",
      "subsampling for dataset 08_celeba...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 223, 'Anomalies Ratio(%)': 2.23}\n",
      "best param: None\n",
      "best param: None\n",
      "219/219 [==============================] - 0s 562us/step\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 39)]              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                2496      \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_15 (TFOpLa  (None, 32)               0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " tf.math.pow_15 (TFOpLambda)  (None, 32)               0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_15 (TFOp  (None,)                  0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.math.reduce_mean_15 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " tf.__operators__.add_15 (TF  ()                       0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " add_loss_15 (AddLoss)       ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,544\n",
      "Trainable params: 4,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 3.8653 - val_loss: 2.1411\n",
      "Epoch 2/100\n",
      "197/197 [==============================] - 0s 798us/step - loss: 1.5674 - val_loss: 1.1450\n",
      "Epoch 3/100\n",
      "197/197 [==============================] - 0s 838us/step - loss: 0.9245 - val_loss: 0.7799\n",
      "Epoch 4/100\n",
      "197/197 [==============================] - 0s 895us/step - loss: 0.6975 - val_loss: 0.6325\n",
      "Epoch 5/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5855 - val_loss: 0.5419\n",
      "Epoch 6/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.5059 - val_loss: 0.4766\n",
      "Epoch 7/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4538\n",
      "Epoch 8/100\n",
      "197/197 [==============================] - 0s 893us/step - loss: 0.4393 - val_loss: 0.4331\n",
      "Epoch 9/100\n",
      "197/197 [==============================] - 0s 814us/step - loss: 0.4294 - val_loss: 0.4245\n",
      "Epoch 10/100\n",
      "197/197 [==============================] - 0s 849us/step - loss: 0.4218 - val_loss: 0.4148\n",
      "Epoch 11/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.4118 - val_loss: 0.4090\n",
      "Epoch 12/100\n",
      "197/197 [==============================] - 0s 843us/step - loss: 0.4109 - val_loss: 0.4136\n",
      "Epoch 13/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.4133 - val_loss: 0.4080\n",
      "Epoch 14/100\n",
      "197/197 [==============================] - 0s 913us/step - loss: 0.4059 - val_loss: 0.4085\n",
      "Epoch 15/100\n",
      "197/197 [==============================] - 0s 861us/step - loss: 0.4094 - val_loss: 0.4023\n",
      "Epoch 16/100\n",
      "197/197 [==============================] - 0s 818us/step - loss: 0.4024 - val_loss: 0.4040\n",
      "Epoch 17/100\n",
      "197/197 [==============================] - 0s 806us/step - loss: 0.4165 - val_loss: 0.4347\n",
      "Epoch 18/100\n",
      "197/197 [==============================] - 0s 839us/step - loss: 0.4092 - val_loss: 0.4002\n",
      "Epoch 19/100\n",
      "197/197 [==============================] - 0s 784us/step - loss: 0.3982 - val_loss: 0.3987\n",
      "Epoch 20/100\n",
      "197/197 [==============================] - 0s 808us/step - loss: 0.3982 - val_loss: 0.4011\n",
      "Epoch 21/100\n",
      "197/197 [==============================] - 0s 824us/step - loss: 0.4037 - val_loss: 0.4023\n",
      "Epoch 22/100\n",
      "197/197 [==============================] - 0s 821us/step - loss: 0.3989 - val_loss: 0.3991\n",
      "Epoch 23/100\n",
      "197/197 [==============================] - 0s 791us/step - loss: 0.4008 - val_loss: 0.3975\n",
      "Epoch 24/100\n",
      "197/197 [==============================] - 0s 780us/step - loss: 0.4029 - val_loss: 0.4086\n",
      "Epoch 25/100\n",
      "197/197 [==============================] - 0s 799us/step - loss: 0.4014 - val_loss: 0.3995\n",
      "Epoch 26/100\n",
      "197/197 [==============================] - 0s 806us/step - loss: 0.3980 - val_loss: 0.4012\n",
      "Epoch 27/100\n",
      "197/197 [==============================] - 0s 824us/step - loss: 0.4006 - val_loss: 0.4078\n",
      "Epoch 28/100\n",
      "197/197 [==============================] - 0s 914us/step - loss: 0.4014 - val_loss: 0.3939\n",
      "Epoch 29/100\n",
      "197/197 [==============================] - 0s 858us/step - loss: 0.3955 - val_loss: 0.3960\n",
      "Epoch 30/100\n",
      "197/197 [==============================] - 0s 862us/step - loss: 0.3954 - val_loss: 0.4037\n",
      "Epoch 31/100\n",
      "197/197 [==============================] - 0s 813us/step - loss: 0.3986 - val_loss: 0.3932\n",
      "Epoch 32/100\n",
      "197/197 [==============================] - 0s 798us/step - loss: 0.3942 - val_loss: 0.3952\n",
      "Epoch 33/100\n",
      "197/197 [==============================] - 0s 929us/step - loss: 0.3953 - val_loss: 0.3929\n",
      "Epoch 34/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3958\n",
      "Epoch 35/100\n",
      "197/197 [==============================] - 0s 905us/step - loss: 0.4006 - val_loss: 0.4017\n",
      "Epoch 36/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.3935\n",
      "Epoch 37/100\n",
      "197/197 [==============================] - 0s 956us/step - loss: 0.3954 - val_loss: 0.3926\n",
      "Epoch 38/100\n",
      "197/197 [==============================] - 0s 823us/step - loss: 0.3909 - val_loss: 0.3892\n",
      "Epoch 39/100\n",
      "197/197 [==============================] - 0s 813us/step - loss: 0.3910 - val_loss: 0.3999\n",
      "Epoch 40/100\n",
      "197/197 [==============================] - 0s 797us/step - loss: 0.4025 - val_loss: 0.3951\n",
      "Epoch 41/100\n",
      "197/197 [==============================] - 0s 794us/step - loss: 0.3918 - val_loss: 0.3925\n",
      "Epoch 42/100\n",
      "197/197 [==============================] - 0s 865us/step - loss: 0.3911 - val_loss: 0.3907\n",
      "Epoch 43/100\n",
      "197/197 [==============================] - 0s 833us/step - loss: 0.3945 - val_loss: 0.3907\n",
      "Epoch 44/100\n",
      "197/197 [==============================] - 0s 804us/step - loss: 0.3893 - val_loss: 0.3901\n",
      "Epoch 45/100\n",
      "197/197 [==============================] - 0s 796us/step - loss: 0.3932 - val_loss: 0.3908\n",
      "Epoch 46/100\n",
      "197/197 [==============================] - 0s 828us/step - loss: 0.3927 - val_loss: 0.4037\n",
      "Epoch 47/100\n",
      "197/197 [==============================] - 0s 804us/step - loss: 0.3952 - val_loss: 0.3879\n",
      "Epoch 48/100\n",
      "197/197 [==============================] - 0s 813us/step - loss: 0.3890 - val_loss: 0.3899\n",
      "Epoch 49/100\n",
      "197/197 [==============================] - 0s 807us/step - loss: 0.3889 - val_loss: 0.3898\n",
      "Epoch 50/100\n",
      "197/197 [==============================] - 0s 816us/step - loss: 0.3901 - val_loss: 0.3902\n",
      "Epoch 51/100\n",
      "197/197 [==============================] - 0s 797us/step - loss: 0.3909 - val_loss: 0.3922\n",
      "Epoch 52/100\n",
      "197/197 [==============================] - 0s 793us/step - loss: 0.3903 - val_loss: 0.4029\n",
      "Epoch 53/100\n",
      "197/197 [==============================] - 0s 812us/step - loss: 0.3919 - val_loss: 0.3884\n",
      "Epoch 54/100\n",
      "197/197 [==============================] - 0s 834us/step - loss: 0.3874 - val_loss: 0.3873\n",
      "Epoch 55/100\n",
      "197/197 [==============================] - 0s 870us/step - loss: 0.3929 - val_loss: 0.3973\n",
      "Epoch 56/100\n",
      "197/197 [==============================] - 0s 881us/step - loss: 0.3941 - val_loss: 0.3878\n",
      "Epoch 57/100\n",
      "197/197 [==============================] - 0s 879us/step - loss: 0.3870 - val_loss: 0.3866\n",
      "Epoch 58/100\n",
      "197/197 [==============================] - 0s 873us/step - loss: 0.3870 - val_loss: 0.3883\n",
      "Epoch 59/100\n",
      "197/197 [==============================] - 0s 979us/step - loss: 0.3882 - val_loss: 0.3881\n",
      "Epoch 60/100\n",
      "197/197 [==============================] - 0s 974us/step - loss: 0.3912 - val_loss: 0.3909\n",
      "Epoch 61/100\n",
      "197/197 [==============================] - 0s 873us/step - loss: 0.3887 - val_loss: 0.3868\n",
      "Epoch 62/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.3897\n",
      "Epoch 63/100\n",
      "197/197 [==============================] - 0s 942us/step - loss: 0.3890 - val_loss: 0.3889\n",
      "Epoch 64/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.3868\n",
      "Epoch 65/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.3864\n",
      "Epoch 66/100\n",
      "197/197 [==============================] - 0s 922us/step - loss: 0.3866 - val_loss: 0.3889\n",
      "Epoch 67/100\n",
      "197/197 [==============================] - 0s 802us/step - loss: 0.3897 - val_loss: 0.3898\n",
      "Epoch 68/100\n",
      "197/197 [==============================] - 0s 857us/step - loss: 0.3905 - val_loss: 0.3930\n",
      "Epoch 69/100\n",
      "197/197 [==============================] - 0s 829us/step - loss: 0.3885 - val_loss: 0.3894\n",
      "Epoch 70/100\n",
      "197/197 [==============================] - 0s 877us/step - loss: 0.3906 - val_loss: 0.3872\n",
      "Epoch 71/100\n",
      "197/197 [==============================] - 0s 932us/step - loss: 0.3864 - val_loss: 0.3881\n",
      "Epoch 72/100\n",
      "197/197 [==============================] - 0s 876us/step - loss: 0.3906 - val_loss: 0.4030\n",
      "Epoch 73/100\n",
      "197/197 [==============================] - 0s 855us/step - loss: 0.3898 - val_loss: 0.3855\n",
      "Epoch 74/100\n",
      "197/197 [==============================] - 0s 830us/step - loss: 0.3864 - val_loss: 0.3862\n",
      "Epoch 75/100\n",
      "197/197 [==============================] - 0s 784us/step - loss: 0.3872 - val_loss: 0.3862\n",
      "Epoch 76/100\n",
      "197/197 [==============================] - 0s 832us/step - loss: 0.3866 - val_loss: 0.3893\n",
      "Epoch 77/100\n",
      "197/197 [==============================] - 0s 827us/step - loss: 0.3880 - val_loss: 0.3894\n",
      "Epoch 78/100\n",
      "197/197 [==============================] - 0s 858us/step - loss: 0.3902 - val_loss: 0.3868\n",
      "Epoch 79/100\n",
      "197/197 [==============================] - 0s 803us/step - loss: 0.3862 - val_loss: 0.3856\n",
      "Epoch 80/100\n",
      "197/197 [==============================] - 0s 809us/step - loss: 0.3858 - val_loss: 0.3847\n",
      "Epoch 81/100\n",
      "197/197 [==============================] - 0s 797us/step - loss: 0.3905 - val_loss: 0.3884\n",
      "Epoch 82/100\n",
      "197/197 [==============================] - 0s 812us/step - loss: 0.3886 - val_loss: 0.3865\n",
      "Epoch 83/100\n",
      "197/197 [==============================] - 0s 796us/step - loss: 0.3852 - val_loss: 0.3888\n",
      "Epoch 84/100\n",
      "197/197 [==============================] - 0s 790us/step - loss: 0.3891 - val_loss: 0.3921\n",
      "Epoch 85/100\n",
      "197/197 [==============================] - 0s 790us/step - loss: 0.3881 - val_loss: 0.3848\n",
      "Epoch 86/100\n",
      "197/197 [==============================] - 0s 811us/step - loss: 0.3842 - val_loss: 0.3878\n",
      "Epoch 87/100\n",
      "197/197 [==============================] - 0s 860us/step - loss: 0.3878 - val_loss: 0.3858\n",
      "Epoch 88/100\n",
      "197/197 [==============================] - 0s 839us/step - loss: 0.3900 - val_loss: 0.3900\n",
      "Epoch 89/100\n",
      "197/197 [==============================] - 0s 927us/step - loss: 0.3878 - val_loss: 0.3858\n",
      "Epoch 90/100\n",
      "197/197 [==============================] - 0s 842us/step - loss: 0.3858 - val_loss: 0.3856\n",
      "Epoch 91/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.3843\n",
      "Epoch 92/100\n",
      "197/197 [==============================] - 0s 889us/step - loss: 0.3881 - val_loss: 0.3901\n",
      "Epoch 93/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.3840\n",
      "Epoch 94/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.3917\n",
      "Epoch 95/100\n",
      "197/197 [==============================] - 0s 933us/step - loss: 0.3878 - val_loss: 0.3885\n",
      "Epoch 96/100\n",
      "197/197 [==============================] - 0s 811us/step - loss: 0.3879 - val_loss: 0.3922\n",
      "Epoch 97/100\n",
      "197/197 [==============================] - 0s 793us/step - loss: 0.3858 - val_loss: 0.3844\n",
      "Epoch 98/100\n",
      "197/197 [==============================] - 0s 832us/step - loss: 0.3855 - val_loss: 0.3877\n",
      "Epoch 99/100\n",
      "197/197 [==============================] - 0s 849us/step - loss: 0.3889 - val_loss: 0.3859\n",
      "Epoch 100/100\n",
      "197/197 [==============================] - 0s 910us/step - loss: 0.3848 - val_loss: 0.3849\n",
      "219/219 [==============================] - 0s 501us/step\n",
      "94/94 [==============================] - 0s 468us/step\n",
      "subsampling for dataset 09_census...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 616, 'Anomalies Ratio(%)': 6.16}\n",
      "best param: None\n",
      "best param: None\n",
      "219/219 [==============================] - 0s 530us/step\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 500)]             0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                32000     \n",
      "                                                                 \n",
      " net_output (Dense)          (None, 32)                2048      \n",
      "                                                                 \n",
      " tf.math.subtract_17 (TFOpLa  (None, 32)               0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " tf.math.pow_17 (TFOpLambda)  (None, 32)               0         \n",
      "                                                                 \n",
      " tf.math.reduce_sum_17 (TFOp  (None,)                  0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " tf.math.reduce_mean_17 (TFO  ()                       0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " tf.__operators__.add_17 (TF  ()                       0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " add_loss_17 (AddLoss)       ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,048\n",
      "Trainable params: 34,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 10.8119 - val_loss: 18.7082\n",
      "Epoch 2/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 5.1597 - val_loss: 46.8471\n",
      "Epoch 3/100\n",
      "197/197 [==============================] - 0s 856us/step - loss: 3.3399 - val_loss: 96.2502\n",
      "Epoch 4/100\n",
      "197/197 [==============================] - 0s 856us/step - loss: 2.4117 - val_loss: 170.3418\n",
      "Epoch 5/100\n",
      "197/197 [==============================] - 0s 925us/step - loss: 1.9169 - val_loss: 276.7769\n",
      "Epoch 6/100\n",
      "197/197 [==============================] - 0s 907us/step - loss: 1.6441 - val_loss: 441.3258\n",
      "Epoch 7/100\n",
      "197/197 [==============================] - 0s 843us/step - loss: 1.4882 - val_loss: 581.0657\n",
      "Epoch 8/100\n",
      "197/197 [==============================] - 0s 842us/step - loss: 1.3692 - val_loss: 859.4349\n",
      "Epoch 9/100\n",
      "197/197 [==============================] - 0s 827us/step - loss: 1.2576 - val_loss: 1231.6486\n",
      "Epoch 10/100\n",
      "197/197 [==============================] - 0s 849us/step - loss: 1.1349 - val_loss: 1510.1021\n",
      "Epoch 11/100\n",
      "197/197 [==============================] - 0s 879us/step - loss: 1.1028 - val_loss: 2044.1309\n",
      "Epoch 12/100\n",
      "197/197 [==============================] - 0s 848us/step - loss: 1.0658 - val_loss: 2580.2727\n",
      "Epoch 13/100\n",
      "197/197 [==============================] - 0s 829us/step - loss: 0.9969 - val_loss: 3348.2368\n",
      "Epoch 14/100\n",
      "197/197 [==============================] - 0s 982us/step - loss: 0.9526 - val_loss: 4358.1465\n",
      "Epoch 15/100\n",
      "197/197 [==============================] - 0s 835us/step - loss: 0.9108 - val_loss: 5228.1987\n",
      "Epoch 16/100\n",
      "197/197 [==============================] - 0s 841us/step - loss: 0.9182 - val_loss: 6230.8042\n",
      "Epoch 17/100\n",
      "197/197 [==============================] - 0s 833us/step - loss: 0.8836 - val_loss: 7333.6606\n",
      "Epoch 18/100\n",
      "197/197 [==============================] - 0s 848us/step - loss: 0.8801 - val_loss: 8487.1104\n",
      "Epoch 19/100\n",
      "197/197 [==============================] - 0s 851us/step - loss: 0.8720 - val_loss: 9034.4697\n",
      "Epoch 20/100\n",
      "197/197 [==============================] - 0s 855us/step - loss: 0.8799 - val_loss: 9681.4023\n",
      "Epoch 21/100\n",
      "197/197 [==============================] - 0s 869us/step - loss: 0.8543 - val_loss: 10452.7539\n",
      "Epoch 22/100\n",
      "197/197 [==============================] - 0s 866us/step - loss: 0.8533 - val_loss: 11760.1963\n",
      "Epoch 23/100\n",
      "197/197 [==============================] - 0s 843us/step - loss: 0.8644 - val_loss: 11820.7500\n",
      "Epoch 24/100\n",
      "197/197 [==============================] - 0s 837us/step - loss: 0.8526 - val_loss: 13091.0596\n",
      "Epoch 25/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8474 - val_loss: 13625.5615\n",
      "Epoch 26/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8389 - val_loss: 13917.1309\n",
      "Epoch 27/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8484 - val_loss: 12838.1592\n",
      "Epoch 28/100\n",
      "197/197 [==============================] - 0s 923us/step - loss: 0.8310 - val_loss: 13959.6523\n",
      "Epoch 29/100\n",
      "197/197 [==============================] - 0s 929us/step - loss: 0.8518 - val_loss: 13369.9062\n",
      "Epoch 30/100\n",
      "197/197 [==============================] - 0s 874us/step - loss: 0.8301 - val_loss: 15297.8906\n",
      "Epoch 31/100\n",
      "197/197 [==============================] - 0s 848us/step - loss: 0.8284 - val_loss: 16317.2061\n",
      "Epoch 32/100\n",
      "197/197 [==============================] - 0s 829us/step - loss: 0.8600 - val_loss: 15513.0088\n",
      "Epoch 33/100\n",
      "197/197 [==============================] - 0s 884us/step - loss: 0.8475 - val_loss: 16615.5645\n",
      "Epoch 34/100\n",
      "197/197 [==============================] - 0s 953us/step - loss: 0.8394 - val_loss: 17318.0684\n",
      "Epoch 35/100\n",
      "197/197 [==============================] - 0s 924us/step - loss: 0.8341 - val_loss: 18043.4648\n",
      "Epoch 36/100\n",
      "197/197 [==============================] - 0s 947us/step - loss: 0.8237 - val_loss: 18919.0996\n",
      "Epoch 37/100\n",
      "197/197 [==============================] - 0s 908us/step - loss: 0.8335 - val_loss: 19241.7871\n",
      "Epoch 38/100\n",
      "197/197 [==============================] - 0s 948us/step - loss: 0.8229 - val_loss: 18413.8457\n",
      "Epoch 39/100\n",
      "197/197 [==============================] - 0s 908us/step - loss: 0.8263 - val_loss: 17914.0312\n",
      "Epoch 40/100\n",
      "197/197 [==============================] - 0s 833us/step - loss: 0.8353 - val_loss: 14298.9600\n",
      "Epoch 41/100\n",
      "197/197 [==============================] - 0s 880us/step - loss: 0.8295 - val_loss: 15582.4443\n",
      "Epoch 42/100\n",
      "197/197 [==============================] - 0s 905us/step - loss: 0.8277 - val_loss: 16075.9434\n",
      "Epoch 43/100\n",
      "197/197 [==============================] - 0s 917us/step - loss: 0.8239 - val_loss: 17052.5117\n",
      "Epoch 44/100\n",
      "197/197 [==============================] - 0s 869us/step - loss: 0.8133 - val_loss: 19071.1953\n",
      "Epoch 45/100\n",
      "197/197 [==============================] - 0s 935us/step - loss: 0.8382 - val_loss: 18241.9551\n",
      "Epoch 46/100\n",
      "197/197 [==============================] - 0s 995us/step - loss: 0.8173 - val_loss: 18474.1641\n",
      "Epoch 47/100\n",
      "197/197 [==============================] - 0s 887us/step - loss: 0.8171 - val_loss: 19325.1797\n",
      "Epoch 48/100\n",
      "197/197 [==============================] - 0s 917us/step - loss: 0.8444 - val_loss: 18134.6504\n",
      "Epoch 49/100\n",
      "197/197 [==============================] - 0s 961us/step - loss: 0.8284 - val_loss: 16624.2051\n",
      "Epoch 50/100\n",
      "197/197 [==============================] - 0s 893us/step - loss: 0.8257 - val_loss: 17144.4238\n",
      "Epoch 51/100\n",
      "197/197 [==============================] - 0s 964us/step - loss: 0.8492 - val_loss: 19749.3691\n",
      "Epoch 52/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8279 - val_loss: 19900.1230\n",
      "Epoch 53/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8239 - val_loss: 20247.5762\n",
      "Epoch 54/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8221 - val_loss: 20082.7637\n",
      "Epoch 55/100\n",
      "197/197 [==============================] - 0s 992us/step - loss: 0.8243 - val_loss: 19130.0020\n",
      "Epoch 56/100\n",
      "197/197 [==============================] - 0s 892us/step - loss: 0.8183 - val_loss: 19088.9355\n",
      "Epoch 57/100\n",
      "197/197 [==============================] - 0s 927us/step - loss: 0.8196 - val_loss: 21054.2129\n",
      "Epoch 58/100\n",
      "197/197 [==============================] - 0s 865us/step - loss: 0.8368 - val_loss: 22776.1836\n",
      "Epoch 59/100\n",
      "197/197 [==============================] - 0s 878us/step - loss: 0.8337 - val_loss: 22086.0938\n",
      "Epoch 60/100\n",
      "197/197 [==============================] - 0s 939us/step - loss: 0.8183 - val_loss: 23189.6836\n",
      "Epoch 61/100\n",
      "197/197 [==============================] - 0s 991us/step - loss: 0.8148 - val_loss: 24981.7441\n",
      "Epoch 62/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8291 - val_loss: 24954.4941\n",
      "Epoch 63/100\n",
      "197/197 [==============================] - 0s 990us/step - loss: 0.8227 - val_loss: 23608.6016\n",
      "Epoch 64/100\n",
      "197/197 [==============================] - 0s 890us/step - loss: 0.8184 - val_loss: 23711.6641\n",
      "Epoch 65/100\n",
      "197/197 [==============================] - 0s 949us/step - loss: 0.8092 - val_loss: 23609.7793\n",
      "Epoch 66/100\n",
      "197/197 [==============================] - 0s 900us/step - loss: 0.8134 - val_loss: 23671.2441\n",
      "Epoch 67/100\n",
      "197/197 [==============================] - 0s 862us/step - loss: 0.8128 - val_loss: 23812.3223\n",
      "Epoch 68/100\n",
      "197/197 [==============================] - 0s 874us/step - loss: 0.8021 - val_loss: 24904.6582\n",
      "Epoch 69/100\n",
      "197/197 [==============================] - 0s 869us/step - loss: 0.8057 - val_loss: 23992.7871\n",
      "Epoch 70/100\n",
      "197/197 [==============================] - 0s 899us/step - loss: 0.7991 - val_loss: 24784.6543\n",
      "Epoch 71/100\n",
      "197/197 [==============================] - 0s 874us/step - loss: 0.8080 - val_loss: 25477.8887\n",
      "Epoch 72/100\n",
      "197/197 [==============================] - 0s 871us/step - loss: 0.8140 - val_loss: 27387.9141\n",
      "Epoch 73/100\n",
      "197/197 [==============================] - 0s 858us/step - loss: 0.7975 - val_loss: 25823.0684\n",
      "Epoch 74/100\n",
      "197/197 [==============================] - 0s 857us/step - loss: 0.8119 - val_loss: 25613.4746\n",
      "Epoch 75/100\n",
      "197/197 [==============================] - 0s 878us/step - loss: 0.8150 - val_loss: 26414.2188\n",
      "Epoch 76/100\n",
      "197/197 [==============================] - 0s 873us/step - loss: 0.8040 - val_loss: 26701.1699\n",
      "Epoch 77/100\n",
      "197/197 [==============================] - 0s 859us/step - loss: 0.7968 - val_loss: 27214.9004\n",
      "Epoch 78/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.7895 - val_loss: 26861.7422\n",
      "Epoch 79/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.7850 - val_loss: 27844.0430\n",
      "Epoch 80/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8156 - val_loss: 27408.6113\n",
      "Epoch 81/100\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.8182 - val_loss: 24977.3379\n",
      "Epoch 82/100\n",
      "197/197 [==============================] - 0s 998us/step - loss: 0.8175 - val_loss: 29277.7207\n",
      "Epoch 83/100\n",
      "197/197 [==============================] - 0s 879us/step - loss: 0.8001 - val_loss: 27391.2559\n",
      "Epoch 84/100\n",
      "197/197 [==============================] - 0s 863us/step - loss: 0.7962 - val_loss: 28515.5020\n",
      "Epoch 85/100\n",
      "197/197 [==============================] - 0s 854us/step - loss: 0.7966 - val_loss: 26275.3496\n",
      "Epoch 86/100\n",
      "197/197 [==============================] - 0s 948us/step - loss: 0.8002 - val_loss: 27095.1309\n",
      "Epoch 87/100\n",
      "197/197 [==============================] - 0s 861us/step - loss: 0.8039 - val_loss: 27527.8496\n",
      "Epoch 88/100\n",
      "197/197 [==============================] - 0s 852us/step - loss: 0.7998 - val_loss: 27349.2109\n",
      "Epoch 89/100\n",
      "197/197 [==============================] - 0s 856us/step - loss: 0.7961 - val_loss: 26769.8926\n",
      "Epoch 90/100\n",
      "197/197 [==============================] - 0s 869us/step - loss: 0.7909 - val_loss: 29594.2949\n",
      "Epoch 91/100\n",
      "197/197 [==============================] - 0s 875us/step - loss: 0.7864 - val_loss: 29069.1016\n",
      "Epoch 92/100\n",
      "197/197 [==============================] - 0s 854us/step - loss: 0.8081 - val_loss: 29434.8516\n",
      "Epoch 93/100\n",
      "197/197 [==============================] - 0s 847us/step - loss: 0.8029 - val_loss: 29481.1855\n",
      "Epoch 94/100\n",
      "197/197 [==============================] - 0s 842us/step - loss: 0.8099 - val_loss: 29595.3730\n",
      "Epoch 95/100\n",
      "197/197 [==============================] - 0s 855us/step - loss: 0.8062 - val_loss: 29552.2012\n",
      "Epoch 96/100\n",
      "197/197 [==============================] - 0s 883us/step - loss: 0.7823 - val_loss: 30215.0938\n",
      "Epoch 97/100\n",
      "197/197 [==============================] - 0s 974us/step - loss: 0.7919 - val_loss: 30958.2188\n",
      "Epoch 98/100\n",
      "197/197 [==============================] - 0s 973us/step - loss: 0.8044 - val_loss: 31654.1562\n",
      "Epoch 99/100\n",
      "197/197 [==============================] - 0s 871us/step - loss: 0.8263 - val_loss: 29717.4766\n",
      "Epoch 100/100\n",
      "197/197 [==============================] - 0s 862us/step - loss: 0.8081 - val_loss: 28281.7637\n",
      "219/219 [==============================] - 0s 518us/step\n",
      "94/94 [==============================] - 0s 517us/step\n"
     ]
    }
   ],
   "source": [
    "# seed for reproducible results\n",
    "seed = 42\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    \"\"\"\n",
    "    la: ratio of labeled anomalies, from 0.0 to 1.0\n",
    "    realistic_synthetic_mode: types of synthetic anomalies, can be local, global, dependency or cluster\n",
    "    noise_type: inject data noises for testing model robustness, can be duplicated_anomalies, irrelevant_features or label_contamination\n",
    "    \"\"\"\n",
    "\n",
    "    # import the dataset\n",
    "    datagenerator.dataset = dataset  # specify the dataset name\n",
    "    data = datagenerator.generator(\n",
    "        la=0.1, realistic_synthetic_mode=None, noise_type=None\n",
    "    )  # only 10% labeled anomalies are available\n",
    "\n",
    "    for name, clf in model_dict.items():\n",
    "        # model initialization\n",
    "        clf = clf(seed=seed, model_name=name)\n",
    "\n",
    "        # training, for unsupervised models the y label will be discarded\n",
    "        clf = clf.fit(X_train=data[\"X_train\"], y_train=data[\"y_train\"])\n",
    "\n",
    "        # output predicted anomaly score on testing set\n",
    "        score = clf.predict_score(data[\"X_test\"])\n",
    "\n",
    "        # evaluation\n",
    "        result = utils.metric(y_true=data[\"y_test\"], y_score=score)\n",
    "\n",
    "        # save results\n",
    "        df_AUCROC.loc[dataset, name] = result[\"aucroc\"]\n",
    "        df_AUCPR.loc[dataset, name] = result[\"aucpr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:50:14.555117Z",
     "start_time": "2022-07-08T07:50:14.511234Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IForest</th>\n",
       "      <th>DeepSVDD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01_ALOI</th>\n",
       "      <td>0.496581</td>\n",
       "      <td>0.47864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02_annthyroid</th>\n",
       "      <td>0.826387</td>\n",
       "      <td>0.75085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03_backdoor</th>\n",
       "      <td>0.737342</td>\n",
       "      <td>0.58035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04_breastw</th>\n",
       "      <td>0.979938</td>\n",
       "      <td>0.484761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05_campaign</th>\n",
       "      <td>0.696507</td>\n",
       "      <td>0.571301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06_cardio</th>\n",
       "      <td>0.944193</td>\n",
       "      <td>0.680346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07_Cardiotocography</th>\n",
       "      <td>0.708773</td>\n",
       "      <td>0.484776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08_celeba</th>\n",
       "      <td>0.736814</td>\n",
       "      <td>0.38509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09_census</th>\n",
       "      <td>0.620219</td>\n",
       "      <td>0.511897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      IForest  DeepSVDD\n",
       "01_ALOI              0.496581   0.47864\n",
       "02_annthyroid        0.826387   0.75085\n",
       "03_backdoor          0.737342   0.58035\n",
       "04_breastw           0.979938  0.484761\n",
       "05_campaign          0.696507  0.571301\n",
       "06_cardio            0.944193  0.680346\n",
       "07_Cardiotocography  0.708773  0.484776\n",
       "08_celeba            0.736814   0.38509\n",
       "09_census            0.620219  0.511897"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AUCROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:50:14.587032Z",
     "start_time": "2022-07-08T07:50:14.560106Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IForest</th>\n",
       "      <th>DeepSVDD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01_ALOI</th>\n",
       "      <td>0.031745</td>\n",
       "      <td>0.031912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02_annthyroid</th>\n",
       "      <td>0.353499</td>\n",
       "      <td>0.205219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03_backdoor</th>\n",
       "      <td>0.050942</td>\n",
       "      <td>0.366501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04_breastw</th>\n",
       "      <td>0.954298</td>\n",
       "      <td>0.382458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05_campaign</th>\n",
       "      <td>0.249056</td>\n",
       "      <td>0.198592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06_cardio</th>\n",
       "      <td>0.615718</td>\n",
       "      <td>0.312177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07_Cardiotocography</th>\n",
       "      <td>0.478403</td>\n",
       "      <td>0.299519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08_celeba</th>\n",
       "      <td>0.094863</td>\n",
       "      <td>0.050561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09_census</th>\n",
       "      <td>0.075204</td>\n",
       "      <td>0.069327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      IForest  DeepSVDD\n",
       "01_ALOI              0.031745  0.031912\n",
       "02_annthyroid        0.353499  0.205219\n",
       "03_backdoor          0.050942  0.366501\n",
       "04_breastw           0.954298  0.382458\n",
       "05_campaign          0.249056  0.198592\n",
       "06_cardio            0.615718  0.312177\n",
       "07_Cardiotocography  0.478403  0.299519\n",
       "08_celeba            0.094863  0.050561\n",
       "09_census            0.075204  0.069327"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AUCPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
