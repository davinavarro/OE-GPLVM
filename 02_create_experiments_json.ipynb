{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6c99cc-3f7e-49a2-80bc-10524cdf11f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c9a38cf9-72e5-4790-93d1-8ff46db4afdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"experiments/dataset_infos.json\", \"r\") as json_file:\n",
    "    datasets = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6660003a-9e75-4041-80e3-f9896c1fe2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_list = datasets\n",
    "## Create a new dictionary with the desired structure\n",
    "# dataset_dict = {}\n",
    "# for data_dict in data_list:\n",
    "#    if 'dataset' in data_dict:\n",
    "#        dataset_name = data_dict['dataset']\n",
    "#        del data_dict['dataset']\n",
    "#        dataset_dict[dataset_name] = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "30ec620a-5160-4f07-8490-d93b642dd196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"dataset_infos.json\", \"w\") as json_file:\n",
    "#    json.dump(dataset_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "44422f44-d5a3-4ef8-ba42-4444208d720a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "list_datasets = []\n",
    "for dataset in os.listdir(\"datasets/Classical\"):\n",
    "    list_datasets.append(dataset.replace(\".npz\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6947434e-847d-47f6-9b05-63adbb74b3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dictionaries_with_dependency(input_dict, dependency_rule):\n",
    "    import itertools\n",
    "\n",
    "    # Separate the keys and values, and prepare for combinations excluding the dependent key\n",
    "    keys, values = zip(\n",
    "        *((k, v) for k, v in input_dict.items())\n",
    "    )  # if k != \"latent_dim\"))\n",
    "    combinations = list(itertools.product(*values))\n",
    "\n",
    "    # Create dictionaries with combinations of values and add the dependent key-value pair\n",
    "    result_dicts = []\n",
    "    for combination in combinations:\n",
    "        new_dict = dict(zip(keys, combination))\n",
    "        #    # Apply the dependency rule to determine the value of the dependent key\n",
    "        #    new_dict[\"latent_dim\"] = dependency_rule(new_dict[\"dataset\"])\n",
    "        result_dicts.append(new_dict)\n",
    "\n",
    "    return result_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c15d4a4d-ccf0-4e25-8937-ec21c476898a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a rule for the dependency of 'key4' on the value of 'key1'\n",
    "import math\n",
    "\n",
    "\n",
    "def dependency_rule(value_of_key1: str):\n",
    "    dataset = value_of_key1.split(\"_\")[1]\n",
    "    n_features = datasets[dataset][\"n_features\"]\n",
    "    if n_features > 30:\n",
    "        latent_dim = 15\n",
    "    elif n_features >= 4 and n_features <= 30:\n",
    "        latent_dim = n_features / 2\n",
    "    else:\n",
    "        latent_dim = 2\n",
    "    return int(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "91e1d492-9c6c-4a40-8572-c95b7a8d1a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predefined_lists_with_dependency = {\n",
    "    \"dataset\": list_datasets,\n",
    "    \"kernel\": [\"rbf\"],\n",
    "    \"batch_size\": [\"128\"],\n",
    "    \"learning_rate\": [\"0.01\"],\n",
    "    # \"latent_dim\": [\"2\",\"5\"],\n",
    "    \"loss\": [\"normal\"],  # \"loe_hard\", \"loe_soft\"],\n",
    "    \"anomaly_type\": [\"normal\"],  # ,\"global\", \"local\", \"dependency\", \"cluster\"],\n",
    "    \"noise_type\": [\"normal\"],  # ,\"irrelevant_features\", \"duplicated_anomalies\"],\n",
    "    \"noise_ratio\": [\"0\"],  # \"0.01\",\"0.05\",\"0.1\",\"0.25\",\"0.50\",],\n",
    "    \"layers\": [\"5,5\", \"10,10\", \"15,15\", \"10,10,10\"],\n",
    "    \"n_inducing\": [\"25\", \"50\", \"100\"],\n",
    "    \"n_epochs\": [\"1000\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c4265f9e-aeca-467f-a5c2-e1fc36c91b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = generate_dictionaries_with_dependency(\n",
    "    predefined_lists_with_dependency, dependency_rule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bf55195e-7d1a-42bf-8eba-00f26eab2322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    dataset_name = experiment[\"dataset\"].split(\"_\")[1]\n",
    "    experiment.update(datasets[dataset_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cd443ca7-cede-409f-9d7e-da11252cdd99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"experiments/refine/001_normal_study.json\", \"w\") as json_file:\n",
    "    json.dump(experiments, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bd76ae1f-8649-44c5-bd74-3be91959927c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    if experiment[\"latent_dim\"] >=4:\n",
    "        experiment[\"latent_dim\"] = int(experiment[\"latent_dim\"]/2)\n",
    "    elif experiment[\"latent_dim\"] == 3:\n",
    "        experiment[\"latent_dim\"] = 2\n",
    "    else:\n",
    "        experiment[\"latent_dim\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bf02a8e0-6a13-4719-aa0d-2b76127bbc22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"experiments/refine/001_normal_study_latent_quarter.json\", \"w\") as json_file:\n",
    "    json.dump(experiments, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e14be0c2-e505-4473-8915-8b99499644c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(experiments).sort_values(\"latent_dim\")[\"latent_dim\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5ed79965-5ff6-4833-864c-4b4bbb037c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"experiments/refine/001_normal_study_latent_half.json\") as json_file:\n",
    "    exp_half = json.load(json_file)\n",
    "with open(\"experiments/refine/001_normal_study_latent_quarter.json\") as json_file:\n",
    "    exp_quarter = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3b6efd15-7eca-46b0-8aed-b311b6b5ecda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = exp_half + exp_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7e6fa675-2c5d-4584-b06c-8ca855baa61d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(experiments).sort_values(by = [\"dataset\", \"latent_dim\"] ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "96608ed0-aee8-41e4-9205-b632012aed76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[['dataset', 'n_samples', 'n_features', 'n_anomaly', 'pct_anomaly',\n",
    "       'domain','kernel', 'batch_size', 'learning_rate','loss',\n",
    "       'anomaly_type', 'noise_type', 'noise_ratio', 'latent_dim', 'layers', 'n_inducing',\n",
    "       'n_epochs',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7e780730-29d6-4812-a6d8-00b5ebf87c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_json(\"experiments/refine/001_complete_normal_study.json\", orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199786a-7463-4556-8c95-1d6a990c9fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
