{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install prettytable\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import the necessary package\n",
    "from baseline.OE_GPLVM.aeb_gplvm import AEB_GPLVM, NNEncoder, DataInput, DataOutput, Metrics, Parameters\n",
    "from baseline.OE_GPLVM.train import *\n",
    "from baseline.OE_GPLVM.utils import *\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from torch.distributions import kl_divergence\n",
    "from gpytorch.priors import MultivariateNormalPrior\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.myutils import Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "plt.style.use(\"ggplot\")\n",
    "datagenerator = DataGenerator()  # data generator\n",
    "utils = Utils()  # utils function\n",
    "\n",
    "# dataset and model list / dict\n",
    "dataset_list = [\n",
    "    \"01_ALOI\",\n",
    "    \"02_annthyroid\",\n",
    "    \"03_backdoor\",\n",
    "    \"04_breastw\",\n",
    "    \"05_campaign\",\n",
    "    \"06_cardio\",\n",
    "    \"07_Cardiotocography\",\n",
    "    \"08_celeba\",\n",
    "    \"09_census\",\n",
    "    \"99_linear\",\n",
    "    \"99_circles\",\n",
    "    \"99_moons\",\n",
    "    \"99_clusters\",\n",
    "]\n",
    "dataset = dataset_list[-1]\n",
    "datagenerator.dataset = dataset\n",
    "labeled_anomalies = .1\n",
    "data = datagenerator.generator(la=labeled_anomalies, realistic_synthetic_mode=None, noise_type=None)\n",
    "\n",
    "Y_train = torch.tensor(data[\"X_train\"], dtype=torch.float32)\n",
    "Y_test = torch.tensor(data[\"X_test\"], dtype=torch.float32)\n",
    "lb_train = torch.tensor(data[\"y_train\"], dtype=torch.float32)\n",
    "lb_test = torch.tensor(data[\"y_test\"], dtype=torch.float32)\n",
    "ratio = data[\"y_test\"].sum() / data[\"y_test\"].shape[0]\n",
    "\n",
    "experiment = Experiment(\n",
    "    dataset,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    lb_train,\n",
    "    lb_test,\n",
    "    len(Y_train),\n",
    "    Y_train.shape[-2],\n",
    "    100,\n",
    "    3,\n",
    "    50,\n",
    "    100,\n",
    "    (5, 5),\n",
    "    0.005,\n",
    "    \"loe\",\n",
    "    \"hard\",\n",
    ")\n",
    "\n",
    "N = experiment.N\n",
    "data_dim = experiment.data_dim\n",
    "latent_dim = experiment.latent_dim\n",
    "n_inducing = experiment.n_inducing\n",
    "n_epochs = experiment.n_epochs\n",
    "nn_layers = experiment.nn_layers\n",
    "lr = experiment.lr\n",
    "method = experiment.method\n",
    "elbo_type = experiment.elbo\n",
    "batch_size = experiment.batch_size\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.subplot(111)\n",
    "plt.scatter(\n",
    "    data[\"X_train\"][:, 0][np.where(lb_train == 1)[0]],\n",
    "    data[\"X_train\"][:, 1][np.where(lb_train == 1)[0]],\n",
    "    label=\"Anomaly\",\n",
    ")\n",
    "plt.scatter(\n",
    "    data[\"X_train\"][:, 0][np.where(lb_train == 0)[0]],\n",
    "    data[\"X_train\"][:, 1][np.where(lb_train == 0)[0]],\n",
    "    label=\"Normal\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(Y_train)\n",
    "data_dim = Y_train.shape[1]\n",
    "nn_layers = (5, 5)\n",
    "n_inducing = 50\n",
    "latent_dim = 2\n",
    "lr = 0.01\n",
    "X_prior_mean = torch.zeros(n_train, latent_dim)\n",
    "X_prior_covar = torch.eye(X_prior_mean.shape[1])\n",
    "prior_x = MultivariateNormalPrior(X_prior_mean, X_prior_covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = NNEncoder(n_train, latent_dim, prior_x, data_dim, nn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEB_GPLVM(\n",
    "    n_train,\n",
    "    data_dim,\n",
    "    latent_dim,\n",
    "    n_inducing,\n",
    "    encoder,\n",
    "    nn_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\": model.parameters()},\n",
    "        {\"params\": likelihood.parameters()},\n",
    "    ],\n",
    "    lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = DataInput(\n",
    "    X_train=data[\"X_train\"].tolist(),\n",
    "    X_test=data[\"X_test\"].tolist(),\n",
    "    lb_train=data[\"y_train\"].tolist(),\n",
    "    lb_test=data[\"y_test\"].tolist(),\n",
    "    ratio=ratio,\n",
    "    labeled_anomalies=labeled_anomalies,\n",
    ")\n",
    "parameters = Parameters(\n",
    "    nn_layers=nn_layers,\n",
    "    nn_architeture=\"default\",\n",
    "    kernel=\"rbf\",\n",
    "    lr=0.01,\n",
    "    epoch=1000,\n",
    "    batch_size=128,\n",
    ")\n",
    "metrics = Metrics()\n",
    "data_output = DataOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(y_train):\n",
    "    idx_a = np.where(y_train == 1)[0]\n",
    "    idx_n = np.where(y_train == 0)[0]\n",
    "    ratio = len(idx_a) / (len(idx_a) + len(idx_n))\n",
    "    qtd_anomaly = int(ratio * batch_size)\n",
    "    qtd_normal = batch_size - qtd_anomaly\n",
    "    idx_n = torch.tensor(np.random.choice(idx_n, qtd_normal, replace=True))\n",
    "\n",
    "    if qtd_anomaly == 0:\n",
    "        idx_a = torch.tensor(np.random.choice(idx_n, qtd_anomaly, replace=True))\n",
    "    else:\n",
    "        idx_a = torch.tensor(np.random.choice(idx_a, qtd_anomaly, replace=True))\n",
    "\n",
    "    batch_index = torch.cat([idx_n, idx_a])\n",
    "\n",
    "    return idx_n, idx_a, batch_index, ratio\n",
    "\n",
    "\n",
    "def get_loe_index(X, indices):\n",
    "    ll_0, klu_0, kl_0, _ = calculate_terms(X, indices)\n",
    "    score = ll_0 - kl_0\n",
    "\n",
    "    qtd_normal = int(score.shape[0] * (1 - ratio))\n",
    "    qtd_anormal = batch_size - int(score.shape[0] * (1 - ratio))\n",
    "\n",
    "    _, loe_idx_n = torch.topk(score, qtd_normal, largest=True, sorted=False)\n",
    "    _, loe_idx_a = torch.topk(score, qtd_anormal, largest=False, sorted=False)\n",
    "    return indices[loe_idx_n], indices[loe_idx_a]\n",
    "\n",
    "\n",
    "def create_dist_qx(model, batch_target):\n",
    "    mu = model.predict_latent(batch_target)[0]\n",
    "    sigma = model.predict_latent(batch_target)[1]\n",
    "    local_q_x = MultivariateNormal(mu, sigma)\n",
    "    return mu, sigma, local_q_x\n",
    "\n",
    "\n",
    "def create_dist_prior(\n",
    "    batch_target,\n",
    "    mu,\n",
    "):\n",
    "    local_p_x_mean = torch.zeros(batch_target.shape[0], mu.shape[1])\n",
    "    local_p_x_covar = torch.eye(mu.shape[1])\n",
    "    local_p_x = MultivariateNormalPrior(local_p_x_mean, local_p_x_covar)\n",
    "    return local_p_x\n",
    "\n",
    "\n",
    "def kl_divergence_variational(target):\n",
    "    ll_shape = torch.zeros_like(target.T)\n",
    "    klu = (\n",
    "        ll_shape.T.add_(model.variational_strategy.kl_divergence().div(batch_size))\n",
    "        .sum(-1)\n",
    "        .T.div((n_train))\n",
    "    )\n",
    "    return klu\n",
    "\n",
    "\n",
    "def calculate_terms(X, indices):\n",
    "    batch_target = X[indices]\n",
    "    mu, sigma, local_q_x = create_dist_qx(model, batch_target)\n",
    "    local_p_x = create_dist_prior(batch_target, mu)\n",
    "    batch_output = model(model.sample_latent_variable(batch_target))\n",
    "    log_likelihood = (\n",
    "        likelihood.expected_log_prob(batch_target.T, batch_output)\n",
    "        .sum(0)\n",
    "        .div(batch_size)\n",
    "    )\n",
    "    kl_x = kl_divergence(local_q_x, local_p_x).div(n_train)\n",
    "    kl_u = kl_divergence_variational(batch_target)\n",
    "    log_marginal = (\n",
    "        likelihood.log_marginal(batch_target.T, batch_output).sum(0).div(batch_size)\n",
    "    )\n",
    "    return log_likelihood, kl_u, kl_x, log_marginal\n",
    "\n",
    "\n",
    "def predict_score(X_test):\n",
    "    n_test = len(X_test)\n",
    "    mu, sigma, local_q_x = create_dist_qx(model, X_test)\n",
    "    local_p_x = create_dist_prior(X_test, mu)\n",
    "    X_pred = model(model.sample_latent_variable(X_test))\n",
    "    exp_log_prob = likelihood.expected_log_prob(X_test.T, X_pred)\n",
    "    log_likelihood = exp_log_prob.sum(0).div(n_test)\n",
    "    kl_x = kl_divergence(local_q_x, local_p_x).div(n_test)\n",
    "    kl_u = kl_divergence_variational(X_test)\n",
    "    score = -(log_likelihood - kl_u - kl_x).detach().numpy()\n",
    "    score = MinMaxScaler().fit_transform(np.reshape(score, (-1, 1)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "iterator = trange(10000, leave=True)\n",
    "for i in iterator:\n",
    "    optimizer.zero_grad()\n",
    "    _, _, batch_index, ratio = get_indices(lb_train)\n",
    "    idx_n, idx_a = get_loe_index(Y_train, batch_index)\n",
    "\n",
    "    ll_n, klu_n, kl_n, lm_n = calculate_terms(Y_train, idx_n)\n",
    "    ll_a, klu_a, kl_a, lm_a = calculate_terms(Y_train, idx_a)\n",
    "    loss_normal, loss_anomaly = (ll_n - klu_n - kl_n).sum(), (ll_a - klu_a - kl_a).sum()\n",
    "    loss = -(loss_normal + loss_anomaly).sum()\n",
    "    metrics.loss_normal.append(loss_normal.detach().tolist())\n",
    "    metrics.loss_anomaly.append(loss_anomaly.detach().tolist())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    iterator.set_description(\n",
    "        \"Loss: \" + str(float(np.round(loss.item(), 2))) + \", iter no: \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "Y_pred_mean, Y_pred_covar = model.reconstruct_y(Y_test)\n",
    "X_pred_mean, X_pred_covar = model.predict_latent(Y_test)\n",
    "\n",
    "model.get_X_mean(Y_test)\n",
    "\n",
    "\n",
    "score = predict_score(Y_test)\n",
    "lengthscale = model.covar_module.base_kernel.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5205796360969543, -0.7222080230712891]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_X_mean(Y_test).detach().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = DataOutput(\n",
    "    X_mean_pred= X_pred_mean.detach().tolist(),\n",
    "    X_cov_pred= X_pred_covar.detach().tolist(),\n",
    "    Y_mean_pred= Y_pred_mean.detach().tolist(),\n",
    "    Y_cov_pred= Y_pred_covar.detach().tolist(),\n",
    "    score= score.flatten().tolist(),\n",
    "    lenghtscale= lengthscale.detach().tolist()[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_output.X_cov_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "inv_lengthscale = 1 / model.covar_module.base_kernel.lengthscale\n",
    "print(inv_lengthscale)\n",
    "\n",
    "latent_dim = model.X.latent_dim\n",
    "\n",
    "values, indices = torch.topk(\n",
    "    model.covar_module.base_kernel.lengthscale, k=2, largest=False\n",
    ")\n",
    "\n",
    "l1 = indices.numpy().flatten()[0]\n",
    "l2 = indices.numpy().flatten()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = model.get_X_mean(Y_train)\n",
    "X_train_scales = model.get_X_scales(Y_train)\n",
    "X_scales = []\n",
    "for torch_tensor in model.get_X_scales(Y_train):\n",
    "    X_scales.append(torch_tensor.tolist())\n",
    "X_scales = np.array(X_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.get_cmap(\"tab10\").colors[::-1]\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(131)\n",
    "plt.title(f\"2d latent subspace [GPLVM]\", fontsize=\"small\")\n",
    "plt.xlabel(\"Latent dim 1\", fontsize=\"small\")\n",
    "plt.ylabel(\"Latent dim 2\", fontsize=\"small\")\n",
    "# Select index of the smallest lengthscales by examining model.covar_module.base_kernel.lengthscales\n",
    "for i, label in enumerate([0, 1]):\n",
    "    X_i = X_train_mean[lb_train.numpy() == label]\n",
    "    plt.scatter(X_i[:, l1], X_i[:, l2], c=[colors[i]], label=label, s=25)\n",
    "    if X_train_scales is not None:\n",
    "        scale_i = X_scales[lb_train.numpy() == label]\n",
    "        plt.errorbar(\n",
    "            X_i[:, l1],\n",
    "            X_i[:, l2],\n",
    "            xerr=scale_i[:, l1],\n",
    "            yerr=scale_i[:, l2],\n",
    "            label=label,\n",
    "            c=\"black\",\n",
    "            fmt=\"none\",\n",
    "        )\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.bar(\n",
    "    np.arange(latent_dim),\n",
    "    height=inv_lengthscale.detach().numpy().flatten(),\n",
    "    color=\"teal\",\n",
    ")\n",
    "plt.title(\"Inverse Lengthscale with SE-ARD kernel\", fontsize=\"small\")\n",
    "plt.xlabel(\"Latent dims\", fontsize=\"small\")\n",
    "\n",
    "# plt.subplot(133)\n",
    "# plt.plot(losses,label='batch_size=100', color='orange')\n",
    "# plt.xlabel('Steps', fontsize='small')\n",
    "# plt.title('Neg. ELBO Loss', fontsize='small')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_output(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
