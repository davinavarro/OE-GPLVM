{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:53.643140Z",
     "start_time": "2022-07-08T07:41:41.552946Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 16:45:42.557793: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 16:45:42.593540: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from new_aeb_gplvm import *\n",
    "import warnings\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import the necessary package\n",
    "from tqdm import trange\n",
    "from utils.data_generator import DataGenerator\n",
    "from utils.myutils import Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "\n",
    "datagenerator = DataGenerator()  # data generator\n",
    "utils = Utils()  # utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_list = os.listdir(\"datasets/Classical\")\n",
    "def fix_name(name):\n",
    "    fixed = name.replace(\".npz\", \"\")\n",
    "    return fixed\n",
    "datasets = list(map(fix_name, dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hyper = pd.read_json(\"experiments/complete/gplvm/000_gplvm_normal_03_best.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:55.627834Z",
     "start_time": "2022-07-08T07:41:53.682035Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hypers(dataset):\n",
    "    hypers = df_hyper[df_hyper.dataset == dataset]\n",
    "    hp = hypers[\n",
    "        [\n",
    "            \"kernel\",\n",
    "            \"batch_size\",\n",
    "            \"learning_rate\",\n",
    "            \"latent_dim\",\n",
    "            \"layers\",\n",
    "            \"n_inducing\",\n",
    "            \"n_epochs\",\n",
    "        ]\n",
    "    ].to_dict(orient=\"records\")\n",
    "    return hp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:55.627834Z",
     "start_time": "2022-07-08T07:41:53.682035Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from baseline.PyOD import PYOD\n",
    "model_dict = {\n",
    "    \"IForest\": PYOD,\n",
    "    \"KNN\": PYOD,\n",
    "    \"CBLOF\": PYOD,\n",
    "    \"PCA\": PYOD,\n",
    "    \"ECOD\": PYOD,\n",
    "    \"GPLVM\": AD_GPLVM,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:50:14.507244Z",
     "start_time": "2022-07-08T07:41:55.631823Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "An exception occurred: Expected parameter covariance_matrix (Tensor of shape (2075, 15, 15)) of distribution MultivariateNormal(loc: torch.Size([2075, 15]), covariance_matrix: torch.Size([2075, 15, 15])) to satisfy the constraint PositiveDefinite(), but found invalid values:\n",
      "tensor([[[10.2992, -2.0823,  2.1149,  ..., -3.0006, -3.6259, -2.7677],\n",
      "         [-2.0823, 11.0313,  2.0093,  ...,  1.6486, -1.2371,  1.9174],\n",
      "         [ 2.1149,  2.0093, 11.4331,  ..., -1.5180, -2.7600,  5.1689],\n",
      "         ...,\n",
      "         [-3.0006,  1.6486, -1.5180,  ..., 10.4996,  4.8290, -0.8837],\n",
      "         [-3.6259, -1.2371, -2.7600,  ...,  4.8290, 12.5394, -2.2803],\n",
      "         [-2.7677,  1.9174,  5.1689,  ..., -0.8837, -2.2803, 13.7366]],\n",
      "\n",
      "        [[10.2990, -2.0824,  2.1150,  ..., -3.0006, -3.6255, -2.7677],\n",
      "         [-2.0824, 11.0313,  2.0094,  ...,  1.6484, -1.2374,  1.9178],\n",
      "         [ 2.1150,  2.0094, 11.4331,  ..., -1.5178, -2.7599,  5.1689],\n",
      "         ...,\n",
      "         [-3.0006,  1.6484, -1.5178,  ..., 10.4998,  4.8292, -0.8838],\n",
      "         [-3.6255, -1.2374, -2.7599,  ...,  4.8292, 12.5394, -2.2799],\n",
      "         [-2.7677,  1.9178,  5.1689,  ..., -0.8838, -2.2799, 13.7365]],\n",
      "\n",
      "        [[10.1872, -1.7680,  2.1222,  ..., -2.8559, -3.5688, -2.6576],\n",
      "         [-1.7680, 11.0303,  1.8977,  ...,  1.6520, -1.2635,  2.0476],\n",
      "         [ 2.1222,  1.8977, 11.5297,  ..., -1.4106, -2.9667,  5.0030],\n",
      "         ...,\n",
      "         [-2.8559,  1.6520, -1.4106,  ..., 10.4745,  4.5582, -0.7530],\n",
      "         [-3.5688, -1.2635, -2.9667,  ...,  4.5582, 12.6567, -2.2821],\n",
      "         [-2.6576,  2.0476,  5.0030,  ..., -0.7530, -2.2821, 13.6130]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[10.2992, -2.0816,  2.1130,  ..., -2.9996, -3.6268, -2.7685],\n",
      "         [-2.0816, 11.0317,  2.0099,  ...,  1.6494, -1.2357,  1.9156],\n",
      "         [ 2.1130,  2.0099, 11.4332,  ..., -1.5184, -2.7589,  5.1691],\n",
      "         ...,\n",
      "         [-2.9996,  1.6494, -1.5184,  ..., 10.4990,  4.8282, -0.8833],\n",
      "         [-3.6268, -1.2357, -2.7589,  ...,  4.8282, 12.5402, -2.2809],\n",
      "         [-2.7685,  1.9156,  5.1691,  ..., -0.8833, -2.2809, 13.7366]],\n",
      "\n",
      "        [[10.2975, -2.0833,  2.1135,  ..., -3.0003, -3.6250, -2.7679],\n",
      "         [-2.0833, 11.0312,  2.0090,  ...,  1.6477, -1.2375,  1.9181],\n",
      "         [ 2.1135,  2.0090, 11.4331,  ..., -1.5180, -2.7591,  5.1698],\n",
      "         ...,\n",
      "         [-3.0003,  1.6477, -1.5180,  ..., 10.4992,  4.8297, -0.8818],\n",
      "         [-3.6250, -1.2375, -2.7591,  ...,  4.8297, 12.5401, -2.2798],\n",
      "         [-2.7679,  1.9181,  5.1698,  ..., -0.8818, -2.2798, 13.7361]],\n",
      "\n",
      "        [[10.1876, -1.7675,  2.1227,  ..., -2.8559, -3.5695, -2.6574],\n",
      "         [-1.7675, 11.0303,  1.8974,  ...,  1.6524, -1.2630,  2.0472],\n",
      "         [ 2.1227,  1.8974, 11.5299,  ..., -1.4104, -2.9670,  5.0025],\n",
      "         ...,\n",
      "         [-2.8559,  1.6524, -1.4104,  ..., 10.4744,  4.5575, -0.7531],\n",
      "         [-3.5695, -1.2630, -2.9670,  ...,  4.5575, 12.6564, -2.2828],\n",
      "         [-2.6574,  2.0472,  5.0025,  ..., -0.7531, -2.2828, 13.6131]]],\n",
      "       grad_fn=<ExpandBackward0>)\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "An exception occurred: Expected parameter covariance_matrix (Tensor of shape (2075, 15, 15)) of distribution MultivariateNormal(loc: torch.Size([2075, 15]), covariance_matrix: torch.Size([2075, 15, 15])) to satisfy the constraint PositiveDefinite(), but found invalid values:\n",
      "tensor([[[ 9.8133,  4.9792, -2.5786,  ...,  3.6970, -5.3000,  0.8872],\n",
      "         [ 4.9792, 10.5447, -0.4559,  ..., -1.0452, -3.8521, -2.3028],\n",
      "         [-2.5786, -0.4559, 12.2491,  ..., -5.9029,  7.4189, -0.8057],\n",
      "         ...,\n",
      "         [ 3.6970, -1.0452, -5.9029,  ...,  8.1617, -4.4406,  1.8422],\n",
      "         [-5.3000, -3.8521,  7.4189,  ..., -4.4406, 13.1218, -1.2105],\n",
      "         [ 0.8872, -2.3028, -0.8057,  ...,  1.8422, -1.2105,  7.5389]],\n",
      "\n",
      "        [[ 9.8135,  4.9794, -2.5787,  ...,  3.6969, -5.2998,  0.8873],\n",
      "         [ 4.9794, 10.5448, -0.4559,  ..., -1.0450, -3.8519, -2.3027],\n",
      "         [-2.5787, -0.4559, 12.2490,  ..., -5.9028,  7.4187, -0.8060],\n",
      "         ...,\n",
      "         [ 3.6969, -1.0450, -5.9028,  ...,  8.1617, -4.4407,  1.8423],\n",
      "         [-5.2998, -3.8519,  7.4187,  ..., -4.4407, 13.1218, -1.2104],\n",
      "         [ 0.8873, -2.3027, -0.8060,  ...,  1.8423, -1.2104,  7.5385]],\n",
      "\n",
      "        [[ 9.8134,  4.9798, -2.5789,  ...,  3.6965, -5.3008,  0.8872],\n",
      "         [ 4.9798, 10.5448, -0.4562,  ..., -1.0453, -3.8528, -2.3021],\n",
      "         [-2.5789, -0.4562, 12.2489,  ..., -5.9026,  7.4191, -0.8050],\n",
      "         ...,\n",
      "         [ 3.6965, -1.0453, -5.9026,  ...,  8.1617, -4.4401,  1.8422],\n",
      "         [-5.3008, -3.8528,  7.4191,  ..., -4.4401, 13.1218, -1.2105],\n",
      "         [ 0.8872, -2.3021, -0.8050,  ...,  1.8422, -1.2105,  7.5387]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.9713,  5.0742, -2.1527,  ...,  3.6155, -4.7179,  0.7332],\n",
      "         [ 5.0742, 10.4909, -0.2697,  ..., -1.5546, -3.8690, -2.2240],\n",
      "         [-2.1527, -0.2697, 12.2509,  ..., -5.9603,  7.4924, -1.0944],\n",
      "         ...,\n",
      "         [ 3.6155, -1.5546, -5.9603,  ...,  8.1689, -3.9559,  2.1585],\n",
      "         [-4.7179, -3.8690,  7.4924,  ..., -3.9559, 13.1827, -1.4392],\n",
      "         [ 0.7332, -2.2240, -1.0944,  ...,  2.1585, -1.4392,  7.5016]],\n",
      "\n",
      "        [[ 9.9684,  5.0722, -2.1525,  ...,  3.6164, -4.7213,  0.7299],\n",
      "         [ 5.0722, 10.4871, -0.2706,  ..., -1.5549, -3.8755, -2.2219],\n",
      "         [-2.1525, -0.2706, 12.2518,  ..., -5.9616,  7.4956, -1.0935],\n",
      "         ...,\n",
      "         [ 3.6164, -1.5549, -5.9616,  ...,  8.1683, -3.9614,  2.1596],\n",
      "         [-4.7213, -3.8755,  7.4956,  ..., -3.9614, 13.1822, -1.4429],\n",
      "         [ 0.7299, -2.2219, -1.0935,  ...,  2.1596, -1.4429,  7.5057]],\n",
      "\n",
      "        [[ 9.8135,  4.9794, -2.5787,  ...,  3.6969, -5.2998,  0.8874],\n",
      "         [ 4.9794, 10.5448, -0.4559,  ..., -1.0450, -3.8518, -2.3027],\n",
      "         [-2.5787, -0.4559, 12.2491,  ..., -5.9028,  7.4187, -0.8060],\n",
      "         ...,\n",
      "         [ 3.6969, -1.0450, -5.9028,  ...,  8.1618, -4.4407,  1.8422],\n",
      "         [-5.2998, -3.8518,  7.4187,  ..., -4.4407, 13.1218, -1.2103],\n",
      "         [ 0.8874, -2.3027, -0.8060,  ...,  1.8422, -1.2103,  7.5386]]],\n",
      "       grad_fn=<ExpandBackward0>)\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "An exception occurred: Expected parameter covariance_matrix (Tensor of shape (1634, 15, 15)) of distribution MultivariateNormal(loc: torch.Size([1634, 15]), covariance_matrix: torch.Size([1634, 15, 15])) to satisfy the constraint PositiveDefinite(), but found invalid values:\n",
      "tensor([[[ 5.6286e+00,  4.5622e-01, -5.1955e-01,  ...,  2.5991e+00,\n",
      "           8.1890e-01,  4.0095e-01],\n",
      "         [ 4.5622e-01,  7.8886e+00,  6.1952e-01,  ...,  1.6879e+00,\n",
      "           8.1919e-01,  1.0007e+00],\n",
      "         [-5.1955e-01,  6.1952e-01,  5.4098e+00,  ...,  7.5501e-01,\n",
      "           5.9786e-02,  2.2101e-01],\n",
      "         ...,\n",
      "         [ 2.5991e+00,  1.6879e+00,  7.5501e-01,  ...,  7.2090e+00,\n",
      "           1.1832e+00,  1.3092e+00],\n",
      "         [ 8.1890e-01,  8.1919e-01,  5.9786e-02,  ...,  1.1832e+00,\n",
      "           6.3204e+00,  3.1631e+00],\n",
      "         [ 4.0095e-01,  1.0007e+00,  2.2101e-01,  ...,  1.3092e+00,\n",
      "           3.1631e+00,  3.4180e+00]],\n",
      "\n",
      "        [[ 9.1693e+00, -2.3809e-01,  2.0293e+00,  ...,  5.6242e-01,\n",
      "          -1.8812e+00, -7.1816e-01],\n",
      "         [-2.3809e-01,  8.7583e+00,  3.1910e-01,  ...,  3.0625e+00,\n",
      "          -3.2018e+00,  4.9315e-01],\n",
      "         [ 2.0293e+00,  3.1910e-01,  5.8484e+00,  ...,  4.2754e-01,\n",
      "          -1.4896e+00,  9.0202e-01],\n",
      "         ...,\n",
      "         [ 5.6242e-01,  3.0625e+00,  4.2754e-01,  ...,  8.4897e+00,\n",
      "           7.5725e-02,  2.8168e+00],\n",
      "         [-1.8812e+00, -3.2018e+00, -1.4896e+00,  ...,  7.5725e-02,\n",
      "           9.4232e+00, -1.6875e-01],\n",
      "         [-7.1816e-01,  4.9315e-01,  9.0202e-01,  ...,  2.8168e+00,\n",
      "          -1.6875e-01,  7.1386e+00]],\n",
      "\n",
      "        [[ 3.1208e+00,  9.2720e-01,  7.7444e-01,  ..., -3.0236e-01,\n",
      "           3.8641e-01,  1.3464e+00],\n",
      "         [ 9.2720e-01,  3.0400e+00,  1.0192e+00,  ...,  5.7119e-01,\n",
      "          -1.1416e+00,  7.9149e-01],\n",
      "         [ 7.7444e-01,  1.0192e+00,  1.6682e+00,  ..., -3.1188e-02,\n",
      "          -7.8169e-02,  5.0483e-01],\n",
      "         ...,\n",
      "         [-3.0236e-01,  5.7119e-01, -3.1188e-02,  ...,  2.7617e+00,\n",
      "          -1.2196e+00,  1.1517e-03],\n",
      "         [ 3.8641e-01, -1.1416e+00, -7.8169e-02,  ..., -1.2196e+00,\n",
      "           2.2425e+00,  2.4384e-01],\n",
      "         [ 1.3464e+00,  7.9149e-01,  5.0483e-01,  ...,  1.1517e-03,\n",
      "           2.4384e-01,  3.9022e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0463e+01, -1.1920e+00,  5.5322e-02,  ..., -2.3006e+00,\n",
      "          -1.2083e+00,  9.4030e-01],\n",
      "         [-1.1920e+00,  8.8387e+00, -2.6272e+00,  ..., -2.4026e-01,\n",
      "          -3.7781e+00,  6.0157e-01],\n",
      "         [ 5.5322e-02, -2.6272e+00,  8.7730e+00,  ...,  1.8563e+00,\n",
      "           5.6533e+00,  2.2889e+00],\n",
      "         ...,\n",
      "         [-2.3006e+00, -2.4026e-01,  1.8563e+00,  ...,  1.0261e+01,\n",
      "           1.5984e+00, -2.3784e-01],\n",
      "         [-1.2083e+00, -3.7781e+00,  5.6533e+00,  ...,  1.5984e+00,\n",
      "           1.0821e+01,  1.5577e+00],\n",
      "         [ 9.4030e-01,  6.0157e-01,  2.2889e+00,  ..., -2.3784e-01,\n",
      "           1.5577e+00,  8.4600e+00]],\n",
      "\n",
      "        [[ 4.1894e+00, -3.9088e-01,  3.8225e-01,  ...,  1.6368e-01,\n",
      "           4.5807e-03, -1.9517e-01],\n",
      "         [-3.9088e-01,  3.0232e+00, -5.4526e-01,  ..., -5.8025e-01,\n",
      "          -3.8021e-01, -1.2112e+00],\n",
      "         [ 3.8225e-01, -5.4526e-01,  3.7920e+00,  ...,  1.5468e+00,\n",
      "           8.0180e-01, -8.4864e-01],\n",
      "         ...,\n",
      "         [ 1.6368e-01, -5.8025e-01,  1.5468e+00,  ...,  2.8808e+00,\n",
      "           2.0117e-01, -1.1587e+00],\n",
      "         [ 4.5807e-03, -3.8021e-01,  8.0180e-01,  ...,  2.0117e-01,\n",
      "           4.2496e+00, -2.7149e-01],\n",
      "         [-1.9517e-01, -1.2112e+00, -8.4864e-01,  ..., -1.1587e+00,\n",
      "          -2.7149e-01,  4.1575e+00]],\n",
      "\n",
      "        [[ 3.3266e+00, -4.3728e-01,  1.2402e+00,  ..., -8.7858e-01,\n",
      "           2.4994e-01,  9.1142e-01],\n",
      "         [-4.3728e-01,  2.9770e+00, -9.1878e-01,  ...,  3.8113e-01,\n",
      "          -1.7079e+00, -1.4087e+00],\n",
      "         [ 1.2402e+00, -9.1878e-01,  3.1798e+00,  ..., -7.7056e-01,\n",
      "           2.0556e+00,  1.6531e+00],\n",
      "         ...,\n",
      "         [-8.7858e-01,  3.8113e-01, -7.7056e-01,  ...,  5.6532e+00,\n",
      "          -1.0998e+00,  7.2366e-01],\n",
      "         [ 2.4994e-01, -1.7079e+00,  2.0556e+00,  ..., -1.0998e+00,\n",
      "           5.7574e+00,  1.9846e+00],\n",
      "         [ 9.1142e-01, -1.4087e+00,  1.6531e+00,  ...,  7.2366e-01,\n",
      "           1.9846e+00,  4.6959e+00]]], grad_fn=<ExpandBackward0>)\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "An exception occurred: Could not form valid cluster separation. Please change n_clusters or change clustering method\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n",
      "An exception occurred: Expected parameter covariance_matrix (Tensor of shape (2408, 15, 15)) of distribution MultivariateNormal(loc: torch.Size([2408, 15]), covariance_matrix: torch.Size([2408, 15, 15])) to satisfy the constraint PositiveDefinite(), but found invalid values:\n",
      "tensor([[[ 1.1065e+01,  1.2230e+00, -7.6936e-01,  ..., -3.3291e+00,\n",
      "          -5.0982e-01, -3.2385e+00],\n",
      "         [ 1.2230e+00,  1.3926e+01, -1.1103e+01,  ..., -9.3999e-01,\n",
      "           3.6684e+00, -5.4162e-01],\n",
      "         [-7.6936e-01, -1.1103e+01,  1.3077e+01,  ..., -3.7473e-01,\n",
      "          -5.2663e+00,  8.5841e-01],\n",
      "         ...,\n",
      "         [-3.3291e+00, -9.3999e-01, -3.7473e-01,  ...,  1.3501e+01,\n",
      "           2.2763e+00,  4.2613e+00],\n",
      "         [-5.0982e-01,  3.6684e+00, -5.2663e+00,  ...,  2.2763e+00,\n",
      "           8.2642e+00, -4.8759e-02],\n",
      "         [-3.2385e+00, -5.4162e-01,  8.5841e-01,  ...,  4.2613e+00,\n",
      "          -4.8759e-02,  1.1480e+01]],\n",
      "\n",
      "        [[ 4.5635e+00,  1.5097e+00,  3.1184e-01,  ...,  1.3898e+00,\n",
      "          -5.0873e-02,  1.2550e-01],\n",
      "         [ 1.5097e+00,  7.1280e+00, -1.2187e+00,  ..., -1.3992e+00,\n",
      "           6.3128e-01,  1.5800e+00],\n",
      "         [ 3.1184e-01, -1.2187e+00,  3.3054e+00,  ..., -2.6550e-01,\n",
      "           6.5238e-01, -1.5587e+00],\n",
      "         ...,\n",
      "         [ 1.3898e+00, -1.3992e+00, -2.6550e-01,  ...,  8.1421e+00,\n",
      "          -1.4624e+00, -2.0767e+00],\n",
      "         [-5.0873e-02,  6.3128e-01,  6.5238e-01,  ..., -1.4624e+00,\n",
      "           4.7268e+00, -1.1564e-01],\n",
      "         [ 1.2550e-01,  1.5800e+00, -1.5587e+00,  ..., -2.0767e+00,\n",
      "          -1.1564e-01,  7.3365e+00]],\n",
      "\n",
      "        [[ 1.1508e+01, -1.1153e+00,  1.9140e+00,  ..., -5.6637e-01,\n",
      "           3.2294e+00,  7.1491e-01],\n",
      "         [-1.1153e+00,  1.0644e+01,  2.1905e+00,  ..., -6.5486e-01,\n",
      "           2.3314e+00,  4.2943e+00],\n",
      "         [ 1.9140e+00,  2.1905e+00,  8.5275e+00,  ..., -3.4125e+00,\n",
      "          -1.2045e-01,  5.3893e-01],\n",
      "         ...,\n",
      "         [-5.6637e-01, -6.5486e-01, -3.4125e+00,  ...,  1.0630e+01,\n",
      "           2.1541e+00, -9.2896e-01],\n",
      "         [ 3.2294e+00,  2.3314e+00, -1.2045e-01,  ...,  2.1541e+00,\n",
      "           7.8010e+00, -2.3349e-01],\n",
      "         [ 7.1491e-01,  4.2943e+00,  5.3893e-01,  ..., -9.2896e-01,\n",
      "          -2.3349e-01,  7.9603e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0635e+01, -1.6957e+00,  7.7785e-01,  ..., -7.8189e-01,\n",
      "           2.5518e+00,  1.0512e+00],\n",
      "         [-1.6957e+00,  1.0602e+01,  1.8878e+00,  ...,  4.4018e-02,\n",
      "           2.3152e+00,  4.6091e+00],\n",
      "         [ 7.7785e-01,  1.8878e+00,  8.8147e+00,  ..., -3.6502e+00,\n",
      "          -2.4311e-01, -2.6804e-01],\n",
      "         ...,\n",
      "         [-7.8189e-01,  4.4018e-02, -3.6502e+00,  ...,  1.0537e+01,\n",
      "           2.2693e+00, -5.3096e-01],\n",
      "         [ 2.5518e+00,  2.3152e+00, -2.4311e-01,  ...,  2.2693e+00,\n",
      "           8.0207e+00,  7.9496e-02],\n",
      "         [ 1.0512e+00,  4.6091e+00, -2.6804e-01,  ..., -5.3096e-01,\n",
      "           7.9496e-02,  7.4639e+00]],\n",
      "\n",
      "        [[ 1.1049e+01,  1.3160e+00, -8.1329e-01,  ..., -3.3225e+00,\n",
      "          -4.7282e-01, -3.2051e+00],\n",
      "         [ 1.3160e+00,  1.3926e+01, -1.1104e+01,  ..., -9.2622e-01,\n",
      "           3.6123e+00, -5.5638e-01],\n",
      "         [-8.1329e-01, -1.1104e+01,  1.3080e+01,  ..., -3.9471e-01,\n",
      "          -5.2125e+00,  8.7117e-01],\n",
      "         ...,\n",
      "         [-3.3225e+00, -9.2622e-01, -3.9471e-01,  ...,  1.3487e+01,\n",
      "           2.2851e+00,  4.2173e+00],\n",
      "         [-4.7282e-01,  3.6123e+00, -5.2125e+00,  ...,  2.2851e+00,\n",
      "           8.2774e+00, -1.3554e-02],\n",
      "         [-3.2051e+00, -5.5638e-01,  8.7117e-01,  ...,  4.2173e+00,\n",
      "          -1.3554e-02,  1.1492e+01]],\n",
      "\n",
      "        [[ 1.1053e+01,  1.2419e+00, -7.9178e-01,  ..., -3.3248e+00,\n",
      "          -5.0039e-01, -3.2290e+00],\n",
      "         [ 1.2419e+00,  1.3926e+01, -1.1102e+01,  ..., -9.5096e-01,\n",
      "           3.6624e+00, -5.3665e-01],\n",
      "         [-7.9178e-01, -1.1102e+01,  1.3076e+01,  ..., -3.5983e-01,\n",
      "          -5.2531e+00,  8.8093e-01],\n",
      "         ...,\n",
      "         [-3.3248e+00, -9.5096e-01, -3.5983e-01,  ...,  1.3513e+01,\n",
      "           2.2719e+00,  4.2539e+00],\n",
      "         [-5.0039e-01,  3.6624e+00, -5.2531e+00,  ...,  2.2719e+00,\n",
      "           8.2699e+00, -4.5336e-02],\n",
      "         [-3.2290e+00, -5.3665e-01,  8.8093e-01,  ...,  4.2539e+00,\n",
      "          -4.5336e-02,  1.1492e+01]]], grad_fn=<ExpandBackward0>)\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "subsampling for dataset 23_mammography...\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "subsampling for dataset 32_shuttle...\n",
      "subsampling for dataset 33_skin...\n",
      "subsampling for dataset 34_smtp...\n",
      "An exception occurred: list index out of range\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "subsampling for dataset 01_ALOI...\n",
      "subsampling for dataset 03_backdoor...\n",
      "generating duplicate samples for dataset 04_breastw...\n",
      "subsampling for dataset 05_campaign...\n",
      "subsampling for dataset 08_celeba...\n",
      "subsampling for dataset 09_census...\n",
      "subsampling for dataset 10_cover...\n",
      "subsampling for dataset 11_donors...\n",
      "subsampling for dataset 13_fraud...\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "subsampling for dataset 16_http...\n"
     ]
    }
   ],
   "source": [
    "# seed for reproducible results\n",
    "seed = 42\n",
    "df_AUCROC = pd.DataFrame(data=None, index=datasets, columns=model_dict.keys())\n",
    "df_AUCPR = pd.DataFrame(data=None, index=datasets, columns=model_dict.keys())\n",
    "for mode in [\"normal\", \"contaminated\"]:\n",
    "    for duplicates in [2,3,4,5,6]:\n",
    "        for dataset in datasets:\n",
    "            # import the dataset\n",
    "            datagenerator.dataset = dataset  # specify the dataset name\n",
    "            data = datagenerator.generator(\n",
    "                la=1.0,\n",
    "                realistic_synthetic_mode=None,\n",
    "                noise_type=\"duplicated_anomalies\",\n",
    "                duplicate_times= duplicates,\n",
    "                stdscale=True,\n",
    "                minmax=False,\n",
    "            )\n",
    "            ratio = data[\"y_train\"].sum() / len(data[\"y_train\"])\n",
    "            Y_train, Y_test, lb_train, lb_test = (\n",
    "                data[\"X_train\"],\n",
    "                data[\"X_test\"],\n",
    "                data[\"y_train\"],\n",
    "                data[\"y_test\"],\n",
    "            )\n",
    "            \n",
    "            if mode == \"normal\":\n",
    "                idx_n = np.where(lb_train == 0)[0]\n",
    "                Y_train = Y_train[idx_n]\n",
    "                lb_train = lb_train[idx_n]\n",
    "        \n",
    "            for name, clf in model_dict.items():\n",
    "                try:\n",
    "                    if name != \"GPLVM\":\n",
    "                        clf = clf(seed=seed, model_name=name)\n",
    "                        clf = clf.fit(X_train=Y_train, y_train=lb_train)\n",
    "                        score = clf.predict_score(Y_test)\n",
    "                    else:\n",
    "                        hp = get_hypers(dataset)\n",
    "                        clf = AD_GPLVM(\n",
    "                            latent_dim=hp[\"latent_dim\"],\n",
    "                            n_inducing=hp[\"n_inducing\"],\n",
    "                            n_epochs=hp[\"n_epochs\"],\n",
    "                            nn_layers=tuple(map(int, hp[\"layers\"].split(\",\"))),\n",
    "                            lr=hp[\"learning_rate\"],\n",
    "                            batch_size=hp[\"batch_size\"],\n",
    "                            kernel=hp[\"kernel\"],\n",
    "                        )\n",
    "                        clf.fit(torch.tensor(Y_train, dtype=torch.float32))\n",
    "                        score = clf.predict_score(torch.tensor(Y_test, dtype=torch.float32))\n",
    "        \n",
    "                    result = utils.metric(y_true=lb_test, y_score=score)\n",
    "        \n",
    "                    # save results\n",
    "                    df_AUCROC.loc[dataset, name] = result[\"aucroc\"]\n",
    "                    df_AUCPR.loc[dataset, name] = result[\"aucpr\"]\n",
    "        \n",
    "                except Exception as error:\n",
    "                    print(\"An exception occurred:\", error)\n",
    "                    df_AUCROC.loc[dataset, name] = 0.0\n",
    "                    df_AUCPR.loc[dataset, name] = 0.0\n",
    "        df_AUCROC.to_json(f\"experiments/complete/adbench/duplicate_anomalies/ad_vs_gp_aucroc_{mode}_{duplicates}.json\", orient = \"records\")\n",
    "        df_AUCPR.to_json( f\"experiments/complete/adbench/duplicate_anomalies/ad_vs_gp_aucpr_{mode}_{duplicates}.json\", orient = \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_AUCROC.to_json(\"experiments/complete/002_adbench_vs_gplvm_aucroc_contaminated.json\", orient = \"records\")\n",
    "#df_AUCROC.to_csv(\"experiments/complete/002_adbench_vs_gplvm_aucroc_contaminated.csv\")\n",
    "#df_AUCPR.to_json(\"experiments/complete/002_adbench_vs_gplvm_aucpr_contaminated.json\", orient = \"records\")\n",
    "#df_AUCPR.to_csv(\"experiments/complete/002_adbench_vs_gplvm_aucpr_contaminated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(data = df_AUCROC)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data = df_AUCPR)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_AUCROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_AUCROC.reset_index().rename(columns={\"index\": \"dataset\"}).to_json(\n",
    "#    \"experiments/complete/002_adbench_auc_roc_normal_results.json\", orient=\"records\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_AUCPR.reset_index().rename(columns={\"index\": \"dataset\"}).to_json(\n",
    "#    \"experiments/complete/002_adbench_auc_pr_normal_results.json\", orient=\"records\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
