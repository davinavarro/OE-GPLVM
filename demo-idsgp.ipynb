{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qaU4gXwvfsjn",
        "l2iTn0p3fvor",
        "WavAb4OWl9N-",
        "VoAF__CJsDJu",
        "ZGIvJ52jzd5j",
        "6T0mJ5_J0agd",
        "SVv9CUl176nF",
        "ifMSyPm7Dbfa",
        "lO_ILxumKAXY",
        "leV_UdDHOWVg",
        "IUUBKOPpEk0A",
        "ZI4g02EodhEn",
        "c5R4t8gz5KH0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n",
        "* pytorch-geometric\n",
        "* gpytorch\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNFgYm18fP8s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HbmmI4Ve2L3"
      },
      "outputs": [],
      "source": [
        "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "# !pip install torch-geometric\n",
        "# !pip install gpytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Imports"
      ],
      "metadata": {
        "id": "qaU4gXwvfsjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import warnings\n",
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "rGE3wR-bfuar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Process Model\n",
        "In order to build the main model, one must write some GPyTorch modules:\n",
        "* Distribution\n",
        "* Variational strategy\n",
        "* Variational ELBO"
      ],
      "metadata": {
        "id": "l2iTn0p3fvor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Dependent Variational Distribution\n",
        "The distribution must be implemented in such a way that its parameters are updated based on the estimation given by an outside non-linear function (i.e., a Graph Neural Network). Thus, the new class, called **InputDependentDistribution**, instantiates the GPyTorch class **_VariationalDistribution** and add a new function ***update_params($q_{\\mu}\\in\\mathbb{R}^{N\\times C\\times M}$, $q_{L}\\in\\mathbb{R}^{N\\times M\\times M}$)***, where $N$ is the number of samples, $D$ is the number of features, and $M$ is the number of inducing points."
      ],
      "metadata": {
        "id": "WavAb4OWl9N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpytorch.variational import _VariationalDistribution\n",
        "from gpytorch.lazy import TriangularLazyTensor, CholLazyTensor\n",
        "from gpytorch.distributions import MultivariateNormal\n",
        "\n",
        "class InputDependentVariationalDistribution(_VariationalDistribution):\n",
        "    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=1e-3):\n",
        "        super(InputDependentVariationalDistribution, self).__init__(\n",
        "            num_inducing_points=num_inducing_points,\n",
        "            batch_shape=batch_shape,\n",
        "            mean_init_std=mean_init_std\n",
        "        )\n",
        "\n",
        "        self.variational_mean = torch.zeros(num_inducing_points).repeat(*batch_shape, 1).unsqueeze(0)\n",
        "        self.chol_variational_covar = torch.eye(num_inducing_points).repeat(*batch_shape, 1, 1).unsqueeze(0)\n",
        "\n",
        "    def forward(self):\n",
        "        chol_variational_covar = TriangularLazyTensor(self.chol_variational_covar)\n",
        "        variational_covar = CholLazyTensor(chol_variational_covar)\n",
        "        return MultivariateNormal(self.variational_mean, variational_covar)\n",
        "\n",
        "    def initialize_variational_distribution(self, prior_dist):\n",
        "        pass\n",
        "\n",
        "    def shape(self):\n",
        "        return torch.Size(self.variational_mean.shape)\n",
        "\n",
        "    def update_params(self, variational_mean, chol_variational_covar_vec):\n",
        "        self._update_variational_mean(variational_mean)\n",
        "        self._update_chol_variational_covar(chol_variational_covar_vec)\n",
        "\n",
        "    def _update_variational_mean(self, variational_mean):\n",
        "        self.variational_mean = variational_mean\n",
        "\n",
        "    def _update_chol_variational_covar(self, chol_variational_covar_vec):\n",
        "        # getting the indices of a triangular inferior matrix\n",
        "        tril_i = torch.tril_indices(self.num_inducing_points, self.num_inducing_points)\n",
        "\n",
        "        # getting the indices of the diagonal\n",
        "        diag_i = torch.arange(self.num_inducing_points)\n",
        "\n",
        "        # transforming the vectorized cholesky into a matrix\n",
        "        num_inputs, num_classes, _ = chol_variational_covar_vec.shape\n",
        "        chol_variational_covar_shape = [num_inputs, num_classes, self.num_inducing_points, self.num_inducing_points]\n",
        "        self.chol_variational_covar = torch.zeros(chol_variational_covar_shape, device=chol_variational_covar_vec.device)\n",
        "        self.chol_variational_covar[..., tril_i[0], tril_i[1]] = chol_variational_covar_vec\n",
        "\n",
        "        # the diagonal of the matrices pass thourgh a Softplus function\n",
        "        self.chol_variational_covar[..., diag_i, diag_i] = F.softplus(self.chol_variational_covar[..., diag_i, diag_i])"
      ],
      "metadata": {
        "id": "s6Y3vGnnfxdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Dependent Variational Strategy\n",
        "The variational strategy must be rewritten in order to support 3D tensor operations. Thus, a new class **InputDependentVariationalStrategy** is created which instantiates **VariationalStrategy**. At last, the functions ***prior_distribution*** and ***forward*** are rewritten."
      ],
      "metadata": {
        "id": "VoAF__CJsDJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpytorch.variational import VariationalStrategy\n",
        "from gpytorch.lazy import DiagLazyTensor, SumLazyTensor, MatmulLazyTensor\n",
        "from gpytorch.utils import cached\n",
        "from gpytorch.settings import trace_mode, _linalg_dtype_cholesky\n",
        "\n",
        "class InputDependentVariationalStrategy(VariationalStrategy):\n",
        "    def __init__(self, model, inducing_points, variational_distribution):\n",
        "        super(InputDependentVariationalStrategy, self).__init__(\n",
        "            model, inducing_points, variational_distribution, learn_inducing_locations=False\n",
        "        )\n",
        "\n",
        "        delattr(self, \"inducing_points\")\n",
        "        self.inducing_points = inducing_points\n",
        "\n",
        "    @property\n",
        "    @cached(name=\"prior_distribution_memo\")\n",
        "    def prior_distribution(self):\n",
        "        zeros = torch.zeros(self.variational_distribution.mean.shape, device=self.inducing_points.device)\n",
        "        ones = torch.ones_like(zeros, device=zeros.device)\n",
        "        return MultivariateNormal(zeros, DiagLazyTensor(ones))\n",
        "\n",
        "    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None, **kwargs):\n",
        "\n",
        "        # compute p(f), p(u), and Kuf\n",
        "        dist_data = self.model.forward(x, covar_part=\"Kff\", **kwargs)\n",
        "        dist_induc = self.model.forward(inducing_points, covar_part=\"Kuu\", **kwargs)\n",
        "        induc_data_covar = self.model.covar_module(inducing_points, x, covar_part=\"Kuf\", **kwargs)\n",
        "\n",
        "        test_mean = dist_data.mean\n",
        "        induc_induc_covar = dist_induc.lazy_covariance_matrix.add_jitter(1e-4)\n",
        "        data_data_covar = dist_data.lazy_covariance_matrix\n",
        "        induc_data_covar = induc_data_covar.evaluate()\n",
        "\n",
        "        # compute interpolation terms\n",
        "        # Kuu^{-1/2} Kuf\n",
        "        L = self._cholesky_factor(induc_induc_covar)\n",
        "        interp_term = L.inv_matmul(induc_data_covar.type(_linalg_dtype_cholesky.value())).to(inducing_points.dtype)\n",
        "\n",
        "        # compute the mean of q(f)\n",
        "        # Kfu Kuu^{-1/2} (m - Kuu^{-1/2}u_mean) + f_mean\n",
        "        predictive_mean = (interp_term.unsqueeze(-3).transpose(-1, -2) @ inducing_values.unsqueeze(-1)).squeeze(-1) + test_mean.unsqueeze(-1)\n",
        "\n",
        "        # compute the covariance of q(f)\n",
        "        # Kff + Kfu Kuu^{-1/2} (S - I) Kuu^{-1/2} Kuf\n",
        "        middle_term = self.prior_distribution.lazy_covariance_matrix.mul(-1)\n",
        "        if variational_inducing_covar is not None:\n",
        "            middle_term = SumLazyTensor(variational_inducing_covar, middle_term)\n",
        "\n",
        "        if trace_mode.on():\n",
        "            predictive_covar = (\n",
        "                data_data_covar.add_jitter(1e-4)\n",
        "                + interp_term.transpose(-1, -2) @ middle_term.evaluate() @ interp_term\n",
        "        )\n",
        "        else:\n",
        "            predictive_covar = SumLazyTensor(\n",
        "                data_data_covar.add_jitter(1e-4).unsqueeze(-3),\n",
        "                MatmulLazyTensor(interp_term.unsqueeze(-3).transpose(-1, -2), middle_term @ interp_term.unsqueeze(-3))\n",
        "            )\n",
        "\n",
        "        # return the distribution\n",
        "        return MultivariateNormal(predictive_mean, predictive_covar)\n",
        "    \n",
        "    def __call__(self, x, prior=False, **kwargs):\n",
        "        if not self.training:\n",
        "            self._clear_cache()\n",
        "        return super().__call__(x, prior=prior, **kwargs)"
      ],
      "metadata": {
        "id": "GxRG5NX3t3qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Dependent Variational ELBO\n",
        "The variational ELBO must be adapted to 3D tensor operations. In this way, a new class **InputDependentVariationalELBO** is created which instantiates **VariationalELBO** and the function ***_log_likelihood_term*** is rewritten."
      ],
      "metadata": {
        "id": "ZGIvJ52jzd5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpytorch.mlls import VariationalELBO\n",
        "\n",
        "class InputDependentVariationalELBO(VariationalELBO):\n",
        "    def _log_likelihood_term(self, variational_dist_f, target, **kwargs):\n",
        "        return torch.diag(self.likelihood.expected_log_prob(target, variational_dist_f, **kwargs))"
      ],
      "metadata": {
        "id": "js8CncwGzc_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Dependent Stochastic Variational Gaussian Process\n",
        "The GP model is build using the **InputDependentVariationalDistribution** and **InputDependentVariationalStrategy** classes. In this sense, the class **InputDependentSVGP** is created which instantiates the **ApproximateGP** from *gpytorch.models*. The **InputDependentSVGP** class also implements the function ***update_variational_params*** which is used to update the variational parameters by calling the function ***update_params*** from the **InputDependentVariationalDistribution** class."
      ],
      "metadata": {
        "id": "6T0mJ5_J0agd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpytorch.models import ApproximateGP\n",
        "\n",
        "class InputDependentSVGP(ApproximateGP):\n",
        "    def __init__(self, mean, kernel, inducing_points, num_tasks):\n",
        "        variational_distribution = InputDependentVariationalDistribution(inducing_points.shape[-2], torch.Size([num_tasks]))\n",
        "        variational_strategy = InputDependentVariationalStrategy(self, inducing_points, variational_distribution)\n",
        "\n",
        "        super().__init__(variational_strategy)\n",
        "\n",
        "        self.mean_module = mean\n",
        "        self.covar_module = kernel\n",
        "\n",
        "    def update_variational_params(self, inducing_points, variational_mean, chol_variational_covar):\n",
        "        self.variational_strategy.inducing_points = inducing_points\n",
        "        self.variational_strategy._variational_distribution.update_params(variational_mean, chol_variational_covar)\n",
        "\n",
        "    def forward(self, x, **kwargs) -> MultivariateNormal:\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x, **kwargs)\n",
        "        return MultivariateNormal(mean_x, covar_x)"
      ],
      "metadata": {
        "id": "miiKIWyu0Z4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Neural Networks\n",
        "The following layers are implemented:\n",
        "* Variational Layer\n",
        "\n",
        "The following GNNs are implemented:\n",
        "* Graph Convolutional Networks (GCN)\n",
        "* Graph Attention Networks (GAT)\n",
        "* Approximate Personalized Propagation of Neural Predictions (APPNP)\n",
        "* Graph Convolutional Network via Initial residual and Identity mapping (GCNII)"
      ],
      "metadata": {
        "id": "CIjQNdFHCcBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational Layer\n",
        "The variational is defined by:\n",
        "$$\n",
        "Z, q_{\\mu}, q_{L} = Variational(x),\n",
        "$$\n",
        "where Z, $q_{\\mu}$ and $q_{L}$ are the variational parameters of a GP."
      ],
      "metadata": {
        "id": "SVv9CUl176nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import Module, ModuleList, Sequential, Dropout, Linear, ReLU\n",
        "from torch.nn.init import xavier_uniform_, zeros_\n",
        "from torch_geometric.nn import GCNConv, GATConv, APPNP, GCN2Conv\n",
        "\n",
        "class Variational(Module):\n",
        "    def __init__(self, input_dim, num_features, num_classes, num_inducing_points):\n",
        "        \n",
        "        super(Variational, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.num_classes = num_classes\n",
        "        self.num_inducing_points = num_inducing_points\n",
        "        self.chol_dim = num_inducing_points * (num_inducing_points + 1) // 2\n",
        "\n",
        "        self.Z = Linear(input_dim, num_inducing_points * num_features)\n",
        "        self.q_mu = Linear(input_dim, num_classes * num_inducing_points)\n",
        "        self.q_sqrt = Linear(input_dim, num_classes * self.chol_dim)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        xavier_uniform_(self.Z.weight)\n",
        "        xavier_uniform_(self.q_mu.weight)\n",
        "        xavier_uniform_(self.q_sqrt.weight)\n",
        "\n",
        "        zeros_(self.Z.bias)\n",
        "        zeros_(self.q_mu.bias)\n",
        "        zeros_(self.q_sqrt.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        num_nodes = x.size(0)\n",
        "        z = self.Z(x).reshape((num_nodes, self.num_inducing_points, self.num_features))\n",
        "        q_mu = self.q_mu(x).reshape((num_nodes, self.num_classes, self.num_inducing_points))\n",
        "        q_sqrt = self.q_sqrt(x).reshape((num_nodes, self.num_classes, self.chol_dim))\n",
        "        return z, q_mu, q_sqrt"
      ],
      "metadata": {
        "id": "z8iAV-eL8XP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph Convolutional Networks\n",
        "The GCN is defined by\n",
        "\\begin{equation}\n",
        "    H^{(k + 1)} = \\sigma(\\hat{A}H^{(k)}W^{(k)}),\n",
        "\\end{equation}\n",
        "where $H^{(0)} = X$, and $\\sigma(\\cdot)$ is a activation function."
      ],
      "metadata": {
        "id": "ifMSyPm7Dbfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, num_inducing_points, dropout=0.5):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.hidden = ModuleList([\n",
        "            GCNConv(input_dim, hidden_dim, cached=True),\n",
        "            GCNConv(hidden_dim, hidden_dim, cached=True)\n",
        "        ])\n",
        "        self.q_dist = Variational(hidden_dim, input_dim, num_classes, num_inducing_points)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.hidden:\n",
        "            layer.reset_parameters()\n",
        "        self.q_dist.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.hidden[:-1]:\n",
        "            x = layer(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.hidden[-1](x, edge_index)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        z, q_mu, q_sqrt = self.q_dist(x)\n",
        "        return z, q_mu, q_sqrt"
      ],
      "metadata": {
        "id": "RvMAJPkpCfSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph Attention Networks\n",
        "The GAT model operator is defined by:\n",
        "\\begin{equation}\n",
        "    h_i' = \\alpha_{ii}Wh_i + \\sum_{j\\in\\mathcal{N}(i)}\\alpha_{ij}Wh_j,\n",
        "\\end{equation}\n",
        "where the normalized attention coefficients are computed as follows:\n",
        "\\begin{equation}\n",
        "    \\alpha_{ij} = \\frac{\\exp(LeakyReLU(a^T[Wh_i||Wh_j]))}{\\sum_{k\\in\\mathcal{N}(i)}\\exp(LeakyReLU(a^T[Wh_i||Wh_k]))},\n",
        "\\end{equation}\n",
        "where $||$ is the concatenation operator."
      ],
      "metadata": {
        "id": "lO_ILxumKAXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, num_inducing_points,\n",
        "                 dropout=0.0, in_heads=8, out_heads=1, att_dropout=0.0):\n",
        "        \n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.hidden = ModuleList([\n",
        "            GATConv(input_dim, hidden_dim, heads=in_heads, dropout=att_dropout),\n",
        "            GATConv(hidden_dim * in_heads, hidden_dim * in_heads, heads=out_heads,\n",
        "                    concat=False, dropout=att_dropout)\n",
        "        ])\n",
        "        self.q_dist = Variational(hidden_dim * in_heads, input_dim, num_classes,\n",
        "                                  num_inducing_points)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.hidden:\n",
        "            layer.reset_parameters()\n",
        "        self.q_dist.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.hidden[:-1]:\n",
        "            x = F.elu(layer(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.hidden[-1](x, edge_index)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        z, q_mu, q_sqrt = self.q_dist(x)\n",
        "        return z, q_mu, q_sqrt"
      ],
      "metadata": {
        "id": "EXTS8wWbKDLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approximate Personalized Propagation of Neural Predictions\n",
        "The APPNP has linear computational complexity and approximates topic-sensitive PageRank via $K$ aggregations:\n",
        "\n",
        "\\begin{equation}\n",
        "    H^{(k + 1)} = (1 - \\alpha)\\hat{A}H^{(k)} + \\alpha H^{(0)},\n",
        "\\end{equation}\n",
        "where $H^{(0)} = f(X)$."
      ],
      "metadata": {
        "id": "leV_UdDHOWVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNPP(Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, num_inducing_points,\n",
        "                 dropout=0.5, K=10, alpha=0.1):\n",
        "        \n",
        "        super(GNNPP, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.hidden = ModuleList([\n",
        "            Linear(input_dim, hidden_dim),\n",
        "            Linear(hidden_dim, hidden_dim)\n",
        "        ])\n",
        "        self.propagate = APPNP(K, alpha, cached=True)\n",
        "        self.q_dist = Variational(hidden_dim, input_dim, num_classes, num_inducing_points)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.hidden:\n",
        "            xavier_uniform_(layer.weight)\n",
        "            zeros_(layer.bias)\n",
        "        self.q_dist.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.hidden[:-1]:\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = layer(x).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.hidden[-1](x)\n",
        "        x = self.propagate(x, edge_index)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        z, q_mu, q_sqrt = self.q_dist(x)\n",
        "        return z, q_mu, q_sqrt"
      ],
      "metadata": {
        "id": "pMRsV9lpSyNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph Convolutional Networks via Initial residual and Idendity mapping\n",
        "The GCNII operator is given by:\n",
        "\n",
        "\\begin{equation}\n",
        "    H^{(k)} = (1 - \\alpha)\\hat{A}H^{(k)} + \\alpha H^{(0)},\\\\\n",
        "    H^{(k + 1)} = (1 - \\beta)H^{(k)} + \\beta H^{(k)}W^{(k)}\n",
        "\\end{equation}\n",
        "where $H^{(0)} = f(X)$."
      ],
      "metadata": {
        "id": "IUUBKOPpEk0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.init import kaiming_normal_, ones_\n",
        "\n",
        "class GCNII(Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, num_inducing_points, dropout=0.0,\n",
        "                 depth=64, alpha=0.1, theta=0.5):\n",
        "        \n",
        "        super(GCNII, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.q_dist = Variational(hidden_dim, input_dim, num_classes, num_inducing_points)\n",
        "        self.hidden = ModuleList([Linear(input_dim, hidden_dim)])\n",
        "        for l in range(1, depth + 1):\n",
        "            self.hidden.append(GCN2Conv(hidden_dim, alpha, theta, l, cached=True))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        kaiming_normal_(self.hidden[0].weight)\n",
        "        zeros_(self.hidden[0].bias)\n",
        "        for layer in self.hidden[1:]:\n",
        "            layer.reset_parameters()\n",
        "\n",
        "        self.q_dist.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.hidden[0](x).relu()\n",
        "        h0 = x.clone()\n",
        "        for layer in self.hidden[1:]:\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = layer(x, h0, edge_index).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        z, q_mu, q_sqrt = self.q_dist(x)\n",
        "        return z, q_mu, q_sqrt"
      ],
      "metadata": {
        "id": "pYUu2ufoElK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities\n",
        "The utilities functions are:\n",
        "* Prepare Logging\n",
        "* Load Dataset\n",
        "* Metrics: Accuracy, ECE, Reliability Diagram\n",
        "* Step and Evaluation functions\n",
        "* Get Amortizer, GP and Likelihood, and Train\n",
        "* Grid Search"
      ],
      "metadata": {
        "id": "x6clASlofzEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging and Load Dataset"
      ],
      "metadata": {
        "id": "-0toLrEbdcFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from typing import NamedTuple\n",
        "\n",
        "class Data(NamedTuple):\n",
        "    num_features: int\n",
        "    num_classes: int\n",
        "    x: torch.Tensor\n",
        "    y: torch.Tensor\n",
        "    edge_index: torch.Tensor\n",
        "    train_mask: torch.Tensor\n",
        "    val_mask: torch.Tensor\n",
        "    test_mask: torch.Tensor\n",
        "\n",
        "def prepare_logging(name):\n",
        "    os.makedirs('./reports/', exist_ok=True)\n",
        "    fmt = '%(asctime)s, %(name)s: %(message)s'\n",
        "    datefmt = '%Y.%m.%d - %H:%M:%S'\n",
        "    log_file = f'./reports/{name.lower()}.log'\n",
        "\n",
        "    if os.path.isfile(log_file):\n",
        "        os.remove(log_file)\n",
        "\n",
        "    handler = logging.FileHandler(log_file)\n",
        "    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)\n",
        "    handler.setFormatter(formatter)\n",
        "\n",
        "    log = logging.getLogger(f'{name.lower()}')\n",
        "    log.setLevel(logging.INFO)\n",
        "    log.addHandler(handler)\n",
        "\n",
        "    return log\n",
        "\n",
        "def load_dataset(name, device):\n",
        "    loader = Planetoid(root=f'datasets/{name.lower()}', name=name)\n",
        "    x = loader.data.x.to(device)\n",
        "    if name.lower() != 'pubmed':\n",
        "        x = TfidfTransformer(smooth_idf=True).fit_transform(x.cpu().numpy()).todense()\n",
        "        x = torch.Tensor(x).to(device)\n",
        "    y = loader.data.y.to(device)\n",
        "    edge_index = loader.data.edge_index.to(device)\n",
        "    train_mask = torch.where(loader.data.train_mask)[0].to(device)\n",
        "    val_mask = torch.where(loader.data.val_mask)[0].to(device)\n",
        "    test_mask = torch.where(loader.data.test_mask)[0].to(device)\n",
        "\n",
        "    data = Data(loader.num_features, loader.num_classes, x, y, edge_index,\n",
        "                train_mask, val_mask, test_mask)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "lFgpWRYTf0pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics\n",
        "* Accuracy\n",
        "* Expected Calibration Error\n",
        "* Reliability Diagram"
      ],
      "metadata": {
        "id": "ZI4g02EodhEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_score(y_true, y_pred):\n",
        "    return torch.mean(torch.eq(y_true, y_pred).type(torch.float64)).item()\n",
        "\n",
        "def ECE(y_prob, y, nbins=10):\n",
        "    edges = torch.linspace(0, 1, nbins + 1)\n",
        "    accuracy = torch.zeros(nbins)\n",
        "    confidence = torch.zeros(nbins)\n",
        "    bin_sizes = torch.zeros(nbins)\n",
        "\n",
        "    prob, pred = torch.max(y_prob, dim=1)\n",
        "    for bin in range(nbins):\n",
        "        if bin == (nbins - 1):\n",
        "            in_bin = (edges[bin] <= prob) & (prob <= edges[bin + 1])\n",
        "        else:\n",
        "            in_bin = (edges[bin] <= prob) & (prob < edges[bin + 1])\n",
        "\n",
        "        bin_sizes[bin] = in_bin.sum()\n",
        "        if bin_sizes[bin] > 0:\n",
        "            accuracy[bin] = torch.mean((y[in_bin] == pred[in_bin]).type(torch.double))\n",
        "        confidence[bin] = (edges[bin + 1] + edges[bin]) / 2\n",
        "\n",
        "    ece = torch.sum(torch.abs(accuracy - confidence) * bin_sizes) / len(y)\n",
        "\n",
        "    calibration = {\n",
        "        'ece': ece.item(),\n",
        "        'accuracy': accuracy.tolist(),\n",
        "        'confidence': confidence.tolist(),\n",
        "        'bin_sizes': bin_sizes.tolist()\n",
        "    }\n",
        "\n",
        "    return calibration\n",
        "\n",
        "def reliability_diagram(conf, acc, nbins=10):\n",
        "    plt.bar(conf, height=acc, width=0.1 / (nbins / 10), edgecolor='k', align='center', label='Output')\n",
        "    plt.bar(conf, height=conf - acc, bottom=acc, color='r', width=0.1, edgecolor='k', align='center', alpha=0.5, label='Gap')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rqW4fJn2djnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step and Evaluation functions"
      ],
      "metadata": {
        "id": "VGHBkrQTdDer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step(x, y, edge_index, mask, amortizer, gp_model, likelihood, optimizer, criterion,\n",
        "         num_samples=8):\n",
        "    \n",
        "    amortizer.train()\n",
        "    gp_model.train()\n",
        "    likelihood.train()\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # estimate variational parameters and update the GP model\n",
        "    z, m, L = amortizer(x, edge_index)\n",
        "    gp_model.update_variational_params(z[mask], m[mask], L[mask])\n",
        "\n",
        "    # compute ELBO\n",
        "    with gpytorch.settings.num_likelihood_samples(num_samples):\n",
        "        ll, kl, _ = criterion(gp_model(x[mask].unsqueeze(1)), y[mask])\n",
        "    elbo = (ll.sum() - kl.sum().div(len(mask))).div(len(mask))\n",
        "\n",
        "    # compute loss, gradients and update parameters\n",
        "    loss = -elbo\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return elbo.item()"
      ],
      "metadata": {
        "id": "vowY2H2_dC6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(x, y, edge_index, mask, amortizer, gp, likelihood, criterion, num_samples=16):\n",
        "    amortizer.eval()\n",
        "    gp.eval()\n",
        "    likelihood.eval()\n",
        "\n",
        "    with torch.no_grad(), gpytorch.settings.num_likelihood_samples(num_samples):\n",
        "        # compute variational parameters and update the GP model\n",
        "        z, m, L = amortizer(x, edge_index)\n",
        "        gp.update_variational_params(z[mask], m[mask], L[mask])\n",
        "\n",
        "        # compute the GP output\n",
        "        variational_dist = gp(x[mask].unsqueeze(1))\n",
        "\n",
        "        # compute the ELBO\n",
        "        ll, kl, _ = criterion(variational_dist, y[mask])\n",
        "        elbo = (ll.sum() - kl.sum().div(len(mask))).div(len(mask)).item()\n",
        "\n",
        "        # compute the probabilities\n",
        "        y_prob = likelihood(variational_dist).probs.mean(0).squeeze(1)\n",
        "        y_pred = y_prob.argmax(-1)\n",
        "\n",
        "        # get the ground truth\n",
        "        y_true = y[mask]\n",
        "\n",
        "        # compute the Negative Mean Negative Log-Likelihood, Accuracy and Calibration\n",
        "        mnll = -F.nll_loss(y_prob.log(), y_true).item()\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        calibration = ECE(y_prob.cpu(), y_true.cpu())\n",
        "\n",
        "    return {'elbo': elbo, 'mean_ll': mnll, 'accuracy': accuracy, 'calibration': calibration}"
      ],
      "metadata": {
        "id": "ag3YBOCNgzcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Amortizer, GP and Optimizer, and Train"
      ],
      "metadata": {
        "id": "B5Jl5r5XAlPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpytorch.means import ConstantMean\n",
        "from gpytorch.kernels import ScaleKernel, PolynomialKernel\n",
        "from gpytorch.likelihoods import SoftmaxLikelihood\n",
        "\n",
        "def get_amortizer(gnn, input_dim, num_classes, params):\n",
        "    hidden_dim = int(params.hidden_dim)\n",
        "    num_inducing_points = int(params.num_induc)\n",
        "    dropout = params.dropout\n",
        "\n",
        "    if gnn == 'gcn':\n",
        "        return GCN(input_dim, hidden_dim, num_classes, num_inducing_points, dropout=dropout)\n",
        "\n",
        "    if gnn == 'sgc':\n",
        "        return None\n",
        "    \n",
        "    if gnn == 'gat':\n",
        "        return GAT(input_dim, hidden_dim, num_classes, num_inducing_points, dropout=dropout,\n",
        "                   in_heads=int(params.in_heads), out_heads=int(params.out_heads),\n",
        "                   att_dropout=params.att_dropout)\n",
        "    \n",
        "    if gnn == 'appnp':\n",
        "        return GNNPP(input_dim, hidden_dim, num_classes, num_inducing_points, dropout=dropout)\n",
        "    \n",
        "    if gnn == 'gcnii':\n",
        "        return GCNII(input_dim, hidden_dim, num_classes, num_inducing_points, dropout=dropout,\n",
        "                     depth=int(params.depth))\n",
        "\n",
        "def get_gp(input_dim, num_classes, params):\n",
        "    inducing_points = torch.zeros(int(params.num_induc), input_dim).unsqueeze(0)\n",
        "    mean = ConstantMean()\n",
        "    kernel = ScaleKernel(PolynomialKernel(power=params.power))\n",
        "    gp = InputDependentSVGP(mean, kernel, inducing_points, num_classes)\n",
        "    likelihood = SoftmaxLikelihood(mixing_weights=False, num_classes=num_classes)\n",
        "    return gp, likelihood\n",
        "\n",
        "def get_optimizer(amortizer, gp, likelihood, lr, num_epochs, params,\n",
        "                  steps=[0.25, 0.5], gamma=0.1):\n",
        "    gnn_lr = lr #* 0.1\n",
        "    criterion = InputDependentVariationalELBO(likelihood, gp, num_data=1, combine_terms=False)\n",
        "    optimizer = torch.optim.Adam([\n",
        "        {'params': amortizer.hidden.parameters(), 'weight_decay': params.gnn_wdecay, 'lr': gnn_lr},\n",
        "        {'params': amortizer.q_dist.parameters(), 'weight_decay': params.var_wdecay, 'lr': gnn_lr},\n",
        "        {'params': gp.covar_module.parameters()},\n",
        "        {'params': gp.mean_module.parameters()},\n",
        "        {'params': likelihood.parameters()},\n",
        "    ], lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer, [steps[0] * num_epochs, steps[1] * num_epochs], gamma)\n",
        "    return optimizer, scheduler, criterion\n",
        "\n",
        "def train(amortizer, gp, likelihood, optimizer, scheduler, criterion, x, y, edge_index,\n",
        "          train_mask, val_mask, metric='elbo'):\n",
        "    \n",
        "    elbo_curve = []\n",
        "    val_scores = []\n",
        "    no_improve = 0\n",
        "    best_val_scores = {'elbo': -torch.inf, 'mean_ll': -torch.inf, 'accuracy': -torch.inf}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        elbo = step(x, y, edge_index, train_mask, amortizer, gp, likelihood,\n",
        "                    optimizer, criterion, num_samples=10)\n",
        "        scores = evaluate(x, y, edge_index, val_mask, amortizer, gp, likelihood,\n",
        "                          criterion, num_samples=20)\n",
        "        \n",
        "        elbo_curve.append(elbo)\n",
        "        val_scores.append(scores)\n",
        "        scheduler.step()\n",
        "\n",
        "        if scores[metric] > best_val_scores[metric]:\n",
        "            no_improve = 0\n",
        "            best_val_scores = scores\n",
        "            torch.save(amortizer.state_dict(), './models/amortizer.pt')\n",
        "            torch.save(gp.state_dict(), './models/gp.pt')\n",
        "            torch.save(likelihood.state_dict(), './models/likelihood.pt')\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve == n_iter_no_improve:\n",
        "            break\n",
        "            \n",
        "    best_model = {\n",
        "        'amortizer': torch.load('./models/amortizer.pt'),\n",
        "        'gp': torch.load('./models/gp.pt'),\n",
        "        'likelihood': torch.load('./models/likelihood.pt')\n",
        "    }\n",
        "    return elbo_curve, best_val_scores, best_model"
      ],
      "metadata": {
        "id": "EFtcZ_75Adxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {
        "id": "0EWAB2BMrnwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(data, filtered_grid, gnn, metric, val_with_train, device):\n",
        "    grid_size = filtered_grid.shape[0]\n",
        "    best_scores = {'elbo': -torch.inf, 'mean_ll': -torch.inf, 'accuracy': -torch.inf}\n",
        "    best_params = {}\n",
        "\n",
        "    if grid_size == 1:\n",
        "        return filtered_grid.iloc[0], None\n",
        "\n",
        "    # grid search loop\n",
        "    for i, params in filtered_grid.iterrows():\n",
        "        # initialize models\n",
        "        amortizer = get_amortizer(gnn, data.num_features, data.num_classes, params)\n",
        "        gp, likelihood = get_gp(data.num_features, data.num_classes, params)\n",
        "        optimizer, scheduler, criterion = get_optimizer(amortizer, gp, likelihood, lr,\n",
        "                                                        num_epochs, params)\n",
        "\n",
        "        # setting device\n",
        "        amortizer.to(device).reset_parameters()\n",
        "        gp.to(device)\n",
        "        likelihood.to(device)\n",
        "\n",
        "        # train\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')\n",
        "\n",
        "            mask = data.train_mask if val_with_train else data.val_mask\n",
        "            _, scores, _ = train(\n",
        "                amortizer, gp, likelihood, optimizer, scheduler, criterion,\n",
        "                data.x, data.y, data.edge_index, data.train_mask, mask, metric\n",
        "            )\n",
        "\n",
        "        if scores[metric] > best_scores[metric]:\n",
        "            best_scores = scores\n",
        "            best_params = params\n",
        "\n",
        "        print(f'Grid Search: {i}/{grid_size}, '\\\n",
        "              f'{scores[metric]:.4f} ({best_scores[metric]:.4f})')\n",
        "    \n",
        "    return best_params, best_scores"
      ],
      "metadata": {
        "id": "OjTtuMpOrqVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "id": "o_9vdc6ChgbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# seed and device\n",
        "global_seed = 30\n",
        "device = 'cuda'\n",
        "\n",
        "# settings\n",
        "dataset = 'PubMed'\n",
        "lr = 0.01\n",
        "num_epochs = 2000\n",
        "n_iter_no_improve = 200\n",
        "num_runs = 10\n",
        "metric = 'elbo'\n",
        "val_with_train = True\n",
        "gnn = 'appnp'\n",
        "\n",
        "grid = {\n",
        "    'gnn_wdecay': [5e-3],\n",
        "    'var_wdecay': [5e-4],\n",
        "    'dropout': [0.6],\n",
        "    'hidden_dim': [64],\n",
        "    'power': [3],\n",
        "    'num_induc': [20],\n",
        "}\n",
        "filtered_grid = pd.DataFrame(list(ParameterGrid(grid)))\n",
        "filtered_grid = filtered_grid.query('`var_wdecay` <= `gnn_wdecay`')\n",
        "filtered_grid.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "eqPyMwGjhR1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Seed, Settings and Reports"
      ],
      "metadata": {
        "id": "c5R4t8gz5KH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the seed\n",
        "import random\n",
        "\n",
        "random.seed(global_seed)\n",
        "np.random.seed(global_seed)\n",
        "torch.manual_seed(global_seed)\n",
        "torch.cuda.manual_seed_all(global_seed)\n",
        "\n",
        "# settings informations\n",
        "settings = {\n",
        "    'gnn': gnn,\n",
        "    'grid': grid,\n",
        "    'metric': metric,\n",
        "    'val_with_train': val_with_train,\n",
        "    'learning_rate': lr,\n",
        "    'num_epochs': num_epochs,\n",
        "    'n_iter_no_improve': n_iter_no_improve,\n",
        "    'num_runs': num_runs,\n",
        "}\n",
        "\n",
        "# reports\n",
        "reports = {}\n",
        "\n",
        "# pre-load datasets\n",
        "data = load_dataset(dataset, device)"
      ],
      "metadata": {
        "id": "DrhBN6oif4CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "D7pUdm5Ff1Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "os.makedirs('./models/', exist_ok=True)\n",
        "log = prepare_logging(dataset)\n",
        "log.info(f'Dataset: {dataset}')\n",
        "log.info(f'Model: IDSVGP with {gnn.upper()}\\n')\n",
        "\n",
        "data = load_dataset(dataset, device)\n",
        "hyperparams, _ = grid_search(data, filtered_grid, gnn, metric, val_with_train, device)\n",
        "\n",
        "reports[dataset] = {}\n",
        "reports[dataset]['hyperparams'] = hyperparams\n",
        "for run_id in range(num_runs):\n",
        "    reports[dataset][run_id] = {}\n",
        "        \n",
        "    # train\n",
        "    amortizer = get_amortizer(gnn, data.num_features, data.num_classes, hyperparams)\n",
        "    gp, likelihood = get_gp(data.num_features, data.num_classes, hyperparams)\n",
        "    optimizer, scheduler, criterion = get_optimizer(amortizer, gp, likelihood, lr,\n",
        "                                                    num_epochs, hyperparams)\n",
        "\n",
        "    amortizer.to(device).reset_parameters()\n",
        "    gp.to(device)\n",
        "    likelihood.to(device)\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')\n",
        "\n",
        "        mask = data.train_mask if val_with_train else data.val_mask\n",
        "        train_curve, val_scores, model = train(\n",
        "            amortizer, gp, likelihood, optimizer, scheduler, criterion,\n",
        "            data.x, data.y, data.edge_index, data.train_mask, mask, metric\n",
        "        )\n",
        "\n",
        "    # evalute\n",
        "    amortizer.load_state_dict(model['amortizer'])\n",
        "    gp.load_state_dict(model['gp'])\n",
        "    likelihood.load_state_dict(model['likelihood'])\n",
        "\n",
        "    test_scores = evaluate(data.x, data.y, data.edge_index, data.test_mask,\n",
        "                            amortizer, gp, likelihood, criterion, 300)\n",
        "\n",
        "    # save reports\n",
        "    reports[dataset][run_id]['train_curve'] = train_curve\n",
        "    reports[dataset][run_id]['val_scores'] = val_scores\n",
        "    reports[dataset][run_id]['test_scores'] = test_scores\n",
        "\n",
        "    log.info(f'Run: {run_id}, # Iterations: {len(train_curve)}, '\\\n",
        "                f'Accuracy: {test_scores[\"accuracy\"] * 100:.1f}, '\\\n",
        "                f'Validation: {val_scores[metric]:.4f}')\n",
        "    print(f'Run: {run_id}, # Iterations: {len(train_curve)}, '\\\n",
        "            f'Accuracy: {test_scores[\"accuracy\"] * 100:.1f}, '\\\n",
        "            f'Validation: {val_scores[metric]:.4f}')"
      ],
      "metadata": {
        "id": "LoMROkE2l0XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reports"
      ],
      "metadata": {
        "id": "ub0W4YPif4pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reports[dataset]['hyperparams'] = reports[dataset]['hyperparams'].to_dict()\n",
        "\n",
        "with open('./reports/settings.json', 'w') as f:\n",
        "    json.dump(settings, f)\n",
        "\n",
        "with open('./reports/reports.json', 'w') as f:\n",
        "    json.dump(reports, f)\n",
        "\n",
        "print(f'Mean and standard deviation of {num_runs} runs:')\n",
        "accuracy = []\n",
        "for i in range(num_runs):\n",
        "    accuracy.append(reports[dataset][i]['test_scores']['accuracy'])\n",
        "print(f'{dataset}: {np.mean(accuracy) * 100:.1f} ({np.std(accuracy) * 100:.1f})')"
      ],
      "metadata": {
        "id": "Xj_FxrKif5g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Best Params: {reports[dataset][\"hyperparams\"]}')"
      ],
      "metadata": {
        "id": "hj__7F9kGi9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=[4, 4])\n",
        "plt.plot(train_curve, 'k')\n",
        "plt.grid()\n",
        "plt.xlabel(r'Num. of Epochs')\n",
        "plt.ylabel(rf'ELBO')\n",
        "plt.savefig(f'./reports/idsvgp_{gnn}.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "zGQucYCloHyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmnnBlSkcVb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}